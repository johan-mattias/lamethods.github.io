[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning analytics methods and tutorials",
    "section": "",
    "text": "This is the website for the book “Learning analytics methods and tutorials: A practical guide using R” which will be published by Springer in 2024."
  },
  {
    "objectID": "contributors.html#editors",
    "href": "contributors.html#editors",
    "title": "Contributors",
    "section": "Editors",
    "text": "Editors\nMohammed Saqr, University of Eastern Finland\nSonsoles López-Pernas, University of Eastern Finland"
  },
  {
    "objectID": "contributors.html#associate-editors",
    "href": "contributors.html#associate-editors",
    "title": "Contributors",
    "section": "Associate Editors",
    "text": "Associate Editors\nMiguel Conde, Universidad de León\nRogers Kaliisa, University of Oslo\nKamila Misiejuk, University of Bergen"
  },
  {
    "objectID": "contributors.html#authors",
    "href": "contributors.html#authors",
    "title": "Contributors",
    "section": "Authors",
    "text": "Authors\nEmorie Beck, UC Davis\nJavier Conde, Universidad Politécnica de Madrid\nMiguel Ángel Conde-González, University of León\nCarlos Cuenca-Enrique, Universidad Politécnica de Madrid\nLaura Del-Río-Carazo, Universidad Politécnica de Madrid\nMarion Durand, University of Eastern Finland\nMerja Heinäniemi, University of Eastern Finland\nJouni Helske, University of Jyväskylä\nSatu Helske, University of Turku\nÁngel Hernández-García, Universidad Politécnica de Madrid\nRogers Kaliisa, University of Oslo\nJelena Jovanovic, University of Belgrade\nJoran Jongerling, Tilburg University\nJuho Kopra, University of Eastern Finland\nSonsoles López-Pernas, University of Eastern Finland\nKamila Misiejuk, University of Bergen\nKeefe Murphy, Maynooth University\nGilbert Ritschard, University of Geneva\nAndrew Ruis, University of Wisconsin\nMohammed Saqr, University of Eastern Finland\nLuca Scrucca, Università degli Studi di Perugia\nMatthias Studer, University of Geneva\nDavid Williamson Shaffer, University of Wisconsin\nZachari Swiecki, Monash University\nYuanru Tan, University of Wisconsin\nSanttu Tikka, University of Jyväskylä\nAdrienne Traxler, University of Copenhagen\nLeonie V.D.E. Vogelsmeier, Tilburg University"
  },
  {
    "objectID": "contributors.html#reviewers",
    "href": "contributors.html#reviewers",
    "title": "Contributors",
    "section": "Reviewers",
    "text": "Reviewers\nGökhan Akçapınar, Hacettepe University\nDaniel Amo-Filva, Universitat Ramon Llulll\nEnrique Barra, Universidad Politécnica de Madrid\nUmar Bin Qushem, University of Turku\nManuel Castejón-Limas, Universidad de León\nJavier Conde, Universidad Politécnica de Madrid\nTudor Cristea, TU Eindhoven\nLaura Del-Río-Carazo, Universidad Politécnica de Madrid\nRamy Elmoazen, University of Eastern Finland\nDilrukshi Gamage, Tokyo Tech\nBrian P. Godor, Erasmus University Rotterdam\nFrancisco José García Peñalvo, Universidad de Salamanca\nSami Heikkinen, LAB University of Applied Sciences\nRogers Kaliisa, University of Oslo\nQinyi Liu, University of International Business and Economics\nSonsoles López-Pernas, University of Eastern Finland\nKamila Misiejuk, University of Bergen\nNicolae Nistor, Ludwig-Maximilians-Universität München\nDijana Oreški, Sveučilište u Zagrebu\nSambit Praharaj, Ruhr University Bochum\nMiroslava Raspopović Milić, Belgrade Metropolitan University\nMohammed Saqr, University of Eastern Finland\nYuanru Tan, University of Wisconsin-Madison\nSanttu Tikka, University of Jyväskylä\nTiina Törmänen, University of Oulu\nAdrienne Traxler, University of Copenhagen\nDaphne van de Bongardt, Erasmus University Rotterdam\nThijs Vroegh, Netherlands eScience Center\nJin Zhou, Central China Normal University"
  },
  {
    "objectID": "chapters/ch01-intro/intro.html",
    "href": "chapters/ch01-intro/intro.html",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#introduction",
    "href": "chapters/ch01-intro/intro.html#introduction",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe birth of the field of learning analytics is often ascribed to the first Learning Analytics & Knowledge Conference in 2011, where the widely used definition was coined as “the measurement, collection, analysis, and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs” [1]. Over the years, learning analytics has grown in scope, scale, and diversity and has since attracted researchers from diverse backgrounds bringing several disciplines together under one umbrella. Such increasing interest has resulted in a large number of publications, research projects, specialized units, and funding of large projects. Although other data-intensive fields started before learning analytics (e.g., academic analytics in 2007 and educational data mining in 2008), interest in such methods —and in closely related fields at large— has surged only after the rapid adoption of learning analytics. The unique position of learning analytics at the intersection of education and computer science while reaching out to several other disciplines such as statistics, psychometrics, econometrics, mathematics, and linguistics has accelerated the growth and expansion of the field. Therefore, it is a crucial endeavor for learning analytics researchers to stay abreast of the latest methodological and computational advances to drive their research forward. The diversity and complexity of the existing methods can make this task overwhelming both for newcomers to the learning analytics field and for experienced researchers. When conducting learning analytics research, researchers need to decide which data can and must be collected from learners to give answers to their research questions. They need to understand and decide which analytical methods may apply to such data and their potential limitations. They also need to interpret the findings and contextualize them in light of the existing literature and learning theories. With the motivation to accompany researchers in this challenging journey, this book aims to provide a methodological guide for researchers to study, consult, and embark upon the first steps toward innovation.\nEvery method described in this book was developed before learning analytics, for instance, predicting students’ performance can be traced back to almost a century ago [2], and using social network analysis to understand students’ networks dates back to over five decades ago [3]. Similarly, process mining, sequence analysis, and Markov models can all be traced to times before the birth of the field of learning analytics (e.g., [4–6]). Not only were these methods born outside learning analytics, but they also continue to be developed, refined and advanced in their relevant fields. Nevertheless, learning analytics has succeeded in taking advantage of these methodological developments as well as the increasingly available digital data, computational resources, and data science, and in popularizing data-intensive research in education to bring this field together [7].\nThe diversity of methods and fields that have converged to give rise to learning analytics is evident in the list of authors of the chapters of this book, which include authors of R packages, and experts in methods and applications to drive a state-of-the-art blend. In the first category, we have the expertise of Matthias Studer and Gilbert Ritschard (University of Geneva), essential members of the development team of TraMineR, which is the most central R package for sequence analysis. Closely related, we have Satu and Jouni Helske (University of Turku), developers of seqHMM, an R package for Mixture Hidden Markov Models of sequential data. Within our list of authors, one can also find Luca Scrucca (University of Perugia), author of mclust, an R package for clustering, classification, and density estimation using Gaussian finite mixture models, and Keefe Murphy (Maynooth University), developer of MoEClust, an R package for gaussian parsimonious clustering models with covariates. David Williamson Shaffer (University of Wisconsin-Madison), one of the authors of rENA (an R package for Epistemic Network Analysis), is also included in the book’s author list. We also have Leonie V.D.E. Vogelsmeier (Tilburg University), author of lmfa, an R package for latent Markov factor analysis, and Santtu Tikka (University of Jyväskylä), author of the dynamite R package for dynamic multivariate panel models. In addition, among our authors, we have other statistics experts in several domains: Joran Jongerling (Tilburg University); Emorie Beck (UC Davis); Merja Heinäniemi and Juho Kopra (University of Eastern Finland), and Marieke Schreuder (KU Leuven). Lastly, we count on several emerging and senior learning analytics researchers: Jelena Jovanović (University of Belgrade); Ángel Hernández-García, Javier Conde, Laura Del-Río-Carazo, and Carlos Cuenca-Enrique (Universidad Politécnica de Madrid); Adrienne Traxler (University of Copenhagen); Yuanru Tan and Andrew Ruis (University of Wisconsin-Madison); Zachari Swiecki (Monash University); Kamila Misiejuk (University of Bergen); Miguel Ángel Conde (University of Leon), and Mohammed Saqr, Marion Durand, and Sonsoles López-Pernas (University of Eastern Finland).\nThanks to the unique wealth of authors’ backgrounds and expertise, the book offers a comprehensive array of methods that are described thoroughly with a primer on their usage in prior research in education. A step-by-step tutorial using the R programming language with real-life datasets and case studies is presented for each method. The book starts with an introductory section for readers to get up-to-speed with R programming, in which we cover the basics of the language, data preprocessing, basic statistics, and data visualization. Then, we delve into classic machine learning methods, such as prediction and clustering applied to educational data. These methods enable readers to predict achievement, dropout, and students at risk, as well as to group students into different groups or profiles according to certain characteristics such as motivation or engagement. This is followed by an extensive section devoted to temporal methods such as sequence analysis, Markovian modeling, and process mining, which allow taking advantage of the endless possibilities of trace log data. The book then moves on to discuss network analysis in its many forms including social network analysis, epistemic network analysis, ordered network analysis, and temporal networks. Such methods are crucial for understanding collaboration and relationships between individuals and concepts, which are key aspects of learning. The book concludes with a section on psychometrics such as psychological networks, factor analysis, and structural equation modeling, which are fundamental tools for the analysis of self-reported data among others. We hope the readers find this book useful as a guiding light through the intricate pathways of learning analytics methods, illuminating the ways in which data-driven insights can elevate educational experiences and benefit educators, learners, and researchers alike. The book targets learning analytics researchers (and education researchers at large) at all stages. It is suitable to teach newcomers to the field, even with no experience in R, since the introductory chapters are aimed at getting readers acquainted with the basics of R programming and data analysis. The book also covers advanced methods that may be of interest to experts in the field of learning analytics or data science in general. Moreover, the skills taught are transferable to other fields, i.e., can be applied in other contexts outside of the realm of education."
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#how-the-book-is-structured",
    "href": "chapters/ch01-intro/intro.html#how-the-book-is-structured",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "2 How the book is structured",
    "text": "2 How the book is structured\n\n2.1 Introductory chapters\nThe first section of the book provides the basis for getting up to speed with the R programming language and the data that will be analyzed throughout the book. This section covers the fundamental steps of the data analysis process, such as data preprocessing and exploratory analysis. During data preprocessing, educational data is cleaned and prepared for further analysis. Many crucial decisions about building and conceptualizing learning indicators from raw data are made in this essential step of the learning analytics process. Exploratory analysis enables an early detection of interesting phenomena that can be discovered in the data using visualizations or simple statistics. Using these techniques helps to guide the direction of the in-depth analysis and the selection of more advanced analytical methods.\nChapter 2: A Broad Collection of Datasets for Educational Research Training and Application [8]\nSonsoles López-Pernas, Mohammed Saqr, Javier Conde, Laura Del-Río-Carazo\nSince the goal of this book is to provide a guide and tutorial on how to implement learning analytics methods, the use of relevant data is a key aspect of the contextualization of these methods within learning analytics research. Chapter 2 kicks off the book with an introduction to the most relevant types of data in learning analytics and provides a diverse collection of curated datasets that we will use throughout the book to illustrate the different methods. Understanding the data under examination is a crucial step for the interpretability of the analyses that we will learn to perform in this book, and therefore, readers should familiarize themselves with the datasets described in this chapter to facilitate following the tutorials presented in subsequent ones.\nChapter 3: Getting started with R for Education Research [9]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nThe first tutorial-like chapter of the book provides an introduction to the basics of R programming, with a focus on the Rstudio integrated development environment and the tidyverse programming paradigm. The R programming language has become a popular tool for conducting data analysis in the field of learning analytics. The chapter covers topics such as data types and structures, control structures, pipes, functions, loops, and input/output operations. By the end of the chapter, readers should have a solid understanding of the basics of R programming and have the necessary tools to learn more in-depth topics such as data wrangling and basic statistics using R.\nChapter 4: An R Approach to Data Cleaning and Wrangling for Education [10]\nJuho Kopra, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nAfter learning the basics of the R programming language, Chapter 4 goes one step further by introducing the reader to data wrangling, also known as data cleaning and preprocessing. Data wrangling is a critical step in the data analysis process, particularly in the context of learning analytics. This chapter provides an introduction to data wrangling using R and covers topics such as data importing, cleaning, manipulation, and reshaping with a focus on tidy data. Specifically, readers will learn how to read data from different file formats (e.g., CSV, Excel), how to manipulate data using the dplyr package, and how to reshape data using the tidyr package. Additionally, the chapter covers techniques for combining multiple data sources.\nChapter 5: Introductory Statistics with R for Educational Researchers [11]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nStatistics play a fundamental role in learning analytics, providing a means to analyze and make sense of the vast amounts of data generated by learning environments, visualize relationships, test hypotheses, and make comparisons. Chapter 5 in this book provides an introduction to basic statistical concepts using R and covers topics such as measures of central tendency, variability, correlation, and regression analysis. Specifically, readers will learn how to compute descriptive statistics, test hypotheses, and perform simple linear regression analysis. The chapter also includes practical examples using realistic data sets from the field of learning analytics. By the end of the chapter, readers should have a solid understanding of the basic statistical concepts and methods commonly used in learning analytics, as well as a practical understanding of how to use R to conduct statistical analysis of learning data.\nChapter 6: Visualizing and Reporting Educational Data with R [12]\nSonsoles López-Pernas, Kamila Misiejuk, Santtu Tikka, Mohammed Saqr, Juho Kopra, Merja Heinäniemi\nVisualizing data is central in learning analytics research, underpins learning dashboards, and is a prime method for reporting results and insights to stakeholders. Chapter 6 guides the reader through the process of generating meaningful and aesthetically pleasing visualizations of different types of datasets using well-known R packages. The main visualization types will be demonstrated with an explanation of their usage and use cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. Readers will also be able to generate professional-looking tables with summary statistics.\n\n\n2.2 Machine learning methods\nThe next section follows with some of the classic machine learning methods of learning analytics, which date to the early beginnings of the field: predictive modelling and cluster analysis. Predictive modelling is a supervised learning method used widely in learning analytics research, where past data patterns are analysed to predict students’ future outcomes. Clustering is an unsupervised learning method that detects similar patterns in the data and is typically used to group students based on their personal characteristics, observed behavior, or learning outcomes. Both methods are applied to address various challenges in education, such as preventing student drop-out, comparing strategies to improve academic performance, or identifying disengaged students. In addition, the results are often used to trigger specific interventions to help students succeed or to raise awareness about students’ performance based on specific indicators.\nChapter 7: Predictive Modelling in Learning Analytics using R [13]\nJelena Jovanovic, Sonsoles López Pernas, Mohamed Saqr\nPrediction of learners’ course performance has been a central theme in learning analytics since the inception of the field. The main motivation behind it has been to identify learners who are at risk of low achievement so that they could be offered timely support based on intervention strategies derived from analysis of learners’ data. To predict student success, numerous indicators, from varying data sources, have been examined and reported in the literature, as well as various predictive algorithms. Chapter 7 introduces the reader to predictive modeling, through a review of the main objectives, indicators, and algorithms that have been operationalized in previous works as well as a step-by-step tutorial on how to perform predictive modeling in learning analytics using R. The tutorial demonstrates how to predict student success using learning traces originating from a learning management system, guiding the reader through all the required steps from the data preparation to the evaluation of the built models.\nChapter 8: Clustering Educational Data: A Tutorial with R [14]\nKeefe Murphy, Sonsoles López-Pernas, Mohammed Saqr\nTBD\nChapter 9: An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis [15]\nLuca Scrucca, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 9 presents an alternative approach for capturing different patterns or subgroups within students’ behavior or functioning. Assuming that there is an average pattern that represents the entirety of student populations requires the measured construct to have the same causal mechanism, the same development pattern, and affect students in exactly the same way. Using a person-centered method (Finite Gaussian mixture model or latent profile analysis), this chapter offers an introduction to model-based clustering that includes the principles of the methods, a guide to the choice of number of clusters, an evaluation of clustering results and a detailed guide with code and a real-life dataset. The tutorial part shows how to uncover the heterogeneity within engagement data by identifying latent or unobserved clusters. The discussion elaborates on the interpretation of the results, the advantages of model-based clustering as well as how this method compares with others.\n\n\n2.3 Temporal methods\nWe continue our journey with an introduction to temporal methods in learning analytics. Unlike the methods based on mere counts of events or activities, temporal methods acknowledge the order and temporality of events, as well as the transitions thereof, which are key aspects of learning. Temporal methods have garnered increasing attention since they allow researchers to take advantage of the trace log data that students leave behind when using educational technology and also to study longitudinal processes (e.g., a whole study program). Such methods originate in social sciences and have been imported and adapted into the learning analytics field. We provide three chapters focused on sequence analysis, and two on transition analysis through Markovian modeling and process mining.\nChapter 10: Sequence Analysis in Education: Principles, Technique, and Tutorial with R [16]\nMohammed Saqr, Sonsoles López-Pernas, Satu Helske, Marion Durand, Keefe Murphy, Matthias Studer, Gilbert Ritschard\nSequence analysis is a data mining technique that is increasingly gaining ground in learning analytics. Sequence analysis enables researchers to extract meaningful insights from sequential data, i.e., to summarize the sequential patterns of learning data and classify those patterns into homogeneous groups. Chapter 10 introduces readers to sequence analysis techniques and tools through real-life step-by-step examples of sequential trace log data of students’ online activities. Readers are guided on how to visualize the common sequence plots and interpret such visualizations. An essential part of sequence analysis is the discovery of patterns within sequences through clustering techniques. Therefore, this chapter demonstrates the various sequence clustering methods, calculation of cluster indices, and evaluation of clustering results.\nChapter 11: Modelling the Dynamics of Longitudinal Processes in Education: The VaSSTra Method [17]\nSonsoles López-Pernas, Mohammed Saqr\nBuilding upon the knowledge acquired in the previous chapter, Chapter 11 covers VaSSTra, a method for analyzing multiple variables across multiple time points. The idea behind this method is to summarize multiple variables at each time point into a single state using person-based methods. Then, sequence analysis can be used to analyze the sequences of such states for each person, and clustering techniques can be implemented to detect similar trajectories of the evolution of such states. The method is illustrated in a case study about engagement. Several engagement-related variables are derived from students’ online activities (frequency of each activity, regularity, etc.). These variables are used for clustering students into three states (active, moderate, and disengaged) at each course. Then, sequence analysis is used to map the sequence of engagement states across a whole program. Lastly, clustering mechanisms are used to detect distinct trajectories of engagement.\nChapter 12: A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education [18]\nSatu Helske, Jouni Helske, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 13 presents Markov models, a widely used technique to model temporal processes. Contrary to the deterministic approach seen in the previous sequence analysis chapters, Markovian models are probabilistic models, focusing on the transitions between states instead of studying sequences as a whole. The chapter provides an introduction to Markov models and differentiates between its most common variations: first-order Markov models, hidden Markov models, mixture Markov models, and hidden mixture Markov models. All implementations are illustrated with a step-by-step tutorial using the R package seqHMM using students’ longitudinal data. The chapter also provides a complete guide to performing stochastic process mining with Markovian models as well as plotting, comparing, and clustering different process models\nChapter 13: Multichannel Sequence Analysis in Educational Research Using R [19]\nSonsoles López-Pernas, Satu Helske, Mohammed Saqr, Keefe Murphy\nWhen dealing with learners’ data, sometimes one single source of information is not enough to capture all of the dimensions of the learning process. Fortunately, sequence analysis as a method supports the examination of multiple sequences (termed channels) at the same time as long as they follow the same time scheme. Chapter 13 covers multi-channel sequence analysis, allowing the reader to study and visualize synchronized sequences together, and cluster them into distinct trajectories based on the values of the various channels. We present two methods for clustering: one distance-based (see Chapter 10) and one based on Markovian models (see Chapter 12). We illustrate the method by studying the longitudinal association between student engagement and achievement across a study program.\nChapter 14: The Why, the How, and the When of Educational Process Mining in R [20]\nSonsoles López-Pernas, Mohammed Saqr\nProcess mining is a recent analytical method that enables the extraction of meaningful insights from time-ordered event logs. The goal of process mining is to discover processes from the data, evaluate process efficiency, and help or enhance processes. Since its introduction in education, process mining has been used to map students’ learning processes, visualize learners’ strategies, as well as demonstrate differences in approach to learning across different learning groups. Chapter 14 illustrates how to prepare learners’ data for process mining and how to visualize the process data using the bupaverse framework. Moreover, readers will learn how to examine the transitions between phases or activities within a learning process.\n\n\n2.4 Network Analysis\nThe next section of the book deals with the relational aspects of analyzing educational data such as relationships between students, teachers, and topics. Network analysis is the underlying method used to study such relational aspects. Social network analysis allows researchers to study collaboration and discussion between peers and understand the role each student occupies in the network. Moreover, community finding allows the detection of distinct groups of peers in the network that interact with each other more than with the rest. We can even combine network analysis with the temporal methods presented in the last section through temporal networks or use epistemic or ordered network analysis to explore topic or construct co-occurrence.\nChapter 15: Social Network Analysis: A Primer, a Guide and a Tutorial in R [21]\nMohammed Saqr, Sonsoles López-Pernas, Miguel Ángel Conde, Ángel Hernández-García\nFor five decades, learning networks have been used to map collaboration networks among students, study the influence of peers, and capture the relational dimension of collaborative learning. Additionally, networks have been used to study the semantics of discourse, relations between behaviors, and patterns of relations among teachers. Networks offer a powerful framework with vast potential for data analysis. Chapter 15 introduces the concept and methods of social network analysis and a detailed guide on how researchers can use network analysis using real-world data. The chapter demonstrates network analysis and visualisation with an emphasis on learners’ roles and relevance to the educational context. The chapter further provides a mathematical analysis and interpretation of the different social network metrics such as centrality and betweenness measures with several examples of how they can be used in practice.\nChapter 16: Community Detection in Learning Networks Using R [22]\nÁngel Hernández-García, Carlos Cuenca-Enrique, Adrienne Traxler, Sonsoles López-Pernas, Miguel Ángel Conde, Mohammed Saqr\nIn learning situations, communities can be groups of students within a whole cohort who collaborate with each to a larger extent than with other students in a learning situation. Finding these communities is integral to understanding the interaction process, the structure and behaviour of the formed groups and how they contribute to the overall learning process. Chapter 16 builds on the principles of social networks from Chapter 15 and introduces the topic of community detection. The main aim of community detection is to identify different groups or clusters of nodes within the network that share some similar characteristics. One way of understanding communities in social networks is as subnetworks where the number of internal connections is larger than the number of external connections, and therefore members of a community have a higher probability of being connected to each other than to members of other communities. The chapter focuses on detecting communities (groups of highly connected nodes) within a wider network and shows how to visualize them using R.\nChapter 17: Temporal Network Analysis: Introduction, Methods, and Analysis with R [23]\nMohammed Saqr\nLearning can be viewed as relational, interdependent, and temporal and therefore, methods that account for such multifaceted dynamic processes that unfold overtime are required. Chapter 17 combines the temporal and relational aspects in a single analytics framework: temporal networks. Temporal networks allow modeling of the temporal learning processes i.e., the emergence and flow of activities, communities, and social processes through fine-grained dynamic analysis. This can provide insights into phenomena like knowledge co-construction, information flow, and relationship building. This chapter introduces the basic concepts of temporal networks, their types (i.e, contact and interval), and techniques. The chapter further provides a detailed guide to temporal network analysis, which involves network building, visualization, and statistical analysis at the graph and node level.\nChapter 18: Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics [24]\nYuanru Tan, Zachari Swiecki, Andrew Ruis, David Williamson Shaffer\nThe increasing use of technology in many areas of society and life has led to an increasing amount of Big Data about human behavior and interaction. However, this volume of data is usually too large and strains the capabilities of human interpretation and the traditional social science research approaches. Chapter 18 presents two quantitative ethnographic approaches that links the power of statistics and in-depth ethnographic approaches to understand learning behaviour through large-scale qualitative data. Epistemic Network Analysis (ENA) and Ordered Network Analysis (ONA), are two methods for quantifying, visualizing, and interpreting network data. Taking coded data as input, ENA and ONA represent associations between codes (e.g., topics or categories) in undirected or directed weighted network models, respectively. Both techniques measure the strength of association among codes and illustrate the structure of connections in network graphs, quantify changes in the composition and strength of those connections over time, and enable comparison of networks. The chapter presents a thorough description of the methods and a step-by-step guide on how to implement them with R.\n\n\n2.5 Psychometrics\nWe finalize the book with a section on psychometrics. In the field of educational psychology, psychometrics aims to study how psychological constructs (e.g., intelligence or aptitude) are related to observable variables (e.g., test scores). Traditionally, psychometric methods in educational psychology have relied on self-reported data from validated questionnaire-like instruments, although nowadays researchers have begun to make use of digital data. We present several techniques to investigate the relationship between measured variables and to test hypotheses and theories: psychological networks, factor analysis, and structural equation modeling (SEM).\nChapter 19: Psychological networks [25]\nMohammed Saqr, Emorie Beck, Sonsoles López-Pernas\nWhen analyzing psychological phenomena that take place in educational settings, a multitude of variables are at play that may interact with, trigger, and influence each other. To understand such dependency between variables, it is not enough to analyze the linear relationships between each pair of variables, but rather such complexity calls for using more sophisticated methods that capture the full breadth of the interplay between variables: psychological networks. ​As opposed to social networks where nodes often represent people and edges represent the interactions or relations between them, the nodes in psychological networks represent observed psychological variables and edges represent a statistical relationship between them. Chapter 19 opens the section on psychometric methods by presenting the concept of psychological networks as well as a tutorial for their estimation, visualization, and interpretation with R.\nChapter 20: Factor Analysis in Education Research using R [26]\nLeonie V. D. E. Vogelsmeier, Joran Jongerling, Mohammed Saqr, Sonsoles López-Pernas Chapter 20 presents factor analysis, a method employed to reduce a large number of variables into fewer numbers of factors. The method is commonly used to identify which observable indicators are representative of latent, not directly-observed constructs. This is a key step in developing valid instruments to assess latent constructs such as student engagement in educational research. The chapter describes the two main approaches for conducting factor analysis in detail and provides a tutorial on how to implement both techniques. The first is confirmatory factor analysis (CFA), a more theory-driven approach, in which a researcher actively specifies the number of underlying constructs as well as the pattern of relations between these dimensions and observed variables. The second is exploratory factor analysis (EFA), a more data-driven approach, in which the number of underlying constructs is inferred from the data, and all underlying constructs are assumed to influence all observed variables (at least to some degree).\nChapter 21: Structural Equation Modeling with R for Education Scientists [27]\nJoran Jongerling, Leonie V. D .E. Vogelsmeier, Sonsoles López-Pernas, Mohammed Saqr\nChapter 21 presents the last method in our book: Structural Equation Modeling (SEM). SEM is a suitable and useful method for modeling the multitude of relationships between latent variables and the observable indicators, as well as the relationship between the latent variables themselves to test theories. In its most common form, SEM combines CFA (covered in Chapter 20) with another method named path analysis. Just like CFA, SEM relates observed variables to latent variables that are measured by those observed variables and, as path analysis does, SEM allows for a wide range of regression-type relations between sets of variables (both latent and observed). This chapter presents an introduction to SEM, an integrated strategy for conducting SEM analysis that is well-suited for educational sciences, and a tutorial on how to carry out an SEM analysis in R."
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#the-companion-code-and-data",
    "href": "chapters/ch01-intro/intro.html#the-companion-code-and-data",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "3 The companion code and data",
    "text": "3 The companion code and data\nTo enhance your learning experience and practical understanding of the concepts discussed in this book, we have developed a companion code repository that accompanies each chapter. The repository condenses all the code included in the step-by-step tutorials that illustrate how to implement the different learning analytics methods covered in the book chapters. The code also contains custom functions that encapsulate complex operations, making it easier for anyone to apply the techniques to their own datasets. Moreover, the code will guide the reader on how to generate visualizations, graphs, and plots aimed at helping to interpret and communicate findings effectively. The companion code repository can be accessed at:\n https://github.com/lamethods/code\nAlong with the code, we provide a collection of datasets carefully curated to represent educational scenarios, allowing you to experiment with the techniques discussed in the book and beyond. Each dataset is described in detail in Chapter 2.\n https://github.com/lamethods/data\nBy combining theoretical knowledge with practical application through the companion code and datasets, the reader will be well-equipped to embark on a journey into the fascinating world of learning analytics.\nLet’s get started!"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html",
    "href": "chapters/ch02-data/ch2-data.html",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#introduction",
    "href": "chapters/ch02-data/ch2-data.html#introduction",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning analytics involves the combination of different types of data such as behavioral data, contextual data, performance data, and self-reported data to gain a comprehensive understanding of the learning process [1, 2]. Each type of data provides a unique perspective of the learning process and, when analyzed together, can provide a more complete picture of the learner and the learning environment. Throughout the book, we will work with different types of learning analytics data to illustrate the analysis methods covered. This chapter explores the most common types of data that are commonly used in learning analytics and that we will work with in the subsequent book chapters. Such data include demographic and other contextual data about students, performance data, online activity, interactions with other students and teachers, and self-reported data.\nThis chapter also describes a set of datasets that will be used throughout the book, as well as additional datasets that may be useful for readers to put the newly learned methods into practice. We will discuss the characteristics, structure, and contents of each dataset, as well as the context in which they have been used within the book. The goal of this chapter is to give readers a solid foundation for working with the datasets used in the book, as well as to provide a starting point for those interested in exploring additional data sources."
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#types-of-data",
    "href": "chapters/ch02-data/ch2-data.html#types-of-data",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "2 Types of data",
    "text": "2 Types of data\n\n2.1 Contextual data\nContextual data refer to data that provide information about the environment in which learning takes place, such as demographic information, socioeconomic status, and prior academic achievement. This type of data can be used to understand how external factors may impact learning, to identify students with similar profiles, and to develop targeted interventions. Demographic data can be used to understand the characteristics of the learners, such as age, gender, race, and ethnicity [3]. Socioeconomic data can be used to examine the impact of the socio-economic status of learners, such as income, employment status, and education level [4]. Prior academic achievement data can be used to understand how the academic background of the learners, such as their previous grades and test scores, may influence their learning at present [5]. The data about the learning context is also relevant to better understand and support students; for example, the level and difficulty of the educational program and courses, format (online vs. face-to-face), or pedagogical approach (e.g., flipped classroom, laboratory course, etc.) [6].\nDescriptive statistics can be used to summarize and describe the main characteristics of the contextual data, such as the mean, median, and standard deviation. In Chapters 3 [7] and 4 [8], we will learn how to clean and manipulate data and how to summarize it using descriptive statistics. In Chapter 6 [9], we will learn how to create different types of plots that will allow us to better understand the data. Cluster analysis can also be used to group similar students together. This can be used to identify patterns in the data and, for example, to understand which different groups of students exist in a course or degree and whether such groups differ in terms of, e.g., performance [10]. We cover clustering in Chapters 8 and 9 [11, 12].\nIt is important to bear in mind that contextual data are essential to understand learners’ and the learning process, but they should be used in combination with other types of data to obtain a comprehensive understanding [13]. It is also crucial to comply with data protection laws and regulations and to consider the ethical implications of collecting and operationalizing this type of data, especially when it comes to the existence of bias when making decisions based on contextual data [14].\n\n\n2.2 Self-reported data\nSelf-reported data refers to data provided by students themselves (or other relevant stakeholders), such as data collected through surveys or questionnaires. This type of data can provide valuable insight into learners’ and teachers’ attitudes, motivation, and perspectives on their learning experiences, and can be used to inform the design of educational programs [15]. It is important to keep in mind that the data should be cleaned and pre-processed before applying any analytical techniques, especially when dealing with qualitative data (e.g., free text, video, or recordings), and the results should be interpreted with caution, keeping in mind the limitations of self-reported data [16].\nRegarding the techniques employed to analyze self-reported data, descriptive statistics and data visualization are commonly used to understand the distribution of responses and to identify patterns in the data (see Chapters 4 [8] and 6 [9]). Moreover, inferential statistics can be used to make inferences about a population based on a sample of data. This can include techniques such as t-tests and analysis of variance to identify significant differences between groups or chi-squared tests to identify associations in the data. Chapter 5 will help us better understand the most common statistical tests and how to implement them with R [17]. Depending on the research question, the type of data, and the level of detail required, a more sophisticated choice of analytical techniques might be needed. For instance, Factor Analysis is a statistical technique that can be used to identify underlying factors or dimensions that explain the relationships between multiple variables [18]. We will learn about it in Chapter 20 [19]. Similarly, Structural Equation Modeling (SEM) can be used to test complex models that involve multiple observed and latent variables that depend on one another. We cover this method in Chapter 21 [20]. Moreover, self-reported data can be analyzed using psychological networks, a relatively new approach in the field of psychology that focuses on understanding psychological phenomena as interconnected networks of individual components. We cover this method in Chapter 19 [21]. Lastly, text mining can be used to analyze unstructured data, such as open-ended responses to surveys or interviews. It can be used to identify key concepts and themes, perform sentiment analysis, and summarize text [22]. This type of analysis is beyond the scope of this book.\n\n\n2.3 Activity data\nActivity data in learning analytics refers to the data that is collected about a student’s interactions with educational technology. Activity data can include information such as the learning resources a student visits, the time spent on a resource, the buttons clicked, and the messages posted [23]. Data can be collected automatically by the learning management system (LMS) or other educational technology (e.g., a game, an intelligent tutoring system, eBooks, or coding environments). Log activity data can be used to track student progress, identify areas where students may be struggling, and personalize instruction [24]. For example, if a student is spending an excessive amount of time on a particular concept, it may indicate that they are having difficulty understanding that concept. In this case, the teacher can provide additional support or re-teach the concept to help the student improve. Log activity data can also be used to measure student engagement with the course content and to identify students who are not engaging with the material [25]. Log activity data have been used to detect students’ online tactics and strategies [26] paying attention not only to the frequency but to the order and timing of students’ events.\nBesides basic analysis using descriptive and inferential statistics, activity logs have been operationalized in many ways in learning analytics, especially using temporal methods that allow to take advantage of the availability of large amounts of timestamped data. For example, process mining — which we cover in Chapter 14 [27] — has been used to investigate how students navigate between different online activities [28]. Sequence analysis has been used to detect and interpret students’ online tactics and strategies based on the order of learning activities within learning sessions [29]. We dedicate several chapters to this technique [30–33]. Such analyses have been complemented with cluster analysis, which allows to detect distinct patterns of students with different online behavior [34] (see Chapters 8 and 9 [11, 12]).\n\n\n2.4 Social interaction data\nSocial interaction data in learning analytics refers to the data collected about students’ interactions with each other (and sometimes teachers too) in a learning environment, social media, or messaging platforms. This can include data such as the frequency and nature of interactions, the content of discussions, and the participation of students in group work or collaborative activities. Social interaction data can be used to understand how students are engaging with each other and to identify patterns or roles that students assume [35]. For example, if a student is not participating in group discussions, it may indicate that they are feeling disengaged or are having difficulty understanding the material. Furthermore, social interaction data can be used to study how students’ depth of contributions to the discussion influences performance [36]. For example, an analysis of social interaction data may reveal that students who receive more replies from other students perform better in the course than students whose contributions do not spark a lot of interest.\nSocial Network Analysis (SNA) is the most common method to study social interaction data. SNA comprises a wealth of quantitative metrics that summarize relationships in a network. In most cases in learning analytics, this network is formed based on students’ interactions. These metrics, named centrality measures, pave the path to apply other analytical methods such as cluster analysis to detect collaboration roles [37], or predictive analytics to determine whether performance can be predicted from students’ centrality measures [38]. We cover the basics of SNA in Chapter 15 of the book [39], community finding in Chapter 16 [40], and temporal network analysis in Chapter 17 [41]. Moreover, the nature and content of students’ interactions can be analyzed with Epistemic Network Analysis (ENA), a method for detecting and quantifying connections between elements in coded data and representing them in dynamic network models [42]. We cover this method in Chapter 18 [43].\n\n\n2.5 Performance data\nPerformance data refers to data that measures how well learners are able to apply what they have learned. This type of data can be used to evaluate the effectiveness of a particular educational activity, to identify areas where additional support may be needed, or to detect students at risk. Performance includes assessment data from tests, quizzes, projects, essays, exams, and other forms of evaluation used to track students’ progress. Assessment can be performed by different entities, such as teachers, peers or automated assessment tools. Moreover, assessment data can have different levels of granularity: it can be the grade for a specific task, a midterm or final exam, or a project; it can be the final grade for a course, or even a whole program GPA [44]. Performance data used for learning analytics may not necessarily be assessment data. For instance, pre-test and post-test data are used to evaluate the effectiveness of a particular educational intervention [45]. Another example is the data captured by audience response systems (ARSs) [46], which are often used to evaluate learners’ knowledge retention during lectures.\nPerformance data are rarely analyzed on its own, but rather used in combination with other sources of data. For example, a common use case in learning analytics is to correlate or predict grades with indicators from several sources [13, 47], such as demographic data, activity data or interaction data. In the book, we cover predictive modelling in Chapter 7 [48]. Moreover, grades are often compared among groups or clusters of students, for example, to evaluate the performance of students that use different online learning strategies [29] or to establish whether students’ assuming different levels of collaboration also show differences in performance [49]. Clustering is covered in Chapters 8 and 9 [11, 12].\n\n\n2.6 Other types of data\nIn recent years, the landscape of data used for learning analytics has undergone a remarkable expansion beyond demographics, grades, surveys and digital logs [50]. This evolution has led to the incorporation of novel methodologies designed to capture a more holistic understanding of students’ learning experiences, including their physiological responses [51]. This progression encompasses a diverse range of data acquisition techniques, such as eye-tracking data that traces the gaze patterns of students, electrodermal activity which measures skin conductance and emotional arousal, EEG (electroencephalogram) recordings that capture brain activity patterns, heartbeat analysis reflecting physiological responses to learning stimuli, and motion detection capturing physical movements during learning activities [52]. These physiological datasets are often combined with other forms of information, such as video recordings (e.g., [50]). By amalgamating various data modalities, researchers and educators can unlock a deeper and more nuanced comprehension of how students engage with educational content and respond to different teaching methodologies [51]. This intricate analysis goes beyond the capabilities of conventional digital learning tools, offering insights into the emotional, cognitive, and physical aspects of learning that might otherwise remain concealed [53, 54]. This synergistic analysis of multiple data sources is often referred to as “multimodal learning analytics”. In Chapter 13, we will cover multi-channel sequence analysis, a method suitable for analyzing several modalities of data at the same time [33]."
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#dataset-selection",
    "href": "chapters/ch02-data/ch2-data.html#dataset-selection",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "3 Dataset selection",
    "text": "3 Dataset selection\nThe present section describes a set of curated datasets that will be used throughout the book. In the introductory chapters, the reader will learn how to import datasets in different formats [7], clean and transform data [8], conduct basic statistics [17], and create captivating visualizations [9]. Each of the remaining chapters covers a specific method, which is illustrated in a tutorial-like way using one or more of the datasets described below. All the datasets are available on Github (https://github.com/lamethods/data).\n\n3.1 LMS data from a blended course on learning analytics\n Link to the dataset\nThe first dataset in our book is a synthetic dataset generated from on a real blended course on Learning Analytics offered at the University of Eastern Finland. The course has been previously described in a published article [55] which used the original (non-synthetic) dataset. The lectures in the course provided the bases for understanding the field of learning analytics: the recent advances in the literature, the types of data collected, the methods used, etc. Moreover, the course covered learning theories as well as ethical and privacy concerns related to collecting and using learners’ data. The course had multiple practical sessions which allowed students to become skilled in learning analytics methods such as process mining and social network analysis using real-life datasets and point-and-click software.\nStudents in the course were required to submit multiple assignments; most of them were practical, in which they had to apply the methods learned in the course, but others were focused on discussing learning theories, ethics, and even conducting a small review of the literature. The course had a final project that accounted for 30% of the course final grade in which students had to analyze several datasets in multiple ways and comment and discuss their findings. Moreover, there was a group project in which students had to present an implementation of learning analytics application in an institutional setting, discussing the sources of data collection, the analyses that could be conducted, and how to present and make use of the data and analyses. The course was implemented in a blended format: instruction was face-to-face while the learning materials and assignments were available online, in the Moodle LMS. Discussions among students in the group project also took place online in the LMS forum.\nThe dataset contains four files: a file containing students’ online activities in Moodle, a file containing their grades, a file containing their demographic data, and a file that aggregates all the information. It is shared with a CC BY 4.0 license, which means that anyone is free to share, adapt, and distribute the data as long as appropriate credit is given. The dataset has been used in the introductory chapters of the book to learn the basics of R [7], data cleaning [8], basic statistics [17] and data visualization [9]. Moreover, it has been used in two additional chapters to illustrate to well-known learning analytics methods: sequence analysis [30] and process mining [27]. Below, we provide further details on each of the files of the dataset.\n\n3.1.1 Events\nThe Events.xlsx file contains 95,580 timestamped Moodle logs for 130 distinct students. The activities include viewing the lectures, discussing on forums, and working on individual assignments, as well as discussion in small groups, among other events. The logs were re-coded to balance granularity with meaningfulness, i.e., grouping together logs that essentially represent the same action. For example, the activities related to the group project were all coded as Group_work, log activities related to feedback were coded as Feedback, logs of students’ access to practical resources or assignments were coded as Practicals, social interactions that are unrelated to learning were coded as Social, etc. Below we describe the columns of the dataset and show a preview. In Figure 2.1, we show the distribution of events per student.\n\nEvent.context: Resource of the LMS where the event takes place, for example “Assignment: Literature review”.\nuser: User name in the LMS.\ntimecreated: Timestamp in which each event took place, ranging from September 9th 2019 to October 27th 2019.\nComponent: Type of resource involved in the event. There are 13 distinct entries, such as Forum (39.11%); System (34.33%); Assignment (15.50%) and 10 others.\nEvent.name: Name of the event in Moodle. There are 27 distinct entries, such as Course module viewed (35.89%); Course viewed (26.28%); Discussion viewed (13.77%) and 24 others.\nAction: Column coded based on the combination of the event name and context. There are 12 distinct entries, such as Group_work (34.25%); Course_view (26.45%); Practicals (10.48%) and 9 others.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 1. Number of actions per student per type\n\n\n\n\n\n\n3.1.2 Demographics\nThe Demographics.xlsx file contains simulated demographic data on the students, including name, date of birth, gender, location (on-campus vs. remote student), and employment status. Below, we describe the columns of the dataset and show a preview of the data.\n\nuser: User identifier in the learning management system, with 130 distinct entries.\nName: User’s first name.\nSurname: User’s last name.\nOrigin: Country of origin.\nGender: User’s gender: F (Female, 50%) or M (Male, 50%).\nBirthdate: Date of birth.\nLocation: Whether the student is on campus or studying remotely, with 2 distinct entries, such as On campus (81.54%); Remote (18.46%).\nEmployment: Whether the student is working part-time, full-time or not at all, with 3 distinct entries, such as None (70.77%); Part-time (25.38%); Full-time (3.85%).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.1.3 Results\nPerformance data is provided in the Results.xlsx file, including grades per assignment and the overall course grade. Below we describe the dataset columns and show a preview of the data (the column names have been abbreviated in the preview). Figure 2.2 shows the distribution of grades per graded item.\n\nuser: User name in the learning management system (it matches the previous files).\nGrade.SNA_1: Grade of the first SNA assignment (0-10).\nGrade.SNA_2: Grade of the second SNA assignment (0-10).\nGrade.Review: Grade of the studies’ review assignment (0-10).\nGrade.Group_self: Individual grade of the group project (0-10).\nGrade.Group_All: Group grade of the group project (0-10).\nGrade.Excercises: Grade of the practical exercises (0-10).\nGrade.Project: Final project grade (0-10).\nGrade.Literature: Grade of the literature review assignment (0-10).\nGrade.Data: Grade of the data analysis assignment (0-5).\nGrade.Introduction: Grade of the introductory assigment (0-10).\nGrade.Theory: Grade of the theory assignment (0-10).\nGrade.Ethics: Grade of the ethics assignment (0-10).\nGrade.Critique: Grade of the critique assignment (0-10).\nFinal_grade: Final course grade (0-10).\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 2. Grade per student and graded item\n\n\n\n\n\n\n3.1.4 AllCombined\nThis file AllCombined.xlsx contains the students’ demographics, grades, and frequency of each action in the LMS. The columns from students’ demographic data and grades remain the same, while the event data has been grouped per student for each type of event (Action column in the Events dataset), i.e., there is a new column for each type of event that contains the number of events of that type that each student had. Moreover, a new column AchievingGroup separates the high achievers from the low achievers. Below we show a preview of the new columns of the data (the column names have been abbreviated) that were not shown in the previous previews and describe them.\n\nFrequency.Applications: Number of events related to the “Applications” resource.\nFrequency.Assignment: Number of events related to the assignments’ submission.\nFrequency.Course_view: Number of visits to the course main page.\nFrequency.Feedback: Number of views of the assignment feedback.\nFrequency.General: Number of events related to general learning resources.\nFrequency.Group_work: Number of events related to the group work.\nFrequency.Instructions: Number of events related to assignment instructions.\nFrequency.La_types: Number of events related to the “LA types” resource.\nFrequency.Practicals: Number of events related to the practicals.\nFrequency.Social: Number of events related to the forum discussion.\nFrequency.Ethics: Number of events related to the “Ethics” resource.\nFrequency.Theory: Number of events related to the “Theory” resource.\nFrequency.Total: Number of events overall.\nAchievingGroup: Categorization as high achievers (top 50% grades) and lows achievers (bottom 50% grades).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 3. Relationship between total frequency of events and final grade\n\n\n\n\n\n\n\n\n\n\n3.2 LMS data from a higher education institution in Oman\n Link to the dataset\nThe next dataset includes students’ log data enrolled in computing specialization a higher education institution in Oman. The dataset contains data from students enrolled in the sixth semester or beyond. The data was recorded across five modules (Spring 2017-2021) and includes the records of 326 students with 40 features in total, including the students’ academic information (24 features), logs of students’ activities performed on the Moodle LMS (10 features), and students’ video interactions on the eDify mobile application (6 features). The academic data includes demographic data, academic data, study plan, and academic violations. The Moodle activity data includes students’ timestamped activities in Moodle. The eDify data contains video interactions in the eDify mobile application.\nThe dataset has been described in an article by Hasan et al. [56] and was originally made available in the Zenodo repository [57] with a CC BY 4.0 license, which means anyone can share and adapt the dataset but must give credit to the author and cannot apply any further restrictions to the dataset. Besides the raw data, the dataset includes a processed file that contains a series of indicators that can be used for different purposes such as predictive modeling or clustering of students’ profiles. It has been used in several publications with the purpose of predicting student performance using different algorithms and approaches [58, 59]. The main files of the dataset are described below.\n\n3.2.1 Student academic information\nStudents’ academic information downloaded from the institutional information system. The data are spread in 15 files (starting with KMS), but have been combined in a single file (KMSmodule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “KMS Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nModuleCode: Module identifier (Module 1-5).\nModuleTitle: Name of the module (Course 1-5).\nSessionName: Class section in which the student has been enrolled (Session-A or Session-B).\nRollNumber: Student identifier (e.g., “Student 83”). There are 306 distinct students.\nCGPA: Students’ cumulative GPA (1-4).\nAttemptCount: Number of attempts per student and module.\nAdvisor: Students’ advisor identifier (e.g., ‘Advisor 16’). There are 50 distinct advisors.\nRemoteStudent: Whether the student is studying remotely. There are 2 possible values: Yes (0.31%) or No (99.69%).\nProbation: Whether the student has previous incomplete modules. There are 2 possible values: Yes (3.36%) or No (96.64%).\nHighRisk: Whether the module has a high risk of failure. There are 2 possible values: Yes (6.73%) or No (93.27%).\nTermExceeded: Whether the student is progressing in their degree plan. There are 2 possible values: Yes (1.83%) or No (98.17%).\nAtRisk: Whether the student has failed two or more modules in the past. There are 2 possible values: Yes (23.55%) or No (76.45%).\nAtRiskSSC: Whether the student has been registed for having any educational deficiencies. There are 2 possible values: Yes (4.59%) or No (95.41%).\nOtherModules: Number of other modules the student is enrolled in on the same semester.\nPrerequisiteModules: Whether the student has enrolled in the prerequistide module.\nPlagiarismHistory: Modules for which the student has a history of plagiarism.\nMalPracticeHistory: Modules for which the student has a history of academic malpractice.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.2 Moodle\nThe Moodle data is spread around 15 files (starting with Moodle), which contain all the clicks that students performed in the Moodle LMS in each module. The 15 files have been combined in a single file (moodleEdify.RDS) for convenience. Below is a preview of the data and a description of its structure:\n\ncsv: Module offering identifier. There are 15 distinct entries (one for each file).\nTime: Timestamp in which the event occurred, ranging between January 1st 2018 to December 9th 2020.\nEvent context: Resource of the LMS where the event takes place, for example “File: Lecture 4”.\nComponent: Type of resource involved in the event. There are 16 distinct entries, such as System (51.71%); File (22.30%); Turnitin Assignment 2 (15.04%) and 13 others.\nEvent name: Name of the event in Moodle. There are 70 distinct entries, such as Course viewed (36.59%); Course module viewed (25.98%); List Submissions (11.08%) and 67 others.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Activity\nAggregated information per student based on the Moodle data, including the activity on campus and at home. The data are also spread in 15 files (starting with Activity), but has been combined in a single file (ActivityModule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Activity Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”).\nOnline C: Duration of the activity (in minutes) within campus.\nOnline O: Duration of the activity (in minutes) off campus.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.4 Results\nStudent performance data in each module. The data are also spread in 10 files (starting with Result), but has been combined in a single file (ResultModule.RDS) for convenience.Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Result Module 1 F19.csv”). There are 10 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”). There are 326 distinct students.\nSessionName: Class section, Session-A (84.66%), or Session-B (9.82%).\nCW1: Grade of students’ first assignment (0-100).\nCW2: Grade of students’ second assignment (0-100).\nESE: Grade of students’ end-of-semester examination (0-100).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.5 eDify\nStudent aggregated activity data in the eDify mobile application. The data are also spread in 15 files (starting with VL), but has been combined in a single file (VLModule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Result Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”). There are 326 distinct students.\nPlayed: Number of times the student has played a video.\nPaused: Number of times the student has paused a video.\nLikes: Number of times the student has liked a video.\nSegment: Number of times a student has scrolled to a specific part of the video.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n3.3 School engagement, academic achievement, and self-regulated learning\n Link to the dataset\nThis dataset includes measures of school engagement, self-regulation and academic performance of a group of primary school students in northern Spain [60]. The data contains responses to the questionnaire from 717 primary education students. The subjects were recruited using convenience sampling from 15 schools (5 privately funded and 10 publicly funded) in northern Spain. The objective of collecting these data was to characterize school engagement and to explore to which extent different engagement profiles are associated with academic performance and self-regulation. In the questionnaire, engagement was assessed with the school engagement measure [61], which allows to differentiate between behavioral, cognitive and emotional engagement. Self-regulation was assessed using the self-regulation strategy inventory [62], which allows measuring students’ approaches to seeking and learning information, maladaptive regulatory behavior, environment management, and time management. The measure used for achievement was students’ self-reported information about their grades in Spanish and mathematics on a scale of 1 (fail) to 5 (outstanding).\nFigure 2.4 summarizes the responses of the students in both subjects, Figure 2.5 presents a set of histograms with the engagement measures, and Figure 2.6 includes a set of histograms with the self-regulation measures. The dataset was analyzed in a previous article [63], where the authors carried out cluster analysis using LPA (Latent Profile Analysis) to identify different groups of students according to their engagement and self-regulation and to compare the performance between these groups. The dataset is used in Chapter 9 [11] of this book, which covers model-based clustering. The dataset is published with a CC BY 4.0 license, which means that you can share, copy and modify this dataset so long as appropriate credit is given. Below we show a preview of the dataset and describe its main variables.\n\nalumno: Student identifier in the school.\nsexo: Student’s gender (1 = Male, 48.40%; 2 = Female, 51.46%).\ncoleg: School identifier (1-15).\ncurso: Grade that students were in 5th grade (62.48%) or 6th grade (37.52%).\ngrup: Class section (1-3).\nren.mat: Mathematics self-reported academic achievement (1-5).\nren.leng: Spanish self-reported academic achievement (1-5).\nEmotion_Engage: Emotional engagement (z-score).\nCognitive_Engage: Cognitive engagement (z-score).\nBehavior_Engage: Behavioural engagement (z-score).\nEnviroment_Manage: Environment management (z-score).\nInformation_help_Manage: Information and help management (z-score).\nMaladapative_Behavior: Maladaptative self-regulation (z-score).\nTime_Manage: Time management self-regulation (z-score).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 4. Self-reported grades of students in Mathematics and Spanish\n\n\n\n\n\n\n\n\n\n\n\n(a) Emotion engagement z-score\n\n\n\n\n\n\n\n(b) Cognitive engagement z-score\n\n\n\n\n\n\n\n(c) Behavior engagement z-score\n\n\n\n\nFigure 5. School engagement results of the survey\n\n\n\n\n\n\n\n\n\n(a) Environment management z-score\n\n\n\n\n\n\n\n(b) Information help management z-score\n\n\n\n\n\n\n\n\n\n(c) Maladaptative behavior z-score\n\n\n\n\n\n\n\n(d) Time management z-score\n\n\n\n\nFigure 6. Self-regulation results of the survey\n\n\n\n\n3.4 Teacher burnout survey data\n Link to the dataset\nThe next dataset presents the responses collected from a survey about teacher burnout in Indonesia [64]. The survey questionnaire contains 18 items in five different categories. The first category contains five items to assess the teacher self-concept (TSC), from the TSC Evaluation Scale [65]. The second category is teacher efficacy (TE), with 5 items adapted from [66]. The remaining categories are Emotional Exhaustion (EE, 5 items), Depersonalization (DP, 3 items), and Reduced Personal Accomplishment (RPA, 5 items), adapted from the Maslach burnout inventory [67]. The survey items were measured using a 5-point Likert scale, where 1 represents “never”, and 5 represents “always”.\nBelow we show a preview of the dataset with the questions and answers, which can be visualized in Figure 2.7 using a Likert scale chart. The dataset has been analyzed using several statistical methods [68]: Content Validity Index (CVI), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), Covariance-Based SEM (CB-SEM). The main aim was to determine the factors that may be predictors of teacher burnout. In this book, we also make use of this dataset to illustrate Factor Analysis [19] and SEM [20]. The files associated with this dataset are licensed under a CC BY 4.0 license, which means you can share, copy and modify this dataset so long as appropriate credit is given to the authors. The variables of the dataset are described below.\n\nTeacher self-concept\n\nTSC1: Response to the item “I think I have good teaching skills and ability”.\nTSC2: Response to the item “I have a reputation for being an efficient teacher”.\nTSC3: Response to the item “My colleagues regard me as a competent teacher”.\nTSC4: Response to the item “I feel I am a valuable person”.\nTSC5: Response to the item “Generally speaking, I am a good teacher”.\n\nTeacher efficacy\n\nTE1: Response to the item “I help my students value learning”.\nTE2: Response to the item “I motivate students who show low interest in schoolwork”.\nTE3: Response to the item “I improve understanding of students who are failing”.\nTE4: Response to the item “I provide appropriate challenges for very capable students”.\nTE5: Response to the item “I get students the students to follow classroom rules”.\n\nEmotional exhaustion\n\nEE1: Response to the item “I feel emotionally drained from my work”.\nEE2: Response to the item “I feel used up at the end of the workday”.\nEE3: Response to the item “I feel fatigued when I get up in the morning and have to face another day on the job”.\nEE4: Response to the item “I feel burnt out from my work”.\nEE5: Response to the item “Working with people all day is really a strain on me”.\n\nDepersonalization\n\nDE1: Response to the item “I’ve become more callous toward people since I took this job”.\nDE2: Response to the item “I worry that this job is hardening me emotionally”.\nDE3: Response to the item “I feel frustrated by my job”.\n\nReduced Personal Accomplishment\n\nRPA1: Response to the item “I cannot easily create a relaxed atmosphere with my students”.\nRPA2: Response to the item “I do not feel exhilarated after working closely with my students”.\nRPA3: Response to the item “I have not accomplished many worthwhile things in this job”.\nRPA4: Response to the item “I do not feel like I’m at the end of my rope”.\nRPA5: Response to question “In my work, I do not deal with emotional problems very calmly”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 7. Results of the burnout survey\n\n\n\n\n\n\n3.5 Interdisciplinary academic writing self-efficacy\n\n Link to the dataset\nThis dataset contains the result of nursing students’ responses to the Situated Academic Writing Self-Efficacy Scale (SAWSES) [69] questionnaire for interdisciplinary students (543 undergraduate and 264 graduate). Participating students were recruited from three higher education institutions and from the general public using social media. The survey contains 16 items based on Bandura’s self-efficacy theory [70] and the model proposed by [71].\nThe questionnaire items are related to three dimensions. The first dimension is Writing Essentials, with three items related to synthesis, emotional control, and language. The second dimension is Relational Reflective Writing, with seven items related to relationship building with writing facilitators and the self through reflection. The third dimension is Creative Identity, with six items related to gaps in student achievement of transformative writing. Demographic data for age, gender, years in post-secondary, English language status, English writing status, and writing attitude are also included.\nThe survey has been validated in a published article [72]. We make use of this dataset in Chapter 8 [12], devoted to clustering algorithms. The dataset is published under the CC0 1.0, which means that anyone can copy, modify, distribute it, even for commercial purposes, without asking permission from the authors. The dataset variables are described below.\nFirst, we describe the demographic variables, which can be previewed below.\n\nAge: Age.\nGender: Student’s gender, 1 = male (24.91%), 2 = female (72.12%), 3 = non-binary (2.97%).\nWritingAttitude: Writing attitude, 1 = dislikes writing (44.73%), 2 = somewhere in between (14.37%), or 3 = likes writing (44.73%).\nTypeStudent: Academic level, 1 = undergraduate (67.29%), 2 = graduate (32.71%).\nUgyears: Undergraduate years in the school for undergraduate students.\nGryears: Graduate years in the school for graduate students.\nWriteEnglish: English writing status (1-5).\nSpeakEnglish: English language status (1-5).\n\n\n\n\n\n\n\n\n  \n\n\n\nNext, we present the questionnaire responses for each of the dimensions. The responses are on a scale of 0-100. Below is a preview of the data, and Figure 2.8 presents a box plot with some the response distribution of the questionnaire.\n\nWriting Essentials\n\novercome: Response to the item “Even when the writing is hard, I can find ways to overcome my writing difficulties”.\nwords: Response to the item “I can successfully use scholarly academic words and phrases when writing in my courses”.\nsynthesize: Response to the item “I can combine or synthesize multiple sources I’ve read to create an original product or text”.\n\nRelational Reflective Writing\n\nmeaning: Response to the item “When I write, I can think about my audience and write so they clearly understand my meaning”.\nimprove: Response to the item “When I receive feedback on my writing, no matter how it makes me feel, I can use that feedback to improve my writing in the future”.\nreflect: Response to the item “When I reflect on what I am writing I can make my writing better”.\nideas: Response to the item “When I read articles about my topic, the connections I feel with the ideas of other authors can inspire me to express my own ideas in writing”.\noverall: Response to the item “When I look at the overall picture I’ve presented in my writing, I can assess how all the pieces tell the complete story of my topic or argument”.\nwander: Response to the item “I can recognize when I’ve wandered away from writing what my audience needs to know and have begun writing about interesting, but unrelated, ideas”.\nadapt: Response to the item “With each new writing assignment, I can adapt my writing to meet the needs of that assignment”.\nfeedback: Response to the item “When I seek feedback on my writing, I can decide when that feedback should be ignored or incorporated into a revision in my writing”.\n\nCreative Identity\n\ncreativity: Response to the item “I can use creativity when writing an academic paper”.\nspark: Response to the item “I feel I can give my writing a creative spark and still sound professional”.\nvoice: Response to the item “I feel I can develop my own writing voice (ways of speaking in my writing that are uniquely me)”.\noriginal: Response to the item “Even with very specific assignment guidelines, I can find ways of writing my assignment to make it original or unique”.\ndiscipline: Response to the item “I can comfortably express the concepts, language, and values of my discipline or major in my writing assignments”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 8. Results of the SAWSES survey\n\n\n\n\n\n\n3.6 Educators’ discussions in a MOOC (SNA)\n Link to the dataset\nThis dataset belongs to two offerings of the MOOC “The Digital Learning Transition in K-12 Schools” [73]. The course was aimed at helping school and district leaders implement digital learning initiatives in K-12 education. The objectives of the course were for participants to understand the advantages of digital learning in schools, to assess the specific goals for their own school, and to devise a plan to achieve such goals. The course consisted of five learning units dealing with the schools of the future, teaching and learning culture, successful digital transition, leading the transition, and crowd sourcing. The MOOCs were offered to American as well as international teachers. There were two offerings of the MOOC, with minor variations regarding duration and groups. The dataset contains the interactions of the MOOC discussion forums and concerns teachers’ communications throughout the courses. The dataset also contains the characteristics of the teachers, e.g., their professional roles, and experience.\nThe dataset is extensively described in a dedicated publication where the authors give details about the context and the courses, the files, and the fields contained in each file [74]. In this book, we use this dataset to illustrate Social Network Analysis [39] and Temporal Network Analysis [41]. The dataset is available with a CC0 1.0 license. Therefore, permission to copy, modify, and distribute, even for commercial purposes, is granted. As a standard Social Network Analysis dataset, it comes in two files for each of the courses (four in total), which we describe in detail below.\n\nEdges file: This file defines who interacted (the source or the sender of the communication) with whom (the target or the receiver of the communication). The edges file comes with other metadata, such as time, discussion topic and group. Below is a preview of one of the edge files, and Figure 2.9 shows the network of collaboration between forum contributors.\n\nSender: Source of the communication identifier (1-445).\nReceiver: Target of the communication identifier (1-445).\nTimestamp: Timestamp of the intervention in “m/d/Y H:M” format, ranging from April 10th 2013 to June 8th 2013.\nDiscussion Title: Title of the discussion.\nDiscussion Category: Category of the discussion.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 9. MOOC participants’ network\n\n\n\n\n\nNodes file: The file defines the characteristics of the interacting teachers, their IDs, their professional status and expertise level. Below is a preview of one of the nodes file data.\n\nUID: Teacher identifier (1-445).\nrole1: Role of the teacher.\nexperience: Level of experience (1-3).\nexperience2: Years of experience.\ncountry: Country of origin.\ngender: Teachers’ gender, female (68.09%); male (31.69%).\nexpert: Level of expertise (0-1).\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.7 High school learners’ interactions (SNA)\n Link to the dataset\nThe next dataset [75] concerns a course of interactions among 30 students in a high school in Kenitra, Morocco. The course under examination had a duration of two months and covered topics related to computer science: the computer information system, algorithmis and programming. The course was implemented in the Moodle LMS, using the forum as a discussion space. Students’ interactions were aimed at communicating, discussing and exchanging knowledge among them.\nThe dataset has been analyzed using social network analysis, is briefly described in an article [76], and is shared under Creative Commons license CC BY 4.0, which means that anyone can share, copy and modify this dataset so long as appropriate credit is given. The dataset includes two files described below.\n\nEdges file: The file contains the interactions source, target and weights. Below is a preview of the edge files. Figure 2.10 presents the graph of all the interactions on the network.\n\nsource: Source node identifier (1-21).\nTarget: Target node identifier (1-21).\nW: Weight of the link (the value is always 1).\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 10. High school learners’ network\n\n\n\n\n\nNodes file: contains the characteristics of the interacting students, e.g., gender and age. Below is a preview of the dataset.\n\nID: Student identifier (1-30).\nUsername: Username of the student.\nname: Name of the student.\ngenre: Gender of the student F (n = 23); M (n = 7).\nDate de naissance: Birthdate of the student in format “D/M/Y”.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.8 Interactions in an LMS forum from a programming course (SNA)\n Link to the dataset\nThis dataset includes message board data collected from a programming undergraduate course in a higher education institution in Spain using the Moodle LMS. The most particular characteristic of the course is that it follows the CTMTC (Comprehensive Training Model of the Teamwork Competence) methodology [77], which allows individualized training and assessment of teamwork across all stages of teamwork-based learning: storming, norming, performing, delivery and documentation [78].\nThis is a mandatory course with a workload of 6 ECTS. The data dates back to the first semester of the 2014-2015 academic year. The course offers foundational knowledge on programming and numeric calculus, has a strong focus on the development of algorithms, and a hands-on approach to teaching. The course starts with a two-hour long introductory session; after this session, students work in class and out of class during the whole semester on a team project, following the CTMTC methodology. Individual evidence of teamwork is collected from forum activity, where the work phases are presented, and group evidence of teamwork is collected from Dropbox and wikis.\nThe dataset refers to individual evidence of teamwork; in other words, it contains information about forum interactions during the course. The original dataset, obtained from Moodle logs, was processed with the help of GraphFES [79] to provide condensed information. The output of GraphFES consists of three different datasets: views, or the number of times user a read a message posted by user b; replies, which informs about the number of replies from user a to user b; and messages, which provides a network with the hierarchical structure of messages in the forum. This dataset has been used previously in [80] and is now being publicly released under a CC 4.0 BY-NC-SA license, which means that anyone is free to share, adapt, and distribute the data as long as appropriate credit is given, it is not used for commercial purposes, and the original license is kept. The dataset is used in Chapter 16 [40] of this book, about community detection. This dataset presents the replies network, a directed graph, and consists of an edges file and a nodes file.\n\n\n\n\nEdges file: this file includes information about who (attribute source) interacted with (replied to) whom (attribute target); in other words, the sender and the receiver of the informational exchange, and how many times that exchange happened during the course (attribute weight), considering all messages exchanged. The dataset includes a total of 662 weighed edges. Below is a preview of the edges file data, and Figure 2.11 represents the complete network of interactions among the Moodle users.\n\nsource: Source node identifier (108 distinct values).\ntarget: Target node identifier (108 distinct values).\nweight: Weight of the link.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNodes file: this file contains information about all users with access to the course in the Moodle space, including students, instructors and administrators. The file includes a total of 124 nodes (users); of these, 110 users are students, distributed in 19 groups of between 5 and 7 members each. The file includes an identifier for each user (attribute id), the username (attribute user; after anonymization, all usernames have the format user_id), the number of initial posts, which refers to the number of first posts in a thread (attribute initPosts), the number of replies, or posts that were a reply to another post (attribute replyPosts) and the total number of posts by that user in the forum (attribute totalPosts), which is the sum of initPosts and replyPosts. It is worth noting that user_55, a central node of the network, corresponds to the main instructor of the course. Below is a preview of the nodes file.\n\nUser: User identifier. There are 124 distinct users.\ninitPosts: Number of first posts in a thread.\nreplyPosts: Number of replies to posts in a thread.\ntotalPosts: Total number of posts by a user in the forum.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 11. Interactions of the users in the Moodle discussion forum\n\n\n\n\n\n\n3.9 Engagement and achievement throughout a study program\n Link to the dataset\nThis dataset contains simulated data of students’ online engagement and academic achievement throughout a study program. The dataset has been simulated based on the results of a published article [81]. The article used students’ logs extracted from a university’s Moodle LMS for all the subjects and for all the students who attended the years: 2015, 2016, 2017 and 2018. The logs were used to derive indicators of engagement, such as frequency of performing learning activities (course browsing, forum consumption, forum contribution, and lecture viewing), session count, total session time, active days and regularity of each activity. Regularity of viewing the course main page, for example, was calculated by dividing main page browse actions for the given student over the total main page browse actions over the course duration; the resulting probabilities are used within a Shannon entropy formula to calculate the entropy.\nThen, Latent Class Analysis was used to cluster students into engagement states for each course: Active, Average or Disengaged. Achievement was measured through course final grades, which were divided into tertiles: Achiever, Intermediate and Low. Hence, for each course, students had an engagement state, and an achievement state. The motivation and process of deriving these states from students’ engagement indicators is explained in Chapter 11 [31].\nThe simulated dataset contains the data for 142 students for 8 sequential courses, including their engagement and achievement states, as well as covariate data. It is shared with a CC BY 4.0 license, which means that anyone is free to share, adapt, and distribute the data, as long as appropriate credit is given. The dataset is used in Chapter 13 [33] of this book, to illustrate multi-channel sequence analysis. A preview of the dataset can be seen below. A visual representation of the evolution of engagement and achievement throughout the program is depicted in Figure 2.13. The dataset contains two files:\n\n3.9.1 Longitudinal engagement indicators and grades\nThe file LongitudinalEngagement.csv contains all of the engagement indicators per student and course (frequency, duration and regularity of learning activities) as well as the final grade. Each column is described below and a preview can be seen below and in Figure 2.12.\n\nUserID: User identifier. There are 142 distinct users.\nCourseID: Course identifier. There are 38 distinct courses.\nSequence: Course sequence for the student (1-8).\nFreq_Course_View: Number of views of the course main page.\nFreq_Forum_Consume: Number of views of the forum posts.\nFreq_Forum_Contribute: Number of forum posts created.\nFreq_Lecture_View: Number of lectures viewed.\nRegularity_Course_View: Regularity of visiting the course main page.\nRegularity_Lecture_View: Regularity of visiting the lectures.\nRegularity_Forum_Consume: Regularity of reading forum posts.\nRegularity_Forum_Contribute: Regularity of writing forum posts.\nSession_Count: Number of online learning sessions.\nTotal_Duration: Total activity time online.\nActive_Days: Number of active days (with online activity).\nFinal_Grade: Final grade (0-100).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 12. Histogram of engagement indicators and final grade throughout the eight courses\n\n\n\n\n\n\n3.9.2 Longitudinal engagement and achievement states\nThe file SequenceEngagementAchievement.xlsx contains students’ engagement and achievement states for each course as well as covariates (their previous grade, their attitude towards learning, and their gender). Engagement states were obtained by applying model-based clustering techniques to the engagement indicators in the previous file. Achievement states were obtained in a similar way for the final grade. The dataset columns are described below and a preview of the data can be seen below and a graphical representation is shown in Figure 2.13.\n\nUserID: User identifier. There are 142 distinct users.\nCourseID: Course identifier. There are 38 distinct courses.\nSequence: Course sequence for the student (1-8).\nEngagement: Engagement state. There are 3 distinct states: Active (29.93%), Average (47.98%), amd Disengaged (22.10%).\nFinal_Grade: Final grade of each student for each course (1-100).\nAchievement: Achievement state calculated using model-based clustering. There are 3 distinct states: Achiever (39.26%), Intermediate (23.33%), and Low (37.41%).\nAchievementNtile: Achievement state calculated using tertiles. There are 3 distinct states: Achiever (33.27%), Intermediate (33.36%), and Low (33.36%).\nPrev_grade: GPA with which the student applied to the program (1-10).\nAttitude: Attitude towards learning (0-20).\nGender: Gender (male/female). There are 44% females and 56% males.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 13. Sequence and engagement for each student across eight courses\n\n\n\n\n\n\n\n3.10 University students’ basic need satisfaction, self-regulated learning and well-being during COVID-19\n Link to the dataset\nThis dataset contains the results of a survey investigating students’ psychological characteristics related to their well-being during the COVID-19 pandemic. The variables under study are related to the satisfaction of basic psychological needs (relatedness, autonomy, and experienced competence), self-regulated learning, positive emotion and intrinsic learning motivation. Moreover, the dataset contains demographic variables, such as country, gender, and age. The data were collected from 6,071 students from Austria and Finland. There are, however, 564 records with missing responses to at least one item.\nThis dataset has been used in a published study to examine the relationships between the different variables using SEM [82]. The dataset has been used in Chapter 19 [21] of this book, to illustrate the implementation of psychological networks. The dataset has been published under a CC BY 4.0 license, which means that you are free to use and adapt the data but you must give appropriate credit, provide a link to the license, and indicate if changes were made. A summary of the responses is in Figure 2.14. Below, we describe each of the dataset columns and provide a preview of its content.\n\nDemographic data\n\ncountry: Country of the student: 0 = Austria (78.60%), 1 = Finland (21.40%).\ngender: Gender of the student: 1 = Female (70.74%), 2 = Male (28.25%), 3 = Other (0.70%).\nage: Age of the student.\n\n\nBelow we describe the items of the questionnaire for each construct. The possible responses are: 1 = strongly agree, 2 = agree, 3 = somewhat agree, 4 = disagree, 5 = strongly disagree.\n\nBasic Psychological Needs: Relatedness\n\nsr1: Response to the item “Currently, I feel connected with my fellow students”.\nsr2: Response to the item “Currently, I feel supported by my fellow students”.\nsr3: Response to the item “Currently, I feel connected with the people who are important to me (family, friends)”.\n\nBasic Psychological Needs: Competence\n\ncomp1: Response to the item “Currently, I am dealing well with the demands of my studies”.\ncomp2: Response to the item “Currently, I have no doubts about whether I am capable of doing well in my studies”.\ncomp3: Response to the item “Currently, I am managing to make progress in studying for university”.\n\nBasic Psychological Needs: Autonomy\n\nauto1: Response to the item “Currently, I can define my own areas of focus in my studies”.\nauto2: Response to the item “Currently, I can perform tasks in the way that best suits me”.\nauto3: Response to the item “In the current home-learning situation, I seek out feedback when I need it”.\n\nPositive Emotion\n\npa1: Response to the item “I feel good”.\npa2: Response to the item “I feel confident”.\npa3: Response to the item “Even if things are difficult right now, I believe that everything will turn out all right”.\n\nIntrinsic learning motivation\n\nlm1: Response to the item “Currently, doing work for university is really fun”.\nlm2: Response to the item “Currently, I am really enjoying studying and doing work for university”.\nlm3: Response to the item “Currently, I find studying for university really exciting”.\n\nSelf-regulated learning\n\ngp1: Response to the item “In the current home-learning situation, I plan my course of action”.\ngp2: Response to the item “In the current home-learning situation, I think about how I want to study before I start”.\ngp3: Response to the item “In the current home-learning situation, I formulate learning goals that I use to orient my studying”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 14. Results of COVID-19 well-being survey"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#discussion",
    "href": "chapters/ch02-data/ch2-data.html#discussion",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "4 Discussion",
    "text": "4 Discussion\nIn this chapter, we have provided an overview of the types of data operationalized in learning analytics research. Our journey encompassed a wide spectrum of data types, ranging from foundational demographic information to the intricate footprints left by the interactions of students with online learning technologies, including clicks, activities, social interactions, and assessment data. We have pointed to some of the most commonly employed analytical techniques for each type of data and we referred the reader to the chapters of the book that have covered each type of analysis. Thereafter, we presented a meticulous curation of illustrative datasets. We have described each dataset in detail, describing and representing the relevant variables. We also acknowledged the ways each dataset have been analyzed throughout the remaining chapters of the book.\nWe must disclose that collecting learners’ data is not an easy endeavor. First and foremost, it is crucial to consider the ethical implications of collecting and using different types of data, and to comply with data protection laws and regulations [14]. Moreover, it is important to ensure the data quality to draw relevant conclusions from the data, especially in scenarios where data come from heterogeneous sources and are provided in large quantities [83], such as in the educational field [84]. These requirements make finding good-quality open datasets online extremely challenging. In this regard, we hope that the selection offered in this chapter is useful for the reader beyond the scope of the book. A few articles have offered other dataset collections suitable for learning analytics [85] or educational data mining [86]. Moreover, the reader is encouraged to consult open data repositories where datasets are continuously published in multiple fields: Zenodo (https://zenodo.org), US Department of Education Open Data Platform (https://data.ed.gov), Harvard Dataverse (https://dataverse.harvard.edu), European Data Portal (https://data.europa.eu), Mendeley Data (https://data.mendeley.com), openICPSR (https://www.openicpsr.org), Google Dataset Search (https://datasetsearch.research.google.com), figshare (https://figshare.com), Open Science Framework (https://osf.io), or data.world (https://data.world).\nAs we venture deeper into the subsequent chapters of this book, the groundwork laid in this chapter will serve as a solid foundation. The multifaceted nature of learning analytics research calls for a holistic understanding of data types and their implications, and this chapter has acted as a compass, guiding us through this intricate terrain. As the field of learning analytics continues to evolve, the insights gained from these data sources promise to unlock new dimensions of understanding in education and beyond."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html",
    "href": "chapters/ch03-intro-r/ch3-intor.html",
    "title": "3  Getting started with R for Education Research",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#introduction",
    "href": "chapters/ch03-intro-r/ch3-intor.html#introduction",
    "title": "3  Getting started with R for Education Research",
    "section": "1 Introduction",
    "text": "1 Introduction\nR is a free, versatile, and open source programming language and software environment specifically designed for statistical computing and data analysis. R has a vast library of packages that enable data manipulation, visualization, modeling and machine learning [1]. Such a wealth of packages enable a wide range of functionalities that saves the users time, effort or the need to write complex code. The fact that R is freely available makes all these functionalities accessible to all. R has a large community of developers, users and researchers who support the development of the platforms as well as provide support and shared knowledge on popular sites such as StackExchange. Thereupon, R is becoming an increasingly popular choice for students, researchers and data scientists [2].\nBeing open source and accessible to researchers, several packages are added continuously to expand the possibilities and functions that R offers. Some of the R packages included in this book have been added by researchers during the last few years to address contemporary scientific problems and state-of-the-art innovations [3]. For example, R software packages for the analysis of psychological networks were developed in the past five years and ever since have grown tremendously due to contributions from a large base of researchers [4].\nAlthough many of the methods described in this book can be implemented with other software tools, it is hard to find a comprehensive platform that can be used to perform practically all the existing learning analytics methods with such maturity, performance and range of possibilities. For instance, Social Network Analysis (SNA) can be performed with several programming languages (e.g., Python) and desktop software applications (e.g., Gephi) [5]. However, both options provide limited capabilities compared to what R provides for the analysis of SNA. This includes a wider range of SNA centrality measures, mathematical models, community finding algorithms and generative models. Sequence analysis is another example in which the possibilities offered by R are hard to match with other software solutions.\nThis book does not make any assumptions about the superiority of R over any other platform. Other languages and software platforms are indeed very helpful and have vast capabilities for researchers. For instance, Python has remarkable tools for machine learning and Gephi offers beautiful graphics for the visualization of networks. Oftentimes, readers may need to learn or use other tools to do specific tasks. Put another way, where R offers a rich toolset for researchers, there is a space for other tools that researchers can use to accomplish certain tasks. In summary, investing time in learning R is a worthwhile endeavor that helps interested researchers to perform and expand their research skills and toolset. Since R is a large platform, it represents a doorway to the vast capabilities of its ever expanding repertoire of functions and packages."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#learning-r",
    "href": "chapters/ch03-intro-r/ch3-intor.html#learning-r",
    "title": "3  Getting started with R for Education Research",
    "section": "2 Learning R",
    "text": "2 Learning R\nThe goal of programming is to write, i.e. code, a program that performs a desired task. A program consists of several commands, each of which does something very simple. In the statistics and data analysis context, R is typically used to write short programs called scripts. R is therefore not intended for developing games or other complicated programs. R is also not a language originally intended for web programming, although with the right packages you can also make web applications with R.\nR is a high-level programming language. This means that there are many ready-made commands in R, which have much more code “underneath” that the R programmer does not have to touch. For example, a statistical t-test requires several mathematical intermediate steps, but an R programmer can perform the test with a single command (t.test) that provides all the necessary computations and information about the test.\nThe best way to learn how to use and program R code is by doing. This text has R code embedded between the text in gray boxes, as in the example below. Lines starting with two hashes, i.e. ##, are not code, but output generated by running the code. Let’s first take the classic “Hello, world!” command as an example:\n\nprint(\"Hello, world!\")\n\n[1] \"Hello, world!\"\n\n\nThe print function prints the given text to the console. It is convenient, for example, for testing the operation of a program and monitoring the progress of a longer program. R can also be used as a calculator. In the example below, we calculate the price of a product after a 35% discount that was originally priced at 80 euros.\n\n80 * (1 - 0.35)\n\n[1] 52\n\n\nHowever, running individual commands is usually not useful unless the results can be saved somewhere. In programming languages, data is stored in variables, which you will become familiar with later."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#rstudio",
    "href": "chapters/ch03-intro-r/ch3-intor.html#rstudio",
    "title": "3  Getting started with R for Education Research",
    "section": "3 RStudio",
    "text": "3 RStudio\nR has a large array of tools and integrated development environments (IDEs) that make writing and managing code easier and more accessible. The most widely used R IDE is RStudio, which is a free open source software that — similarly to R — runs on all major operating systems [6]. RStudio provides a comprehensive and user-friendly interface for writing, running, and debugging R code, which makes it easier for users to get started and become more productive. Together, R and RStudio allow for the creation of reproducible research. The code and results can be easily shared and replicated, making R and RStudio great tools for collaboration and transparency.\nR and RStudio can be very useful in analyzing and visualizing different types of data. This can help researchers, educators and administrators make data-driven decisions and improve the learning experience for students. Whether you are analyzing student performance, demographic data, or tracking the effectiveness of instructional interventions, R and RStudio provide a flexible and efficient platform for achieving your goal.\nFirst install R (step 1) and then RStudio Desktop for your operating system. R and RStudio are available for many operating systems. The interface of RStudio shown in Figure 3.1 has the following default components:\n\n\n\nFigure 1. RStudio user interface.\n\n\n\nEditor: The editor is used to write files containing R code, i.e., R scripts. Scripts will be introduced later, but they are simply a collection of R commands that carry out a specific task when placed together, for example analyze the data of a research project or draw figures of the finished results. A new script can be opened from the “File” menu by selecting “New File” and then “R script”. The same can be accomplished via a keyboard shortcut by pressing Ctrl + Shift + N on Windows and Linux. On macOS, users can use Cmd + Shift + N. Also, see the “Keyboard Shortcuts Help” under the “Tools” menu for all the shortcuts available. The code written in the editor can be run line by line by pressing Ctrl + Enter at the line. Several lines can also be selected and run at once. The Source button at the top runs all the code in the current file. R scripts can be saved just like other files and their file extension is .R. All the code you use to analyze your data should be written in scripts. When you save your code in this way, the next time you can simply run the script to perform the desired task instead of rewriting the code from scratch.\nConsole: R commands are executed in the console. If the code written in the editor is run, RStudio automatically executes the commands in the console. In the console, just pressing Enter is enough to execute a line of code. You can try to write a calculation in the console, such as 2 * 3 and press Enter, and the result will be printed in the console. You can also write code in the editor and press Ctrl + Enter to accomplish the same result. Possible messages, warnings and errors are also printed into the console. The main difference between the console and the editor is that the commands written in the console are not saved in any file. So if you want to keep your code, it should be written in the editor and saved in a script file. Commands made during the same session can be scrolled in the console with the up and down arrows. In addition, the command history can be viewed in the History tab in RStudio.\nWorkspace: Displays the variables in the current working environment of the R session.\nFiles: Shows the directory structure of the operating system, by default the working directory.\nPlots: Graphics drawn with R appear here.\nPackages: Here you can manage the installed packages (instructions for installing the packages are below).\nHelp: Here you can browse the R manual with instructions for every R command. You can try running the ?print command in the editor or console, which opens the help page for the print function."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#best-practices-in-programming",
    "href": "chapters/ch03-intro-r/ch3-intor.html#best-practices-in-programming",
    "title": "3  Getting started with R for Education Research",
    "section": "4 Best practices in programming",
    "text": "4 Best practices in programming\nAs the word “script” already suggests, data analysis often requires a “script” of various steps, such as reading measurements from a file, organizing a table, or describing the results of an analysis, for example by calculating the averages of different sample groups and drawing figures. Writing the R commands that perform these steps in a script provides a documentation of how the analysis was done, and it’s also easy to come back to and possibly add a few additional steps, such as performing a t-test. The script file is also easy to share with others who work with similar data analysis tasks, because with small modifications (file names, table structure) the same code also works for different datasets and workflows. Comments are often used inside the code. Comments are text written along the R commands, which are not written in a programming language, and which are ignored when running the code. The purpose of comments is to describe the behavior and purpose of the code. It is good to practice to comment your own code from the beginning, even if the code for the first tasks is very simple. In R, comments are marked with the ## symbol.\n\n## Assign arbitrary numbers to two variables\nx <- 3\ny <- 5\n## Sum of two variables\nz <- x + y\n## Print the results\nz\n\n[1] 8\n\n\n\n4.1 R Markdown\nWhile scripts can only store code and comments, a more comprehensive format called R Markdown is also available in RStudio. R Markdown is an extension of the Markdown markup language that allows users to create dynamic reports and interactive notebooks that can integrate text, code, and visualizations. R Markdown documents are created in a plain text format and can be rendered into various output formats, such as HTML, PDF, Word, or even presentations. To create a new R Markdown document in RStudio, go to the “File” menu, select “New File” and finally “R Markdown”. In the dialog that opens, choose the output format you want to use, and give your document a title. Figure 3.2 shows the RStudio editor panel for a default R Markdown document of R Studio.\n\n\n\nFigure 2. An example R Markdown document in R Studio.\n\n\nThe YAML metadata wrapped between the --- delimiters at the top of the document can be used to customize the output and contents in various ways. For example, you can change the title, author, or add a table of contents. In this example we have defined the title, the output format (HTML) and the date, but many more options are available besides these. Use Markdown syntax to format your text, and enclose your R code in code chunks with the ```{r} and ``` tags which can also be provided other options. For example, our first code chunk has been given the label setup and the following option include = FALSE means that this chunk will be executed but its output will not be printed into the final document. Within the chunk, a global knitr option is set so that all code chunks will print (echo) their output by default. You can also use inline chunks to quickly reference the results of computations or other R objects. Some examples of standard markdown syntax in the document are second level headers marked with ## (note that # is not used for comments in an R Markdown document) and bold text denoted by wrapping the text with **. Once you are satisfied with your document, you can render it into the chosen output format by clicking the “Knit” button in RStudio, or use the keyboard shortcut “Ctrl + Shift + K”. Figure 3.3 shows the corresponding rendered HTML document.\n\n\n\nFigure 3. The example R Markdown document rendered into HTML.\n\n\nThis simple example shows only a fraction of the full features of R Markdown documents and the Markdown syntax. R Markdown is a powerful tool for creating reproducible research reports, teaching materials, or even websites. It allows users to integrate code and output seamlessly into their written work, making it easier to share and reproduce analyses. As an alternative to R Markdown documents, R Studio also supports the creation of R Notebooks, which are in essence interactive R Markdown documents. R Notebooks can be useful for example when the goal is not to produce one comprehensive analysis report but instead to keep track of the code and try out various approaches to a problem interactively. For an in-depth guide to R Markdown, see [7] which is also freely available online at https://bookdown.org/yihui/rmarkdown/.\n\n\n4.2 How is code developed?\nCode development typically follows similar steps:\n\nDesign parts of the code.\nStart by writing a small piece of code.\nTest whether the code you wrote works. If it doesn’t, find out why and fix it.\nGo to the next piece of code and continue accordingly, always testing piece by piece whether your code works.\n\nAlong with this material, many packages include a “Cheat Sheet” as a summary of basic tasks and functions related to the package. Cheat sheets provide a quick and easy reference for checking how something is done in R if you don’t remember it by heart. There are cheat sheets for various R packages and other entities on the Internet e.g., the base R cheat sheet, or the tidyr [8] cheat sheet.\nIn addition to the basic commands presented in this chapter of the book, practical R programming relies to a great extent on the use of various packages developed by the scientific community. Packages are collections of code that contain new functions, classes and data, i.e., they extend R. Most R packages are available from the Comprehensive R Archive Network (CRAN). They can be installed with the install.packages() function, or via RStudio’s installation window which in practice calls the install.packages() function. You can also install several packages at once. The command below installs the dplyr [9] and tidyr packages:\n\ninstall.packages(c(\"dplyr\", \"tidyr\"))\n\nIn order to use the commands contained in an R package, the package must be installed and attached to the R workspace. This is done with the library() command:\n\nlibrary(\"tidyr\")\n\nNow that the tidyr package is loaded, we can use the commands it provides, for example to manage the learning analytics data in data frame format that we will address later. If you don’t want to attach the entire package, you can use individual commands from packages with the format name_of_the_package::name_of_the_command()."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-operations",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-operations",
    "title": "3  Getting started with R for Education Research",
    "section": "5 Basic operations",
    "text": "5 Basic operations\nBasic operations in R consist of arithmetic operations, logical operations, and assignment. In addition, there are several commands that are often helpful when starting a new project or managing the working directory. For example, the current working directory can be obtained with the following command:\n\ngetwd()\n\n[1] \"/home/sonsoles/labook/chapters/ch03-intro-r\"\n\n\n\n5.1 Arithmetic operators\nR can be used to compute basic arithmetic operations such as addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (^). These operations follow standard precedence rules, and additional brackets can be added to control the evaluation order if needed.\n\n1 + 1 ## Addition\n\n[1] 2\n\n2 - 1 ## Subtraction\n\n[1] 1\n\n2 * 4 ## Multiplication\n\n[1] 8\n\n5 / 2 ## Division\n\n[1] 2.5\n\n2 ^ 4 ## Exponentiation\n\n[1] 16\n\n\n\n\n5.2 Relational operators\nRelational operators compare objects or values to other objects or values. These operators are often required for conditional data filtering, for example when selecting a subset of individuals that satisfy some criterion. In R, there are six such operators: smaller than, greater than, smaller or equal to, greater or equal to, equal to, and not equal to. There operators have the following syntax in R:\n\n1 < 2  ## Smaller than\n\n[1] TRUE\n\n3 > 2  ## Greater than\n\n[1] TRUE\n\n2 <= 2 ## Smaller or equal to\n\n[1] TRUE\n\n3 >= 3 ## Greater or equal to\n\n[1] TRUE\n\n5 == 5 ## Equal to\n\n[1] TRUE\n\n1 != 2 ## Equal to\n\n[1] TRUE\n\n\nIn the previous example we used these operators to compare integers, but we may also use them to compare other types of values, such as characters:\n\n\"a\" == \"b\"\n\n[1] FALSE\n\n\n\n\n5.3 Logical operators\nSimilar to relational operators, logical operators are used to evaluate the logical value of a conjunction of two logical values. In R, there are five logical operators: negation, AND, OR, elementwise AND, and elementwise OR. These operators have the following syntax:\n\n!TRUE         ## Negation\n\n[1] FALSE\n\nTRUE && TRUE  ## Logical AND\n\n[1] TRUE\n\nTRUE || FALSE ## Logical OR\n\n[1] TRUE\n\nTRUE & TRUE   ## Elementwise AND\n\n[1] TRUE\n\nTRUE | FALSE  ## Elementwise OR\n\n[1] TRUE\n\n\nThe elementwise operators & and | can be used to compare multiple pairs of logical values simultaneously, for example\n\nc(TRUE, FALSE, TRUE) | c(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE  TRUE\n\n\nwhereas the operators && and || only accept single values. In the previous example, we also used one of the most important operations of R: the c() function (the letter ‘c’ is short for “combine”) which we used to combine the logical values into a vector, i.e., an ordered sequence of values of the same type. Vectors and other important data types will be discussed in greater details in the next section.\n\n\n5.4 Special operators\nAnother core functionality in R is the assignment operator <-. Assignment can be used to store the results of computations into variables which can then be used again in other computations without having to redo the original computations. For instance\n\nx <- 5 ## Assign value 5 into variable named x\ny <- 7 ## Assign value 7 into variable named y\nx      ## Access value of x (value is printed into the console in RStudio)\n\n[1] 5\n\ny      ## Access value of y\n\n[1] 7\n\nx + y  ## Compute the sum of x and y\n\n[1] 12\n\nx > y  ## Is x greater than y?\n\n[1] FALSE\n\n\nHere we chose the names x and y for our variables, but the names are arbitrary with the caveat that one should avoid assigning values to objects with names that R already uses internally, such as names of common functions like c(), exp(), or lm() to name a few. Variables currently assigned in the working environment can be displayed with the following command:\n\nls()\n\n[1] \"x\" \"y\" \"z\"\n\n\nIt is also possible to use the equal sign = as the assignment operator, but this is often not recommended, because the equal sign also has other purposes in the R language and may cause confusion if used for assignment. There are also some special instances, where the equals sign does not function identically to <-. Therefore, we recommend always using the standard assignment operator <-.\nWhen constructing vectors, the function c() can be cumbersome in some scenarios. For instance, say we wanted to create a vector that contains all integers from 1 to 100. With c(), we would have to write each value individually. Fortunately, such sequences can be constructed effortlessly using the : (colon) operator:\n\n1:100\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nOften we may wish to select only specific values from a vector. Values in a vector can be accessed by their index, starting from 1. This is accomplished by using the subset operator which uses the following square bracket syntax. For example, we could select the first value of a vector x by writing x[1]. For a more involved example, we could simultaneously select the first, 50th and 100th value of the vector 1:100 by writing:\n\nx <- 1:100\nx[c(1, 50, 100)]\n\n[1]   1  50 100\n\n\nEssentially, we write the positions of the values that we wish to select inside the square brackets as a vector. Alternatively, we can create a vector of logical values that has the same length as the vector we are selecting from, and the value TRUE for the values we wish to select, and FALSE otherwise. In the next section, we also show how to select values based on a condition."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-data-types-and-variables",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-data-types-and-variables",
    "title": "3  Getting started with R for Education Research",
    "section": "6 Basic data types and variables",
    "text": "6 Basic data types and variables\nIn the examples of the previous section, we already used several of the most common data types that users are most likely to encounter in practical data analyses. Each object in R has a type, which can be determined with the function typeof(). Types are used to describe what kind of data our variables of interest contain and what kind of operations can be carried out on them. Perhaps the most common type is the numeric type, which describes values that can be interpreted as numbers. Two special instances of the numeric type are integer and double, which correspond to integer values and decimal values, respectively.\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\nThe capital L on the second line denotes that we mean the integer 1, and not the decimal number 1.0. Text data is often represented by the character type. Unlike in some other programming languages, in R the character type does not necessarily describe individual characters as the name would suggest, but character strings, for instance:\n\ntypeof(\"a\")\n\n[1] \"character\"\n\ntypeof(\"hello world!\")\n\n[1] \"character\"\n\n\nAs we can see, both have the same type.\nThe logical type is used to represent the boolean values TRUE and FALSE. Logical values are typically not as common in actual data where such values may often be represented by the integers 1 and 0 instead. However, their importance lies in forming conditions for data filtering and manipulations that we may wish to carry out based on a criterion that is true for only a subset of subjects in our data. For example, suppose we have the values 1, 2, 3, 4, 5, and we wish to programmatically select only those values that are greater than 2. We can accomplish this as follows:\n\nsome_numbers <- 1:5\nsome_numbers[some_numbers > 2]\n\n[1] 3 4 5\n\n\nLet’s walk through what the code above does step by step. On the first line, we create a vector of numeric values 1 through 5 by using the : operator and assign the result to the variable named some_numbers. On the second line, we use the square brackets (the subset operator) to select only those values of some_numbers for which the condition some_numbers > 2 is true. Here we introduce the vectorization feature of R which applies to a vast majority of arithmetic and relational operators. By writing some_numbers > 2, we actually evaluate the condition for every value in the vector some_numbers:\n\nsome_numbers > 2\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n\nAll of the aforementioned types are called atomic types, meaning that vectors of such values can only contain values of one specific atomic type. For example, a vector cannot contain values of both character and integer types:\n\nc(0L, \"a\")\n\n[1] \"0\" \"a\"\n\n\nWe see that the result was automatically converted to an atomic character vector.\nAlongside type, objects in R also have a class, which can be viewed with the class() function. For atomic types, their class is the same as their type, but for more complicated types of variables, a class essentially describes a special instance of a type. Objects within a specific class typically have their own set of functions or methods that can only be applied for that specific class. A common example of a class is factor. Factors are a special class of integer vectors designed to represent categorical and ordinal data. Variables that are of the factor class have levels, which differentiates factor variables from ordinary integer variables. For example, a factor could represent group membership in a randomized experiment indicating inclusion in the control group or the treatment group for each individual. The levels of this factor could be called \"control\" and \"treatment\" for example. Supposing that 0 indicates that an individual belongs in the control group and 1 indicates that an individual belongs in the treatment group, we could create such a factor by writing\n\ngroup <- c(0, 0, 1, 0, 1, 0, 1, 1)\nfactor(group, labels = c(\"control\", \"treatment\"))\n\n[1] control   control   treatment control   treatment control   treatment\n[8] treatment\nLevels: control treatment\n\n\nFactors are important when fitting statistical models or when performing statistical tests, because if we would simply use the corresponding integer values, they may be erroneously interpreted as continuous. The levels of the factor also often help to provide more informative output. In the next section we will discuss more complicated data types including data frames, which can contain data of various types simultaneously."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-r-objects",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-r-objects",
    "title": "3  Getting started with R for Education Research",
    "section": "7 Basic R objects",
    "text": "7 Basic R objects\nIn this section we will discuss the concepts of data frame and tibble. Let’s assume that your data can be stored in a two-dimensional array where the columns represent variables and each row represents a case of measurement. In R, a data frame is a concept for storing such a two-dimensional array of data. Let’s first study how data frames work in R and we will then move on to see how another concept called tibble extends the capabilities of a data frame.\nA data frame which has been loaded into R under the name grades and printed in the R console will look as follows.\n\n\n\n\n\n\n\ngrades\n\n\n\n  \n\n\n\nIn the above printout we can see that R prints the data frame just as we would expect the data to look like. One issue with a standard data frame is that if there are very many columns or rows, then the printout may be difficult to read. In RStudio, a neater (and more flexible) way of inspecting the data is by using a command called View():\n\nView(grades)\n\n\nTo go further with the data, we need tools for data manipulation. We begin with the very basic tools which are commonplace in any data analysis workflow. More comprehensive knowledge about data transforming, cleaning etc. can be found in Chapter 4.\nA data frame can be constructed directly in the R by using the function called data.frame() and then listing variable names and their values. The grades data above has two columns, group and grade, and each row represents a student. The variable group is indicates the number of a group in which each person has studied. The variable grade stores the final grade that the student has received from a course."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#working-with-dataframes",
    "href": "chapters/ch03-intro-r/ch3-intor.html#working-with-dataframes",
    "title": "3  Getting started with R for Education Research",
    "section": "8 Working with dataframes",
    "text": "8 Working with dataframes\nTo extract a column from a data frame, one needs to start with the name of the data frame object and connect the column name with the name of the data frame object by using the dollar symbol ($). Thus, extracting the column group from grades data can be accomplished by writing\n\ngrades$group\n\n[1] 2 2 3 4 4\n\n\nNext, let’s use a couple of basic functions which are often needed when developing R code. First of all, to get a summary of every variable of a data frame, we can call\n\nsummary(grades)\n\n     group       grade      \n Min.   :2   Min.   :2.630  \n 1st Qu.:2   1st Qu.:3.390  \n Median :3   Median :4.670  \n Mean   :3   Mean   :4.496  \n 3rd Qu.:4   3rd Qu.:4.900  \n Max.   :4   Max.   :6.890  \n\n\n\nThe result show us the minimum, maximum, median, mean and quartiles of both variables. You may note that as group was intended to be a categorical variable, so computing its mean value in the data does not make sense. We can change that behavior by converting the group column into a factor.\n\ngrades$group <- as.factor(grades$group)\nsummary(grades)\n\n group     grade      \n 2:2   Min.   :2.630  \n 3:1   1st Qu.:3.390  \n 4:2   Median :4.670  \n       Mean   :4.496  \n       3rd Qu.:4.900  \n       Max.   :6.890  \n\n\nOn the other hand, we might want to calculate the mean or the sample standard deviation of the variable grade. This can be done with functions called mean() and sd(), respectively.\n\nmean(grades$grade)\n\n[1] 4.496\n\nsd(grades$grade)\n\n[1] 1.630178\n\n\nThe functions above can only take numeric vectors as input. If we tried using another type of argument, we would encounter an error message.\n\nsd(grades)\n\nError in is.data.frame(x): 'list' object cannot be coerced to type 'double'\n\n\nThe above message can actually teach us a couple of things. First of all, the error comes from the function is.data.frame(), which is called somewhere in the definition of sd(). Second, the actual error message tells us that the object which we gave to the function as its argument is a list object. List is an object type of R on top of which data.frame objects have been built. We will describe lists in greater detail later. Further, the message tells us that R has tried to coerce the argument object into the double type. This means that the object we supplied to the function is not of the right type and cannot easily be converted into the proper format. \n\n8.1 tibble\nA tibble is an expansion of data.frame objects which is used in the tidyverse programming paradigm [10]. To use tidyverse, we advise to load the tidyverse [11] (meta)package which loads all key tidyverse packages.\n\n## load tidyverse to use as_tibble\nlibrary(\"tidyverse\")\n## convert a data frame as tibble\ngrades2 <- as_tibble(grades)\n\nNext, let’s see what a tibble looks like when printed in the console\n\ngrades2\n\n\n\n  \n\n\n\nTibbles behave similarly compared to data frames when printed, but they also describe the dimensions of the data and the types of the columns right under the column names. For instance, <fct> refers to a factor column and <dbl> refers to a double column. In order to discover the column types when using data frames, one would need to apply the class() or typeof() function to the columns, or write str(grades) to see the types of the columns and the structure of the data. Another useful property of tibble tables is that if a tibble has a large number of observations or variables, then only the rows or the columns which can fit on to the screen are printed.\nTidyverse and tibbles also support so called lazy evaluation, which is useful when your data is stored in a database, for instance. With lazy evaluation, the commands that you use on your data would be evaluated directly in the database (if possible). Without lazy evaluation, the entire data would be downloaded onto your computer only after which the commands would be evaluated. Lazy evaluation can perform many tasks faster and it can also alleviate memory usage of the computer."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#pipes",
    "href": "chapters/ch03-intro-r/ch3-intor.html#pipes",
    "title": "3  Getting started with R for Education Research",
    "section": "9 Pipes",
    "text": "9 Pipes\nPiping is a fairly recent concept in R and was very rarely used in R code just a few years ago. The concept of a pipe originates from a package called magrittr, and pipes are commonly used under the tidyverse programming paradigm, but the pipe was also later added to base R. The notations for a tidyverse pipe and a native R pipe are %>% and |>, respectively. The idea of a pipe is that you can connect multiple function calls sequentially while keeping the code more readable. Pipes also serve to unnest standard R code which often involves using many nested parentheses, and can quickly become hard to read as one has to read the code based on the order of the operations instead of reading it linearly. For example, consider the following code where we apply a sequence of operations on a numeric vector x.\n\nx <- 1:10\nround(mean(diff(log(x))), digits = 2)\n\n[1] 0.26\n\n\nThis code computes the rounded mean differences of the logarithms of the vector x, however this description does not match the order of operations, where the logarithm is computed first. To accomplish the same result using pipes we would write\n\nx <- 1:10\nx |> log() |> diff() |> mean() |> round(digits = 2)\n\n[1] 0.26\n\n\nHere, the order of operations can be easily read from left to right. Next, we will discuss the use of pipes in more detail.\n\n9.1 magrittr pipe %>%\nLet’s have a look at an example, where we call the summary() function for the grades2 data using the magrittr pipe %>%, and we also define that results should be printed with two digits.\n\ngrades2 %>%\n  summary(digits = 2)\n\n group     grade    \n 2:2   Min.   :2.6  \n 3:1   1st Qu.:3.4  \n 4:2   Median :4.7  \n       Mean   :4.5  \n       3rd Qu.:4.9  \n       Max.   :6.9  \n\n\nIn the code above, the object grades2 is taken by the pipe operator %>% and forwarded to the first argument of the summary() function. The summary() function also has a second argument, which is defined by digits = 2. Thus, the pipe only takes the object mentioned before the pipe operator and forwards it to the function after the pipe as the first free argument. It is very common and recommended to structure R code so that there is only one pipe per row and that a new line is started after each pipe.\nAlthough the above example is easy to understand as we already know the summary() function, there is also a more general way to compute summarized information following the tidyverse style. The function summarise() can be used to compute arbitrary statistics from the data, for example the number of observations (via the function n()) and the mean and the sample standard deviation of the variable grade.\n\n\ngrades2 %>%\n  summarise(\n    n = n(),\n    mean = mean(grade), \n    sd = sd(grade)\n  )\n\n\n\n  \n\n\n\nThe summarise() function also produces a tibble enabling further operations via piping, if desired.\n\n\n9.2 Native pipe |>\nIn R version 4.1, the native pipe |> was introduced to the R language, which does not require any external packages to use. In most scenarios, it does not matter whether the native or the magrittr pipe is used. However, there are two technical differences between the magrittr pipe and the native pipe. First, the magrittr pipe is actually a function\n\nclass(`%>%`)\n\n[1] \"function\"\n\n\nwhile the native pipe is not, and simply converts the written code into a non-piped version, i.e., into a form that one would write without using the pipe:\n\nx <- 1:5\nquote(x |> sum())\n\nsum(x)\n\n\nWe see that providing x to the sum function via the native pipe is exactly the same as writing sum(x) directly. What this means in practice is that the native pipe may have better performance for example when passing a large dataset through a large number of pipes. The reason for this is that the magrittr pipe incurs an additional computational function call overhead each time it is called. The second difference between the pipes is that parentheses have to be provided for function calls when using the native pipe, but they can be omitted when using the magrittr pipe\n\nx %>% sum\n\n[1] 15\n\nx %>% sum()\n\n[1] 15\n\nx |> sum()\n\n[1] 15\n\n\n\nx |> sum ## produces an error\n\nError: The pipe operator requires a function call as RHS (<text>:1:6)"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#lists",
    "href": "chapters/ch03-intro-r/ch3-intor.html#lists",
    "title": "3  Getting started with R for Education Research",
    "section": "10 Lists",
    "text": "10 Lists\nEarlier, we already briefly mentioned lists in the context of data frames. Lists are one the most common types of data in R and they resemble basic vectors in many aspects. Like vectors, a list is an ordered sequence of elements, but unlike vectors, lists can contain elements of different types simultaneously and may even contain other lists. For example, we could construct a list that contains a logical value, a numeric value and a character value using the list() function as follows.\n\ny <- list(TRUE, 7.2, \"this is a list\")\n\nSubsetting a list object works slightly differently compared to vectors. When single brackets are used, a sublist is selected, i.e., a list object that contains the elements at the supplied indices, for example:\n\ny[1:2]\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] 7.2\n\ntypeof(y[1:2])\n\n[1] \"list\"\n\n\nTo extract an actual element of a list, double brackets should be used:\n\ny[[2]]\n\n[1] 7.2\n\n\nThe elements of a list may also be named, which enables subsetting via the dollar sign operator similar to data frames, or by giving the element name in double brackets instead of the index:\n\nz <- list(bool = TRUE, num = 7.2, description = \"this is another list\")\nz$bool\n\n[1] TRUE\n\nz[[\"description\"]]\n\n[1] \"this is another list\"\n\n\nOne benefit of using the dollar sign is that it is not necessary to provide the full element name, unlike when using the double brackets. It is sufficient to provide a prefix of the element name so that the full name can be uniquely determined from the prefix. Because all the names of our elements in the previous list z start with a different letter, the first letter of the name suffices as the prefix. The same functionality also applies when using the dollar sign to select columns of data frames.\n\nz$b\n\n[1] TRUE\n\nz$n\n\n[1] 7.2\n\nz$d\n\n[1] \"this is another list\""
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#functions",
    "href": "chapters/ch03-intro-r/ch3-intor.html#functions",
    "title": "3  Getting started with R for Education Research",
    "section": "11 Functions",
    "text": "11 Functions\nA function is a set of statements that when organized together perform a specific task. Each function in R has a name, a set of arguments, a body and a return object. The name of the function usually describes the purpose of the function. For example, the base R function mean() computes the arithmetic mean of the argument vector. The result is returned as a numeric value.\n\nx <- 1:5\nmean(x)\n\n[1] 3\n\n\nFunctions are often much more complicated, which is why it is often helpful to view the documentation of a function before using it in practice. To view the documentation pages of a function, one can simply write the name of the function prefixed by a question mark.\n\n?mean\n\nIn RStudio, the documentation will open in the “Help” tab in the bottom right pane by default. Functions will only be executed when they are called, i.e., when arguments are supplied to them. Simply writing the function name without parentheses will instead print the body of the function to the console, meaning the code that the function consists of and which is executed if the function is called.\nIn the previous sections, we’ve already familiarized ourselves with some commonly used basic functions such as c(), sd(), and summary(). Base R has a wide range of function to accomplish common tasks needed in data analysis, which is further extended by tidyverse and other R packages. This means that one does not typically have to write their own functions when programming in R. We will explore several of the functions provided by the tidyverse in later chapters."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#conditional-statements",
    "href": "chapters/ch03-intro-r/ch3-intor.html#conditional-statements",
    "title": "3  Getting started with R for Education Research",
    "section": "12 Conditional statements",
    "text": "12 Conditional statements\nSometimes we may only wish to execute a piece of code when a certain condition is met. Conditional statements in R can be defined via the if and else clauses. The if clause evaluates a condition, which is an R expression that evaluates to a single logical value, and if this condition evaluates to TRUE, the expression following the clause if executed. Further, if an else clause is also provided, the expression following else will be executed instead if the condition evaluates to FALSE. As R code, the syntax for these clauses is\n\nif (cond) expr\nif (cond) expr else alt_expr\n\nwhere cond is the condition being evaluated, expr is the expression that will be evaluated if cond == TRUE, and alt_expr will be evaluated if cond == FALSE.\nNote that if will only evaluate a single condition. If cond is a vector, an error will be produced:\n\ncond <- c(TRUE, FALSE)\nif (cond) {\n  print(\"This will not be printed\")\n}\n\nError in if (cond) {: the condition has length > 1\n\n\nAs expected, the error message tells us that the condition contained more than one element when it was evaluated. However, there are often scenarios where we may wish to conditionally select or define values based on a vector of conditions. For such instances, the function ifelse() can be used. This function has three arguments: test, yes, and no. When called, the function will pick those elements of the vector yes for which the logical vector test evaluates to TRUE, and those elements of the vector no for which test evaluates to FALSE:\n\ncond <- c(TRUE, FALSE, FALSE, TRUE)\nx <- 1:4\ny <- -(1:4)\nifelse(cond, x, y)\n\n[1]  1 -2 -3  4"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#looping-constructs",
    "href": "chapters/ch03-intro-r/ch3-intor.html#looping-constructs",
    "title": "3  Getting started with R for Education Research",
    "section": "13 Looping constructs",
    "text": "13 Looping constructs\nIn some cases, we may wish to execute the same piece of code multiple times under varying conditions. Instead of writing the same code multiple times for each condition, we can use a looping construct. There are two types of loops in R: the for loop and the while loop. The main difference between the two loops is that for always executes the code associated with the loop a fixed number of times whereas while will continue executing the code as long as a specific condition remains satisfied. The syntax of these loops is\n\nfor (var in seq) expr\nwhile (cond) expr\n\nIn other words, for will execute the expression expr for every element var in some object seq that can be indexed. For-loops are very general, and can be used to loop over most ordered structures such as vectors and lists. Similarly, while will execute the expression expr as long as the condition cond evaluates to TRUE. Care must be taken when using while-loops to ensure that the condition will eventually evaluate to FALSE, otherwise the loop will simply run indefinitely and the program will be stuck. As an example, we will print the number 1 through 5 to the console using both for and while loops:\n\nx <- 1:5\nfor (i in x) {\n  print(x[i])\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\ni <- 0\nwhile (i < length(x)) {\n  i <- i + 1\n  print(x[i])\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nIn contrast to explicit for and while loops, the so-called apply family of functions can often be a simpler alternative (see ?apply). As the name suggests, these functions apply an operation to each element of a list or a vector (and other more general data structures). For example, the above loop example could also be accomplished with the lapply() function as follows:\n\ny <- lapply(x, print)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#discussion-and-other-resources-for-learning-r",
    "href": "chapters/ch03-intro-r/ch3-intor.html#discussion-and-other-resources-for-learning-r",
    "title": "3  Getting started with R for Education Research",
    "section": "14 Discussion and other resources for learning R",
    "text": "14 Discussion and other resources for learning R\nThe main aim behind this chapter was to introduce R to new users. This chapter is, of course, an initial step and can hardly cover all the basics. Interested users are advised to consult other resources e.g., open access books, tutorials, cheat sheets, and package manuals for more information. An introductory book “An Introduction to R” packaged with each R installation can be accessed from the RStudio Help panel by first selecting “Show R Help” and then selecting “An Introduction to R” under “Manuals”. This book covers a wide range of topics on base R programming in great detail. The book “R for Data Science” by Hadley Wicham and Garret Grolemund provides a comprehensive tutorial on using R for data science under the tidyverse paradigm. The book is free to use and readily available online at https://r4ds.had.co.nz/. As we go further, several questions will emerge and the reader will learn by doing and by consulting the literature and help files. In doing so, the reader will build knowledge and experience that helps advance their skills."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#introduction",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#introduction",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "1 Introduction",
    "text": "1 Introduction\nWhen analyzing data, it is crucial that the data is in a suitable format for the tools you will be using. This makes data wrangling essential. Data preparation and cleaning, such as extracting information from raw data or removing erroneous measurements, must also be done before data is ready for analysis. Data wrangling often takes up the majority of the time spent on analysis, sometimes up to 80 percent. To reduce the amount of work required, it is beneficial to use tools that follow the same design paradigm to minimize the time spent on data wrangling. The tidyverse [1] programming paradigm is currently the most popular approach for this in R.\nThe tidyverse has several advantages that make it preferable over other alternatives such as simply using base R for your data wrangling needs. All packages in the tidyverse follow a consistent syntax, making it intuitive to learn and use new tidyverse packages. This consistency also makes the code more easier to read, and maintain, and reduces the risk of errors. The tidyverse also has a vast range of readily available packages that are actively maintained, reducing the need for customized code for each new data wrangling task. Further, these packages integrate seamlessly with one another, facilitating a complete data analysis pipeline.\n\nTo fully realize the benefits of the tidyverse programming paradigm, one must first understand the key concepts of tidy data and pivoting. Tidy data follows three rules:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nLet’s consider examples of tidy data. For instance, if you have data from a Moodle course where two attempts of an exam for each student are located in a single  column. This example data violates the first rule, because there are two variables in a single column instead of separate columns for each variable. What is needed to make this data tidy is to pivot it to a longer format. The pivoted data would have two rows for each student, both of which are different observations (exam attempts 1 and 2). Thus the pivoted data would not conflict with second rule.\n\nData can also be too long, but in practice, this is much more rare. This can occur if two or more variables are stored on a single column across multiple rows. A key indicators of this is if the different rows of the same column have different measurements units (e.g. lb vs. kg). It may also occur that your raw data has multiple values in a single cell. In these cases, it is necessary to split the cells to extract the necessary information. In a simple case, where you have two values systematically in a one cell, the values can be easily separated into their own columns.\nOverall, using tidyverse and understanding the key concepts of tidy data and pivoting can streamline the data analysis process and make code easier to work with and maintain. The rest of this chapter will guide readers through the process of data cleaning and wrangling with R in the field of learning analytics. We demonstrate how data can be grouped and summarized, how to select and transform variables of interest, and how data can be rearranged, reshaped and joined with other datasets. We will strongly rely on the tidyverse programming paradigm for a consistent and coherent approach to data manipulation, with a focus on tidy data."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reading-data-into-r",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reading-data-into-r",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "2 Reading data into R",
    "text": "2 Reading data into R\nData files come in many formats, and getting your data ready for analysis can often be a daunting task. The tidyverse offers much better alternatives to base R functions for reading data, especially in terms of simplicity and speed when reading large files. Additionally, most of the file input functions in the tidyverse follow a similar syntax, meaning that the user does not have to master every function for reading every type of data individually.\nOften, just before the data can be read into R, user must specify the location of data files by setting a working directory. Perhaps most useful way to do that is to create a project in RStudio and then create a folder called “data” within the project folder. Data files can be put into that folder and user can refer to those files just by telling R-functions relative path of data file (e.g. “data/Final%20Dataset.csv”) while the project takes care of the rest of the path. A more traditional way of setting this, which also works without RStudio, is by using a command such as setwd(\"/home/Projects/LAproject/data/Final%20Dataset.csv\"). Here, a function called setwd() is used to set up a folder into location mentioned in a character string given as its argument. A getwd() lists current working directory, which can also be seen in RStudio just above the Console output.\nSome of the most common text data formats are comma-separated files or semicolon-separated files, both of which typically have the file extension .csv. These files can be read into R using the readr [2] package and the functions read_csv() and read_csv2(), respectively. For instance, we can read a comma-separated file R as follows\n\n\n\nFunctions in readr provide useful information about how the file was read into R, which can be used to assess if the input was successful and what assumptions about the data were made during the process. In the printout above, the read_csv() function tells us the number of rows and columns in the data and the column specification, i.e., what type of data is contained within each column. In this case, we have 17 columns with character type of data, and 4 columns of double type of data. Functions in readr try to guess the column specification automatically, but it can also be specified manually when using the function. For more information about this dataset, please refer to Chapter 2 in this book [3].\nData from Excel worksheets can be read into R using the import() function from the rio [4] package. We will use synthetic data generated based on a real blended course of learning analytics for the remainder of this chapter. These data consist of three Excel files which we will first read into R.\n\nlibrary(\"rio\")\nurl <- \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/\"\nevents <- import(paste0(url, \"Events.xlsx\"), setclass = \"tibble\")\nresults <- import(paste0(url, \"Results.xlsx\"), setclass = \"tibble\")\ndemographics <- import(paste0(url, \"Demographics.xlsx\"), setclass = \"tibble\")\n\nThe data files contain information on students’ Moodle events, background information such as their name, study location and employment status, and various grades they’ve obtained during the course. For more information about the dataset, please refer to Chapter 2 in this book [3]. These data are read in the tibble [5] format, a special type of data.frame commonly used by tidyverse packages. We also load the dplyr [6] package which we will use for various tasks throughout this chapter.\n\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#grouping-and-summarizing-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#grouping-and-summarizing-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "3 Grouping and summarizing data",
    "text": "3 Grouping and summarizing data\nInstead of individual-level data metrics, we may be interested in specific groups as specified by some combination of data values. For example, we could compute the number of students studying in each location by gender. To accomplish this, we need to start by creating a grouped dataset with the function group_by(). To compute the number of students, we can use the summarise() function which we already used previously in Chapter 1 and the function count(), which simply returns the number of observations in each category of its argument.\n\ndemographics |>\n  group_by(Gender) |> \n  count(Location)\n\n\n\n  \n\n\n\nThe column n lists the number of students in each group. When a tibble that contains grouped data is printed into the console, the grouping variable and the number of groups will be displayed in the console below the dimensions. Next, we will compute the total number of Moodle events of each student, which we will also use in the subsequent sections.\n\nevents_summary <- events |>\n  group_by(user) |> \n  tally() |>\n  rename(Frequency.Total = n)\nevents_summary\n\n\n\n  \n\n\n\nHere, the function tally() simply counts the number of number rows in the data related to each student, reported in the column n which we rename to Frequency.Total with the rename() function. We could also count the number of events by event type for each student\n\nevents |>\n  group_by(user, Action) |>\n  count(Action)"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#selecting-variables",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#selecting-variables",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "4 Selecting variables",
    "text": "4 Selecting variables\nIn the tidyverse paradigm, selecting columns, i.e., variables from data is done using the select() function. The select() function is very versatile, allowing the user to carry out selections ranging from simple selection of a single variable to highly complicated selections based on multiple criteria. The most basic selection selects only a single variable in the data based on its name. For example, we can select the employment statuses of students as follows\n\ndemographics |> \n  select(Employment)\n\n\n\n  \n\n\n\nNote that using select() with a single variable is not the same as using the $ symbol to select a variable, as the result is still a tibble, select() simply produces a subset of the data, where only the selected columns are present. Select is more similar to subset() in base R, which can accomplish similar tasks as select() and filter() in the tidyverse. However, we do not recommend using subset(), as it may not work correctly when the working environment has variables that have the same name as columns in the data, which can lead to undesired outcomes.\nTo extract the values of the selected column as a vector, we can use the function pull(). We use the head() function here to limit the console output to just the first few values of the vector (default is 6 values).\n\ndemographics |> \n  pull(Employment) |>\n  head()\n\n[1] \"None\"      \"None\"      \"None\"      \"None\"      \"Part-time\" \"Part-time\"\n\n\nThe select() function syntax supports several operations that are similar to base R. We can select ranges of consecutive variables using :, complements using !, and combine selections using c(). The following selections illustrate some of these features:\n\ndemographics |> \n  select(user:Origin)\n\n\n\n  \n\n\ndemographics |> \n  select(!Gender)\n\n\n\n  \n\n\ndemographics |> \n  select(c(user, Surname))\n\n\n\n  \n\n\n\nIn the first selection, we select all variables starting from user on the left to Origin on the right. In the second, we select all variables except Gender. In the third, we select both user and Surname variables.\nSometimes, our selection might not be based directly on the variable names themselves as in the examples above but instead on vectors that contain the names of columns we wish to select. In such cases, we can use the function all_of(). We can consider the intersections or unions of such selections using & and |, respectively.\n\ncols_a <- c(\"user\", \"Name\", \"Surname\")\ncols_b <- c(\"Surname\", \"Origin\")\ndemographics |> \n  select(all_of(cols_a))\n\n\n\n  \n\n\ndemographics |> \n  select(all_of(cols_a) & all_of(cols_b))\n\n\n\n  \n\n\ndemographics |> \n  select(all_of(cols_a) | all_of(cols_b))\n\n\n\n  \n\n\n\nOften the names of data variables follow a similar pattern, and these patterns can be used to construct selections. Selections based on a prefix or a suffix in the variable name can be carried out with the functions starts_with() and ends_with(), respectively. The function contains() is used to look for a specific substring in the names of the variables, and more complicated search patterns can be defined with the function matches() that uses regular expressions (see ?tidyselect::matches for further information).\n\nresults |> \n  select(starts_with(\"Grade\"))\n\n\n\n  \n\n\nresults |> \n  select(contains(\"Data\"))\n\n\n\n  \n\n\n\nSo far, our selections have been based on variable names, but other conditions for selection are also feasible. The general-purpose helper function where() is used to select those variables for which a function provided to it returns TRUE. For example, we could select only those columns that contain character type data or double type data.\n\nresults |> \n  select(where(is.character))\n\n\n\n  \n\n\nresults |> \n  select(where(is.double))"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#filtering-observations",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#filtering-observations",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "5 Filtering observations",
    "text": "5 Filtering observations\nIn contrast to selection which relates to obtaining a subset of the columns of the data, filtering refers to obtaining a subset of the rows. In the tidyverse, data filtering is carried out with the dplyr package function filter(), which should not be confused with the base R filter() function in the stats package. As we have attached the dplyr package, the base R filter() function is masked, meaning that when we write code that uses filter(), the dplyr version of the function will automatically be called.\nFiltering is often a much simpler operation than selecting variables, as the filtering conditions are based solely on the values of the data variables. Using filter() is analogous to the base R subset operator [, but the filtering condition is given as an argument to the filter() function instead. It is good to remind that in R a single equal sign (=) is merely for arguments of function calls, while double equal sign (==) is needed for comparison of two values. And example of filter:\n\ndemographics |> \n  filter(Origin == \"Bosnia\") |> \n  select(Name, Surname)\n\n\n\n  \n\n\n\nThe code above first filters our student demographics data to only those students whose country of origin is Bosnia. Then, we select their first and last names.\nMultiple filtering conditions can be refined and combined using base R logical operators, such as & and |.\n\ndemographics |>\n  filter(Gender == \"F\" & Location == \"Remote\")\n\n\n\n  \n\n\n\nHere, we filtered our data to female students who are studying remotely. The same result could also be obtained by using the filter() function two times\n\ndemographics |>\n  filter(Gender == \"F\") |> \n  filter(Location == \"Remote\")\n\n\n\n  \n\n\n\nThis type of approach may improve the readability of your code especially when there are several independent filtering conditions to be applied simultaneously.\nFilters can naturally be based on numeric values as well. For example, we could select those students whose final grade is higher than 8.\n\nresults |> \n  filter(Final_grade > 8)\n\n\n\n  \n\n\n\nSimilarly, we could select students based on their total number of Moodle events.\n\nevents_summary |>\n  filter(Frequency.Total > 100 & Frequency.Total < 500)"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#transforming-variables",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#transforming-variables",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "6 Transforming variables",
    "text": "6 Transforming variables\nIn the best-case scenario, our data is already in the desired format after it has been read into R, but this is rarely the case with real datasets. We may need to compute new variables that were not present in the original data, convert measurements to different units, or transform text data into a numeric form. In the tidyverse, data transformations are carried out by the mutate() function of the dplyr package. This function can be used to transform multiple variables at the same time or to construct entirely new variables. The syntax of the function is the same in both cases: first, the name of the variable should be provided followed by an R expression that defines the variable. The transformed data is not automatically assigned to any variable, enabling transformations to be used as temporary variables within a chain of piped operations.\nAs a simple example, we could convert the students’ locations into a factor variable.\n\ndemographics |> \n  mutate(Location = factor(Location))\n\n\n\n  \n\n\n\nAs we see from the tibble printout, the Location variable is a factor in the transformed data as indicated by the <fct> heading under the variable name. Note that the original demographics data was not changed, as we did not assign the result of the computation.\nThe gender and employment status of the students could also be used as factors, which we could do in a single mutate() call\n\ndemographics |> \n  mutate(\n    Gender = factor(Gender),\n    Location = factor(Location),\n    Employment = factor(Employment)\n  )\n\n\n\n  \n\n\n\nHowever, writing out individual identical transformations manually is cumbersome when the number of variables is large. For such cases, the across() function can be leveraged, which applies a function across multiple columns. This function uses the same selection syntax that we already familiarized ourselves with earlier to define the columns that will be transformed. To accomplish the same three transformations into a factor format, we could write\n\ndemographics |> \n  mutate(across(c(Gender, Location, Employment), factor))\n\n\n\n  \n\n\n\nThe first argument to the across() function is the selection that defines the variables to be transformed. The second argument defines the transformation, in this case, a function, to be used.\nWorking with dates can often be challenging. When we read the student demographic data into R, the variable Birthdate was assumed to be a character type variable. If we would like to use this variable to e.g., compute the ages of the students, we need to first convert it into a proper format using the as.Date function. Since the dates in the data are not in any standard format, we must provide the format manually. Afterwards, we can use the lubridate [7] package to easily compute the ages of the students, which we will save into a new variable called Age. We will also construct another variable called FullName which formats the first and last names of the students as \"Last, First\".\n\nlibrary(\"lubridate\")\ndemographics |> \n  mutate(\n    Birthdate = as.Date(Birthdate, format = \"%d.%m.%Y\"),\n    Age = year(as.period(interval(start = Birthdate, end = date(\"2023-03-12\")))),\n    FullName = paste0(Surname, \", \", Name) \n  ) |> \n  select(Age, FullName)\n\n\n\n  \n\n\n\nThe computation of the ages involves several steps. First, we construct a time interval object with the interval() function from the birthdate to the date for which we wish to compute the ages. Next, the as.period() function converts this interval into a time duration, from which we lastly get the number of years with the year() function.\nSuppose that we would like to construct a new variable AchievingGroup that categorizes the students into top 50% achievers and bottom 50% achievers based on their final grade on the course. We leverage two functions from the dplyr package to construct this new variable: case_when() and ntile(). The function case_when() is used to transform variables based on multiple sequential conditions. The function ntile() has two arguments, a vector x and an integer n, and it splits x into n equal-sized groups based on the ranks of the values in x.\n\nresults <- results |>\n  mutate(\n    AchievingGroup = factor(\n      case_when(\n        ntile(Final_grade, 2) == 1 ~ \"Low achiever\",\n        ntile(Final_grade, 2) == 2 ~ \"High achiever\"\n      )\n    )\n  )\n\nThe syntax of case_when() is very simple: we describe the condition for each case followed by ~ after which we define the value that the case should correspond to. We assign the result of the computation to the results data, as we will be using the AchievingGroup variable in later chapters.\nWe would also like to categorize the students based on their activity level, i.e., the number of total Moodle events. Our goal is to create three groups of equal size consisting of low activity, moderate activity and high activity students. The approach we applied to categorizing the achievement level of the students is also applicable for this purpose. We name our new variable as ActivityGroup, and we assign the result of the computation, as we will also be using this variable in later chapters.\n\nevents_summary <- events_summary |> \n  mutate(\n    ActivityGroup = factor(\n      case_when(\n        ntile(Frequency.Total, 3) == 1 ~ \"Low activity\",\n        ntile(Frequency.Total, 3) == 2 ~ \"Moderate activity\",\n        ntile(Frequency.Total, 3) == 3 ~ \"High activity\"\n      )\n    )\n  )"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#rearranging-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#rearranging-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "7 Rearranging data",
    "text": "7 Rearranging data\nSometimes we may want to reorder the rows or columns of our data, for example in alphabetical order based on the names of students on a course. The arrange() function from the dplyr package orders the rows of the by the values of columns selected by the user. The values are sorted in ascending order by default, but the order can be inverted by using the desc() function if desired. The variable order in the selection defines how ties should be broken when duplicate values are encountered in the previous variables of the selection. For instance, the following code would arrange the rows of our demographics data by first comparing the surnames of the students, and then the given names for those students with the same surname. Missing values are placed last in the reordered data.\n\ndemographics |> \n  arrange(Surname, Name)\n\n\n\n  \n\n\n\nA descending order based on both names can be obtained by applying the desc() function.\n\ndemographics |>\n  arrange(desc(Surname), desc(Name))\n\n\n\n  \n\n\n\nColumn positions can be changed with the relocate() function of the dplyr package. Like arrange(), we first select the column or columns we wish to move into a different position in the data. Afterwards, we specify the position where the columns should be moved to in relation to positions of the other columns. In our demographics data, the user ID column user is the first column. The following code moves this column after the Employment column so that the user column becomes the last column in the data.\n\ndemographics |>\n  relocate(user, .after = Employment)\n\n\n\n  \n\n\n\nThe mutually exclusive arguments .before and .after of relocate() specify the new column position in relation to columns that were not selected. These arguments also support the select() function syntax for more general selections."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reshaping-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reshaping-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "8 Reshaping data",
    "text": "8 Reshaping data\nBroadly speaking, tabular data typically take one of two formats: wide or long. In the wide format, there is one row per subject, where the subjects are identified by an identifier variable, such as the user variable in our Moodle data, and multiple columns for each measurement. In the long format, there are multiple rows per subject, and the columns describe the type of measurement and its value. For example, the events data is in a long format containing multiple Moodle events per student, but the results and demographics data are in a wide format with one row per student.\nIn the previous section, we constructed a summary of the users’ Moodle events in total and of different types. The latter data is also in a long format with multiple rows per subject, but we would instead like to have a column for each event type with one row per user, which means that we need to convert this data into a wide format. Conversion between the two tabular formats is often called pivoting, and the corresponding functions pivot_wider() and pivot_longer() from the tidyr [8] package are also named according to this convention. We will create a wide format data of the counts of different event types using the pivot_wider() function as follows\n\nlibrary(\"tidyr\")\nevents_types <- events |>\n  group_by(user, Action) |>\n  count(Action) |> \n  pivot_wider(\n    names_from = \"Action\", \n    names_prefix = \"Frequency.\",\n    values_from = \"n\",\n    values_fill = 0\n  )\nevents_types\n\n\n\n  \n\n\n\nHere, we first specify the column name that the names of the wide format data should be taken from in the long format data with names_from. In addition, we specify a prefix for the new column names using names_prefix that helps to distinguish what these new columns will contain, but in general, the prefix is optional. Next, we specify the column that contains the values for the new columns with values_from. Because not every student necessarily has events of every type, we also need to specify what the value should be in cases where there are no events of a particular type by using values_fill. As we are considering the frequencies of the events, it is sensible to select 0 to be this value. We save the results to events_types as we will use the event type data in later sections and chapters."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#joining-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#joining-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "9 Joining data",
    "text": "9 Joining data\nNow that we have computed the total number of events for each student and converted the event type data into a wide format, we still need to merge these new data with the demographics and results data. Data merges are also called joins, and the dplyr package provides several functions for different kinds of joins. Here, we will use the left_join() function that will preserve all observations of the first argument.\n\nleft_join(demographics, events_summary, by = \"user\")\n\n\n\n  \n\n\n\nIn essence, the above left join adds all columns from events_summary to demographics whenever there is a matching value in the by column. To continue, we can use additional left joins to add the remaining variables from the results data, and the Moodle event counts of different types from events_types to have all the student data in a single object.\n\nall_combined <- demographics |> \n  left_join(events_types, by = \"user\") |>\n  left_join(events_summary, by = \"user\") |> \n  left_join(results, by = \"user\")\nall_combined\n\n\n\n  \n\n\n\nWe will use this combined dataset in the following chapters as well."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#missing-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#missing-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "10 Missing data",
    "text": "10 Missing data\nSometimes it occurs that learning analytics data has cells for which the values are missing for some reason. The Moodle event data which we have utilized in this chapter does not naturally contain missing data. Thus, to have an example, we need to create a data which does. Second, handling of missing data is a vast topic of which we can only discuss some of the key points very briefly from a practical perspective. For a more comprehensive overview, we recommend reading [9] and [10] for a hands on approach. A short overview of missingness can be found in [11].\nThe code below will create missing values randomly to each column of events_types data (user column is an exception). To do that, we use the mice [12] package which also has methods for the handling of missing data. Unfortunately, mice is not part of the tidyverse. For more information about mice, a good source is miceVignettes at https://www.gerkovink.com/miceVignettes/. Now, let’s create some missing data.\n\nlibrary(\"mice\")\nset.seed(44)\nevents_types <- events_types |>\n  rename(\n    \"Ethics\" = \"Frequency.Ethics\",\n    \"Social\" = \"Frequency.Social\",\n    \"Practicals\" = \"Frequency.Practicals\"\n  )\nampute_list <- events_types |>\n  ungroup(user) |>\n  select(Ethics:Practicals)|>\n  as.data.frame() |>\n  ampute(prop = 0.3)\nevents_types_mis <- ampute_list$amp |>\n  as_tibble()\nevents_types_mis[2, \"Practicals\"] <- NA\n\nAbove, we also rename the variables that contain the frequencies of Moodle events related to ethics, social and practicals into Ethics, Social and Practicals, respectively. Let’s now see some of the values of events_types_mis\n\nevents_types_mis\n\n\n\n  \n\n\n\nWe can see that now the data contains NA values in some of the cells. These are the cells in which a missing value occurs, meaning that a value for those measurements has not been recorded. A missing data pattern, that is how missing of one variable affects missingness of other variables, can be show as:\n\nmd.pattern(events_types_mis, rotate.names = TRUE)\n\n\n\n\nAbove, each red square indicates a missing value while blue squares stand for observed ones. We can see that there are 95 complete rows, 10 for which Practicals are missing, 17 have missingness on Social and 9 are missing on Ethics. Also, one row has two red squares indicating a missing value on both Social and Practicals.\nLet’s now discuss options of handling missing data briefly. There are four classes of statistical methods for analyzing data with missing values: complete case (CC) methods, weighting methods, imputation methods, and model-based methods. The simplest of these is complete case analysis, which leaves missing values out of the analysis and only uses observations with all variables recorded. This can be done with the tidyr [8] package function drop_na():\n\nevents_types_mis |>\n  drop_na()\n\n\n\n  \n\n\n\nWe can see that after using this method, our data has only 95 rows as those were the rows without any columns having missing values. This made our data much smaller! If there are a lot of missing values, the data may become too small to use for practical purposes.\nA more novel group of methods are imputation methods. One of the options is using single imputation (SI) where the mean of each variable will determine the imputed value. The single mean imputation can be done as follows:\n\nimp <- mice(events_types_mis, method = \"mean\", m = 1, maxit = 1 , print = FALSE)\ncomplete(imp) |> \n  head()\n\n\n\n  \n\n\n\nWe can see from above that the imputed values are not integers anymore. However, if we aim to estimate means or regression coefficients (see Chapter 5 [13] for details) that is not a problem. One of the problems with mean imputation is that the variance and standard error estimates will become downward biased. A mean of Ethics for mean imputation is:\n\nfit <- with(imp, lm(Ethics ~ 1))\nsummary(fit)\n\n\n\n  \n\n\n\nNext, let’s briefly have a look at how we can utilize multiple imputation (MI) which is an improvement over single imputation. The multiple imputation approach generates more than one imputation thus creating many complete data sets for us. For each of these datasets, we can perform any analysis that we are interested in. After the analysis, one must pool the results from the impured datasets to get the final result. Here, we utilize a method called predictive mean matching (method = \"pmm\" in the code below), which uses the neighbour values of data as imputations.\n\nimp2 <- mice(events_types_mis, method = \"pmm\", m = 10, maxit = 100, print = FALSE)\nfit2 <- with(imp2, lm(Ethics ~ Practicals))\npool_fit <- pool(fit2)\n## Multiple imputation\nsummary(pool_fit)\n\n\n\n  \n\n\n## Complete cases\nsummary(lm(Ethics ~ Practicals, events_types_mis))[\"coefficients\"]\n\n$coefficients\n              Estimate Std. Error  t value   Pr(>|t|)\n(Intercept) 2.15459162 2.12235051 1.015191 0.31226283\nPracticals  0.06220884 0.02605001 2.388054 0.01865793\n\n## Without missingness\nsummary(lm(Ethics ~ Practicals, events_types))[\"coefficients\"] \n\n$coefficients\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 1.05409529 2.17821479 0.4839262 0.6292651258\nPracticals  0.08891892 0.02590313 3.4327482 0.0008053447\n\n\nFrom the results above, we can see that in this particular case the multiple imputation performs well in comparison to CC approach. The regression coefficient for full data without any missing values is \\(0.089\\), and it is \\(0.080\\) for multiple imputation, while complete case analysis gives \\(0.062\\). As all of them have very similar standard errors, this yields that MI and full data give statistically significant p-values for significance level 0.01, while CC does not."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#correcting-erroneous-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#correcting-erroneous-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "11 Correcting erroneous data",
    "text": "11 Correcting erroneous data\nLets imagine that our data has an error on the surname variable Surname and that all the names ending with “sen” should end with “ssen”. What we can do is that we can use regular expressions to detect the erroneous rows and we can also use them to replace the values. Let’s first figure out which last names contain a name ending with “sen”. We can use a function str_detect() to return TRUE/FALSE for each row from stringr [14] package within a filter() function call. We define pattern = \"sen$\" where $ indicates the end of the string.\n\nlibrary(\"stringr\")\ndemographics |>\n  filter(str_detect(string = Surname, pattern = \"sen$\")) |>\n  pull(Surname)\n\n[1] \"Nielsen\"  \"Johansen\" \"Joensen\"  \"Jansen\"   \"Olsen\"   \n\n\nAfter pulling the filtered surnames, there seems to be five surnames ending with “sen”. Next, let’s try to replace “sen” with “ssen”. On the next row we filter just as previously to limit output.\n\ndemographics |>\n  mutate(Surname = str_replace(\n    string = Surname, pattern = \"sen$\", replacement = \"ssen\")\n  ) |>\n  filter(str_detect(string = Surname, pattern = \"sen$\")) |>\n  pull(Surname)\n\n[1] \"Nielssen\"  \"Johanssen\" \"Joenssen\"  \"Janssen\"   \"Olssen\"   \n\n\nThus, the following code updates the data so that all the surnames ending with “sen” now end with “ssen” instead.\n\ndemographics <- demographics |>\n  mutate(Surname = str_replace(\n    string = Surname, pattern = \"sen$\", replacement = \"ssen\")\n  )"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#conclusion-and-further-reading",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#conclusion-and-further-reading",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "12 Conclusion and further reading",
    "text": "12 Conclusion and further reading\nData wrangling is one of the most important steps in any data analysis pipeline. This chapter introduced the tidyverse, tidy data, and several commonly used R packages for data manipulation and their use in basic scenarios in the context of learning analytics. However, the tidyverse is vast and can hardly be fully covered in a single chapter. We refer the reader to additional resources such as those found on the tidyverse website at and the book “R for Data Science” by Hadley Wicham and Garret Grolemund. The book is free to use and readily available online at https://r4ds.had.co.nz/."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html",
    "href": "chapters/ch05-basic-stats/ch5-stats.html",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#introduction",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#introduction",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning analytics involves the practical application of statistical methods to quantitative data, which can represent various aspects of the learning process such as student engagement, progress, and outcomes. Thus, knowledge about basic statistical methods is essential. Let’s start with how statistics connects with the research process and how it is also linked with philosophy of science. According to Niiniluoto [1], the research process can be described with the following eight steps:\n\nSetting up a problem.\nDisambiguation of the problem. Building a research strategy.\nCollecting data.\nDescribing the data.\nAnalysis of data.\nInterpreting the analyses.\nWriting the report.\nPublishing the results.\n\nSteps 1 and 2 require knowledge and skills related to the applied field, but also general scientific aptitude. Knowledge about statistics is central in steps 3, 4, 5, and 6. Finally, steps 7 and 8 mostly require skills in writing and communication. Overall, it can be argued that a solid understanding of statistics and statistical methods is crucial for anyone conducting research with quantitative data.\nThis chapter of the book concentrates on steps 4, 5, and 6 of the research process. We start with descriptive statistics, which are statistics that describe the overall features of the data. In contrast, inferential statistics are used to draw conclusions and make inferences about the population under study. Afterwards, we explain the basics of statistical hypothesis testing, which is the most common — although not the only — way to analyze data. The most common statistical tests, such as Student’s t-test, Chi-squared test, Analysis of variance, Levene’s test, and Shapiro-Wilk test are covered in this chapter. We also explain how to interpret the results of each test. We also present the linear regression model, which is not a statistical test but one of the most powerful statistical tools. Basic understanding of linear regression is essential for anyone interested in more advanced regression techniques, including logistic regression which is covered in the final section of this chapter. For a more in-depth view on the statistical tests covered in this chapter, we refer the reader to works such as [2, 3]."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#descriptive-statistics",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#descriptive-statistics",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "2 Descriptive statistics",
    "text": "2 Descriptive statistics\nDescriptive statistics are used to provide a meaningful quantitative overview of data, and to summarize potentially vast amounts of information into more easily comprehensible and manageable quantities. In general, descriptive statistics are used as a first step in a data analysis workflow. In this section we will focus on numeric descriptors while visualizations are the topic of Chapter 6 [4]. For this chapter, we will use the combined Moodle data with students’ demographics, results, and summarized Moodle event activity. For more information about the dataset, please refer to Chapter 2 in this book [5]. We begin by installing all R packages that we will use in this chapter.\n\ninstall.packages(\n  c(\"car\", \"rio\", \"see\", \"dplyr\", \"tidyr\", \n    \"broom\", \"report\", \"correlation\", \"performance\")\n)\n\nWe use the rio [6] package to read the data files into R via the import() function. The Events dataset contains log data on the student’s Moodle activity such as Moodle event types and names. The Demographics dataset contains background information on the students such as their gender and location of study (categorical variables). Finally, the Results data contains grades on various aspects of the Moodle course including the final course grade (numeric variables). We also create a new variable called AchievingGroup that categorizes the students into bottom and top 50% of achievers in terms of the final grade. We will leverage the dplyr [7] and tidyr [8] packages to wrangle the data into a single combined dataset. We begin by reading the data files and by constructing the AchievingGroup variable.\n\nlibrary(\"rio\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nurl <- \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/\"\nevents <- import(paste0(url, \"Events.xlsx\"), setclass = \"tibble\")\ndemographics <- import(paste0(url, \"Demographics.xlsx\"), setclass = \"tibble\")\nresults <- import(paste0(url, \"Results.xlsx\"), setclass = \"tibble\") |> \n  mutate(\n    AchievingGroup = factor(\n      case_when(\n        ntile(Final_grade, 2) == 1 ~ \"Low achiever\",\n        ntile(Final_grade, 2) == 2 ~ \"High achiever\"\n      )\n    )\n  )\n\nNext, we summarize the student’s engagement based on their Moodle activity into three groups: Low activity, Moderate activity and High Activity.\n\nevents_summary <- events |>\n  group_by(user) |>\n  tally() |>\n  rename(Frequency.Total = n) |>\n  mutate(\n    ActivityGroup = factor(\n      case_when(\n        ntile(Frequency.Total, 3) == 1 ~ \"Low activity\",\n        ntile(Frequency.Total, 3) == 2 ~ \"Moderate activity\",\n        ntile(Frequency.Total, 3) == 3 ~ \"High activity\"\n      )\n    )\n  )\n\nWe also count the different types of Moodle events.\n\nevents_types <- events |>\n  group_by(user, Action) |>\n  count(Action) |> \n  pivot_wider(\n    names_from = \"Action\", \n    names_prefix = \"Frequency.\",\n    values_from = \"n\",\n    values_fill = 0\n  )\n\nFinally, we combine the data.\n\nall_combined <- demographics |>\n  left_join(events_types, by = \"user\") |> \n  left_join(events_summary, by = \"user\") |> \n  left_join(results, by = \"user\")\n\nThe various steps of the combined data construction are discussed in greater detail in Chapter 4 [9].\n\n2.1 Measures of central tendency\nA typical way to summarize a univariate data sample is to describe its “middle point” using an appropriate statistic depending on the measurement scale of the data. The most common statistics to describe such a value are the mean, the median, and the mode.\nFor data on the interval or ratio scales (and sometimes also on the ordinal scale), the most common option is to use the arithmetic mean, which is available via the base R function mean(). This function takes a vector of values as its input.\n\nall_combined |> \n  summarise(\n    mean_grade = mean(Final_grade),\n    mean_total = mean(Frequency.Total)\n  )\n\n\n\n  \n\n\n\nThe means are reported as a tibble with a single column for both variables.\nFor data on the ordinal scale (or interval or ratio scales), the median can be used which is defined as the value that separates the lower half from the bottom half of the data sample, i.e., the 50% quantile. The median can be computed in R using the built-in median() function. Similarly to mean(), this function also takes a vector of values as its input.\n\nall_combined |> \n  summarise(\n    median_grade = median(Final_grade),\n    median_total = median(Frequency.Total)\n  )\n\n\n\n  \n\n\n\nJust like before, the medians are reported as a tibble with each value in its own column.\nFor data on the nominal or ordinal scale, the mode is a suitable choice as it describes the category with the highest number of observations. Unfortunately, there is no readily available function in R to compute the mode, and the reader should take care not to mistakenly use the mode() function, which is used to determine the internal storage mode of a variable (similar to the typeof() function). However, we can easily write our own function to compute the statistical mode as follows:\n\nstat_mode <- function(x) {\n  u <- unique(x)\n  u[which.max(tabulate(match(x, u)))]\n}\n\nFunctions in R are written using the following syntax. First, we define the name of the function, just like we would define the name of a variable when assigning data into it, in this case the name is stat_mode. Next, we assign the function definition, which starts with the keyword function. Next, we describe the function arguments within the parentheses, in this case we call our argument x, which we assume contains the data vector we wish to compute the mode for. Next, we define the body of the function within the braces. The body determines what the function does and what its output should be. Within the body, we first determine the unique values in the data vector x, and assign the result to a variable u. Next, we need to count the number of occurrences of each unique value. To start, we first match each observed value in x to the unique values in u to get the corresponding indices, which we will then count using tabulate. We obtain the index of the value with the highest number of occurrences with the function which.max(), and finally the corresponding unique value by selecting it from u using the subset operator, i.e., the brackets. Our function will now work on all types of data.\n\nall_combined |>\n  summarise(\n    mode_gender = stat_mode(Gender),\n    mode_location = stat_mode(Location)\n  )\n\n\n\n  \n\n\n\nThe output is now similar to the mean and median functions that we used earlier, showing the modes of Gender and Location as a two-column tibble. For nominal variables, it is common to also compute the frequencies of each category. This can easily be done with the base R function table()\n\ntable(all_combined$Gender)\n\n\n F  M \n65 65 \n\ntable(all_combined$Location)\n\n\nOn campus    Remote \n      106        24 \n\n\nThe function outputs the names of the categories and the frequency of each category as an integer vector.\n\n\n2.2 Measures of dispersion\nFor data on the interval and ratio scales (and sometimes also on the ordinal scale), it is also meaningful to describe how clustered or scattered the values in the sample are, i.e., how far apart the values are from one another. Commonly used measures of statistical dispersion include the variance, standard deviation, and the interquartile range. Typically, such measures have the value 0 when all values in the sample are identical, and the value increases as the dispersion in the data grows.\nAll three measures can be readily computed with built-in R functions var(), sd(), and IQR() respectively. Like mean() and median(), these functions accept a vector or numeric values as input.\n\nall_combined |>\n  summarise(\n    var_grade = var(Final_grade),\n    sd_grade = sd(Final_grade),\n    iqr_grade = IQR(Final_grade)\n  )\n\n\n\n  \n\n\n\nThe variance, standard deviation and interquartile range of the final grade are returned as a tibble with three columns.\n\n\n2.3 Covariance and Correlation\nCovariance and correlation measure the linear dependence between two variables. Correlation is a unitless measure between -1 and 1, whereas covariance is not, and its scale depends on the scale of the variables. The sign of both measures indicates the tendency of the relationship. Positive sign means that as the value of one variable increases, the value of the other variable tends to increase as well. Conversely, a negative sign indicates that the value of the second variable tends to decrease as the value of the first variable increases.\nBoth covariance and correlation and be computed directly in R using the functions cov() and cor(), respectively.\n\nall_combined |>\n  summarise(\n    cov_grade_total = cov(Final_grade, Frequency.Total),\n    cor_grade_total = cor(Final_grade, Frequency.Total),\n  )\n\n\n\n  \n\n\n\nWe obtain the covariance and correlation between the final grade and total number of Moodle events. We will familiarize ourselves with correlations in greater depth in Section 5.4.\n\n\n2.4 Other common statistics\nThe extreme values of a data sample can be found using the function range(), which computes the minimum and maximum values of the sample as a vector. These values can also be computed individually with the corresponding functions min() and max(). Because summarise() only allows a single value as output per row, we use the reframe() function instead when computing the range.\n\nall_combined |>\n  reframe(\n    range_grade = range(Final_grade)\n  )\n\n\n\n  \n\n\nall_combined |>\n  summarise(\n    min = min(Final_grade),\n    max = max(Final_grade)\n  )\n\n\n\n  \n\n\n\nWith reframe(), we obtain a tibble with two rows, the first containing the minimum value and the second the maximum value of the final grade. If we instead use summarise() like before, we can only obtain one value per computed variable. The summary() function can also be used to quickly compute several of the most common descriptive statistics for all variables of a dataset.\n\nresults |> \n  select(Grade.SNA_1:Grade.Group_self) |> \n  summary()\n\n  Grade.SNA_1      Grade.SNA_2      Grade.Review    Grade.Group_self\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 8.000   1st Qu.: 9.000   1st Qu.: 6.670   1st Qu.: 9.000  \n Median : 9.000   Median :10.000   Median : 8.000   Median :10.000  \n Mean   : 8.346   Mean   : 9.262   Mean   : 7.724   Mean   : 8.085  \n 3rd Qu.:10.000   3rd Qu.:10.000   3rd Qu.: 9.670   3rd Qu.:10.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n\n\nThe output shows the minimum and maximum values, the quartiles, and the mean of each variable that we selected."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#statistical-hypothesis-testing",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#statistical-hypothesis-testing",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "3 Statistical hypothesis testing",
    "text": "3 Statistical hypothesis testing\nStatistical hypothesis testing aims to evaluate hypotheses about a population of interest using probabilistic inference. The starting point of any statistical test is a so-called null hypothesis (denoted by \\(H_0\\)), which typically corresponds to a scenario, where evidence supporting a specific hypothesis is a result of pure chance. For example, when evaluating whether a new drug is an efficient form of treatment via a randomized controlled trial, the null hypothesis could be that the drug has no effect on the response. A null hypothesis is always associated with an alternative hypothesis (denoted by \\(H_1\\)), which is typically the inverse of the null hypothesis and corresponds to the hypothesis of interest, e.g., the drug has an effect on the response.\nStatistical tests operate by assuming that the null hypothesis is true, and highly unlikely events under this assumption are typically regarded as giving cause for rejecting the null hypothesis. A statistical test is associated with a test statistic, which is a measure of how much the observations deviate from the null hypothesis scenario. The distribution of the test statistic under the null hypothesis and the sample test statistic can be used to compute the probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming that the null hypothesis is true. This probability is known as the p-value, which is often mischaracterized even in scientific literature. For instance, the p-value is not the probability that the null hypothesis is true or that the alternative hypothesis is false. The p-value also does not quantify the size of the observed effect, or its real-world importance.\nTypically, a confidence level is decided before applying a statistical test (usually denoted by \\(\\alpha\\)), and the null hypothesis is rejected if the observed p-value is smaller than this confidence level. If the p-value is greater than the confidence level, the null hypothesis is not rejected. Traditionally, the confidence level is 0.05, but this convention varies by field, and should be understood as being arbitrary, i.e., there is nothing special about the value 0.05. If the p-value falls below the confidence level, the result is regarded as statistically significant.\nHypothesis testing is a powerful tool for drawing conclusions from data, but it is important to use it appropriately and to understand its limitations. Every statistical test is associated with a set of assumptions which are often related to the distribution of the data sample. If these assumptions are violated, then the results of the test may be unreliable. In the following sections, some of the most common statistical tests are introduced. We will take advantage of the report [10] package and the corresponding function report() to showcase the results of the various statistical tests. For more information on the concepts and principles related to statistical hypothesis testing, see e.g., [2, 3].\n\n3.1 Student’s t-test\nStudent’s t-test [11] is one of the most well-known statistical tests. It compares the mean values of variables either between two populations or between a single population and a reference level and is thus applicable to continuous variables. The test assumes homogeneity of variance and that the data originates from a normal distribution. For nonhomogeneous data, the test can still be performed by using an approximation [12]. In R, all variants of the t-test test can be applied using the function t.test().\nOur goal is to compare the students’ Moodle activity with respect to their final grade. For this purpose, we use the binary variable called AchievingGroup which categorizes the students into top and bottom 50% achievers in terms of the final grade.\n\n3.1.1 One-sample t-test\nThe one-sample t-test compares the mean of a data sample against a reference value, typically defined by the null hypothesis. Let us begin by testing the hypothesis that the average number of Moodle events (Frequency.Total) is 600 (\\(H_0 : \\mu = 600\\)). The function t.test() can be used in various ways, but in this example we provide the function with a formula object Frequency.Total ~ 1 as the first argument. The formula syntax is a standard method for defining statistical models and other dependency structures in R. The formula defines that the left-hand side of the ~ symbol is a response variable which is explained by the terms on the right-hand side. Because we’re not conducting the test with respect to any other variable, the right-hand side of the formula is simply 1, which means that it is a constant in the R formula syntax. This does not mean for example, that our null hypothesis would be that the number of Moodle events is 1. The expected value that the test is applied against (i.e., the value we assume \\(\\mu\\) to have under the null hypothesis) is defined via the argument mu, which by default has the value 0 for a one-sample t-test. Argument data defines in which environment the formula should be evaluated. By providing the all_combined data, we do not have to explicitly extract the FrequencyTotal variable from the data in the formula by writing all_combined$Frequency.Total or by using pull(). This is especially useful when the formula contains several variables from the same data.\n\nttest_one <- t.test(Frequency.Total ~ 1, data = all_combined, mu = 600)\nttest_one\n\n\n    One Sample t-test\n\ndata:  Frequency.Total\nt = 3.4511, df = 129, p-value = 0.0007553\nalternative hypothesis: true mean is not equal to 600\n95 percent confidence interval:\n 657.8530 813.3163\nsample estimates:\nmean of x \n 735.5846 \n\n\nAs a result, we obtain the value of the test statistic (t = 3.4511), the degrees of freedom (df = 129), and the p-value of the test (p-value = 0.0007553). Because the p-value is very small (much smaller than the standard 0.05 confidence level), we reject the null hypothesis, which means that the average number of Moodle events is significantly different from 600. The output of the test result object also describes the alternative hypothesis \\(H_1\\) under alternative hypothesis, and the confidence interval of the test statistic.\nWe produce a summarized report of the test results with the report() function.\n\nreport(ttest_one)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe One Sample t-test testing the difference between Frequency.Total (mean =\n735.58) and mu = 600 suggests that the effect is positive, statistically\nsignificant, and small (difference = 135.58, 95% CI [657.85, 813.32], t(129) =\n3.45, p < .001; Cohen's d = 0.30, 95% CI [0.13, 0.48])\n\n\nThis produces a description of the results of the test that is easier to read and interpret than the direct output of the test result object. We note that a warning is also produced which we can safely ignore in this case. The warning occurs because the test result object ttest_one does not retain the original data all_combined which we used to carry out the test. If non-approximate effect sizes are desired, the test should be carried out by supplying the variables being compared directly without using the formula interface of the t.test() function. For more information on the effect size, see e.g., [13, 14].\n\n\n3.1.2 Two-sample t-test\nIn contrast to the one-sample t-test, the two-sample t-test compares the means of two data samples against one another. For example, suppose that we’re interested in the hypothesis that the average number of Moodle events is the same for the top and bottom 50% achievers (\\(H_0: \\mu_1 = \\mu_2\\)). We can once again leverage the formula syntax, but instead of the constant 1 on the right-hand side of the formula, we will now replace it with the variable Achievement which defines the achievement level.\n\nttest_two <- t.test(Frequency.Total ~ AchievingGroup, data = all_combined)\nttest_two\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  Frequency.Total by AchievingGroup\nt = 4.4749, df = 95.988, p-value = 2.102e-05\nalternative hypothesis:\ntrue difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 182.6427 473.8496\nsample estimates:\nmean in group High achiever  mean in group Low achiever \n                   899.7077                    571.4615 \n\n\nThe contents of the result object are mostly the same as in the case of the one-sample t-test. The result is again statistically significant (using the 0.05 confidence level) meaning that according to the test, the average number of Moodle events is higher for the top 50% achievers. The report() function can be used to produce a similar summary as in the case of the one-sample t-test.\n\nreport(ttest_two)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of Frequency.Total by\nAchievingGroup (mean in group High achiever = 899.71, mean in group Low achiever =\n571.46) suggests that the effect is positive, statistically significant, and large\n(difference = 328.25, 95% CI [182.64, 473.85], t(95.99) = 4.47, p < .001; Cohen's d\n= 0.91, 95% CI [0.49, 1.33])\n\n\nThis produces the same warning as before in the one-sample case, but we can safely ignore it again.\n\n\n3.1.3 Paired two-sample t-test\nInstead of directly comparing the means of two groups, it may sometimes be of interest to compare differences between pairs of measurements. Such a scenario typically arises in an experiment, where subjects are paired, or two sets of measurements are taken from the same subjects. While our Moodle event data does not contain such measurement pairs, we could still imagine that our data was organized such that each student in the bottom 50% achievers was paired with a student in the top 50% achievers and that there is a one-to-one correspondence between the two achievement groups. This dependency between the two groups is the key difference between the paired test and the two-sample test.\nA more suitable approach for paired data is to test the differences between the pairs, e.g., the differences between the number of Moodle events in our scenario. We supply the t.test() function with the argument paired = TRUE so that it will take the measurement pairs into account. In this test, the null hypothesis is that the mean difference between the student pairs is zero (\\(H_0 : \\mu_d = 0\\)).\n\nttest_paired <- t.test(\n  Frequency.Total ~ AchievingGroup, data = all_combined, paired = TRUE\n)\nttest_paired\n\n\n    Paired t-test\n\ndata:  Frequency.Total by AchievingGroup\nt = 4.3733, df = 64, p-value = 4.599e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 178.3014 478.1910\nsample estimates:\nmean difference \n       328.2462 \n\n\nThe result is once again statistically significant, and we reject the null hypothesis. Because the mean difference between the pairs is positive, this means average number of Moodle events is higher for the top 50% achievers of the pairs.\nPaired two-sample t-test is also supported by report().\n\nreport(ttest_paired)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference of Frequency.Total by AchievingGroup (mean\ndifference = 328.25) suggests that the effect is positive, statistically\nsignificant, and medium (difference = 328.25, 95% CI [178.30, 478.19], t(64) =\n4.37, p < .001; Cohen's d = 0.55, 95% CI [0.28, 0.81])\n\n\nWe can once again safely ignore the produced warning message.\n\n\n\n3.2 Chi-squared test\nThe chi-squared test is used for the analysis of contingency tables; it tests whether two categorical variables are independent or not [15]. A typical use case for this test is to investigate differences between groups such as student attendance by location or gender. The basic idea of the test is to compare the observed contingency table to a table under the null hypothesis where the variables are independent. The chi-squared test is based on the cell-specific differences between these two tables. As a general rule, the test assumes that the expected value is at least 5 in at least 80% of the cells, and that no expected values are below 1. If these assumptions are violated, the results of the test may not be reliable. In such cases, Fisher’s exact test [16] can be used instead via the function fisher.test(), but it may be computationally slow for large contingency tables. Both the chi-squared test and Fisher’s exact test assume that the data is a random sample from the population.\nWe will use the combined Moodle data to investigate whether the achievement level and the activity level of the students are independent. First, we must create the contingency table from the individual-level data. We use the table() function for this purpose.\n\ntab <- table(all_combined$ActivityGroup, all_combined$AchievingGroup)\ntab\n\n                   \n                    High achiever Low achiever\n  High activity                27           16\n  Low activity                 14           30\n  Moderate activity            24           19\n\n\nThe table shows the observed frequencies in each cell, i.e., for each combination of activity and achievement. Next, we apply the chi-squared test using the chisq.test() function.\n\nXsq_test <- chisq.test(tab)\nXsq_test\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 9.2135, df = 2, p-value = 0.009984\n\n\nPrinting the test result object shows the test statistic (X-squared), the associated degrees of freedom (df) and the p-value (p-value). The p-value is very small, meaning that we reject the null hypothesis. In other words, the achievement and activity levels of the students are not independent. This is not a surprising result, as more active students are more likely to engage with the course content and perform better in terms of the learning outcomes. We can confirm that the assumptions of the test related to the expected values of the cells were not violated by using the test result object, which contains the expected values of the cells in the element expected.\n\nall(Xsq_test$expected >= 1)\n\n[1] TRUE\n\nmean(Xsq_test$expected >= 5) >= 0.80\n\n[1] TRUE\n\n\nAll expected values were greater than one, and over 80% of the expected values were greater than 5. This means that the assumptions are satisfied for our data and thus the results are reliable. Here, we used the function all() which takes a logical vector as input and returns TRUE if all elements of the vector were TRUE. Otherwise, the function returns FALSE. Unfortunately, the report() function does not support the chi-squared test.\n\n\n3.3 Analysis of variance\nAnalysis of variance (ANOVA) [17] can be viewed as the generalization of Student’s t-test, where instead of one or two groups, the means of a variable are compared across multiple groups simultaneously. The name of the method comes from its test statistic, which is based on a decomposition of the total variance of the variable into variance within the groups and between the groups. ANOVA makes several assumptions: the observations are independent, the residuals of the underlying linear model follow a normal distribution, and that the variance of the variable is the same across groups (homoscedasticity). If these assumptions are violated, the results of the test may not be reliable. One alternative in such instances is to use the non-parametric Kruskal-Wallis test [18] instead, which is available in R via the function kruskal.test(). This test uses the ranks of the observations, and the null hypothesis is that the medians are the same for each group.\nWe use our combined Moodle data to demonstrate ANOVA. Instead of comparing the total number of Moodle events between top and bottom 50% of achievers, this time we will compare the final grade of the students across three activity groups: low activity, moderate activity, and high activity, described by the variable ActivityGroup. Thus the null and alternative hypotheses are in this case:\n\n\\(H_0\\): The expected values of the final grade are the same across the three activity groups (\\(\\mu_1 = \\mu_2 = \\mu_3\\)),\n\\(H_1\\): At least one activity group has a different expected final grade (\\(\\mu_i \\ne \\mu_j\\) for at least one pair \\(i \\ne j\\)).\n\nTo carry out the analysis, we apply the aov function, which uses the same formula syntax to define the response variable and the groups as the t.test() function does. Next, we apply the summary() function to the aov() function return object fit, as the default output of aov() is not very informative.\n\nfit <- aov(Final_grade ~ ActivityGroup, data = all_combined)\nsummary(fit)\n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nActivityGroup   2  175.7   87.87   25.11 6.47e-10 ***\nResiduals     127  444.4    3.50                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary contains the following columns: Df describes the degrees of freedom of the \\(F\\)-distribution associated with the test, Sum Sq reports the sum of squares related to the groups and the residuals, Mean Sq reports the corresponding mean sum of squares, F value is the value of the test statistic, and finally Pr(>F) is the p-value of the test. For this example, the p-value is very small, which means that the null hypothesis is rejected, and there are statistically significant differences in the final grade between the groups according to the test. In the following sections we will learn how to test for the assumptions related to normality and homoscedasticity. The report() function can be used for the output of aov() as well.\n\nreport(fit)\n\nThe ANOVA (formula: Final_grade ~ ActivityGroup) suggests that:\n\n  - The main effect of ActivityGroup is statistically significant and large (F(2,\n127) = 25.11, p < .001; Eta2 = 0.28, 95% CI [0.17, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nThis output also reports the degrees of freedom, the test statistic value and the p-value but in a more easily readable format.\nNote that ANOVA simply measures if there are differences between the groups but does not provide information on how these differences emerge. For example, there could be a single group that is different from all the rest, or two subgroups where the means are similar within each group, but different between the subgroups. Visualizations can be a helpful tool for gaining more insight into the differences, and post-hoc pairwise tests can be carried out to compare the pairs of groups.\n\n\n3.4 Levene’s test\nLevene’s test is used to investigate whether the variance of a variable is the same across two or more groups [19]. Compared to alternatives such as Bartlett’s test [20], Levene’s test is less sensitive to non-normal observations. The test is not available in base R, but it can be found in the car package as the function leveneTest(). The function uses the same formula syntax as t.test() and aov(). We will investigate the homogeneity of the variance of the final grade between the activity groups.\n\nlibrary(\"car\")\nleveneTest(Final_grade ~ ActivityGroup, data = all_combined)\n\n\n\n  \n\n\n\nThe output of leveneTest() is analogous to the output of the ANOVA summary, and it contains the degrees of freedom (Df), the value of the test statistic (F value) and the p-value of the test (Pr(>F)). The p-value is very small, so we reject the null hypothesis meaning that the variance of the final grade is not the same across the groups according to the test. This means that the assumption of homoscedasticity is violated for the analysis of variance of the final grade, and thus the results may not be reliable. The report() function is not supported for leveneTest().\n\n\n3.5 Shapiro-Wilk test\nShapiro-Wilk test tests the null hypothesis that a data sample originated from a normal distribution [21]. The test is available in base R as the function shapiro.test(). Unfortunately, this function does not support the formula syntax unlike the other test functions we have used thus far. The function only accepts a single numeric vector as its argument. Therefore, to test the normality of multiple groups simultaneously, the data must first be split into the groups to be tested. We apply the test to the final grade in each achievement group. With the help of the broom package [22], we wrap the test results into a tidy format.\n\nlibrary(\"broom\")\nall_combined |> \n  ## Performs the computations in each activity group\n  group_by(ActivityGroup) |> \n  ## Apply a function in each group\n  group_modify(~{\n    ## Apply the Shapiro test in each group and create tidy output\n    shapiro.test(.$Final_grade) |> \n      tidy()\n  }) |> \n  ## Selection of variables to keep in the output\n  select(ActivityGroup, statistic, p.value)\n\n\n\n  \n\n\n\nThis is also a great example of the tidyverse paradigm. First, we group the data by ActivityGroup using group_by(), and then apply a function in each group using group_modify(). We apply the shapiro.test() function to the Final_grade variable, and then we convert the test results into a tidy tibble using the tidy() function from the broom package. We also use the special dot notation . to select the final grade variable from the data in each group. Finally, we select the grouping variable (ActivityGroup), the test statistic (statistic) and the p-value (p.value) of each test using select() and print the results. The resulting object is a tibble with three columns: ActivityGroup, statistic and p.value, the last two of which give the test statistic and p-value of the test for the activity group of the first column.\nWe can see that according to the test, the Final_grade variable is not normally distributed in any of the activity groups, as the p-values are very small. As a consequence, the results of the analysis of variance carried out earlier may not be reliable."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#sec-correlation",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#sec-correlation",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "4 Correlation",
    "text": "4 Correlation\nIn Section 5.2.3, we briefly described covariance and correlation, and showcased the base R functions to compute them. However, there are several powerful and user-friendly packages for the analysis and reporting of correlations, such as the correlation [23] package which we will demonstrate in this section.\n\nlibrary(\"correlation\")\n\nFor example, the package can easily compute all pairwise correlations between the numeric variables of the data with the function correlation(). The argument select can be used to compute the correlations only for a subset of the variables.\n\ncorrs <- correlation(\n  all_combined, \n  select = c(\"Frequency.Total\", \"Grade.Theory\", \"Final_grade\")\n)\ncorrs\n\n\n\n  \n\n\n\nThe columns Parameter1 and Parameter2 describe the variables that the correlation was computed for, r is the value of the sample correlation, and the remaining columns report the 95% confidence interval, the value of the test statistic, and the p-value of the test (a t-test for correlations) along with the statistical significance. By default, Pearson’s correlation coefficient is calculated, but the package also supports many alternative correlation measures. The correlation coefficient to be computed can be selected with the argument method that has the value \"pearson\" by default. Selecting for example method = \"spearman\" would compute the Spearman correlation coefficient instead. We can also obtain a correlation matrix by using summary()\n\nsummary(corrs)\n\n\n\n  \n\n\n\nBy default, redundant correlations are omitted, but they can be obtained by setting redundant = TRUE in the call to summary(). A plot of the correlation matrix can be produced with the help of the package see [24].\n\nlibrary(\"see\")\ncorrs |>\n  ## Also include redundant correlations\n  summary(redundant = TRUE) |>\n  plot()\n\n\n\n\nThe plot shows the strength of the correlations where darker colors imply stronger correlations. Visualizations will be covered at greater length in Chapter 6 [4]."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#linear-regression",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#linear-regression",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "5 Linear regression",
    "text": "5 Linear regression\nLinear regression is a statistical tool where one continuous variable is explained by the values of other variables. The variable of interest is said to be a dependent, while the other variables are called predictors. Predictors may also be called explanatory variables, independent variables or covariates depending on the context, applied field, and perspective.\nConsider a very simple case, where we only have one predictor, which happens to be a continuous variable. In this case, fitting a linear regression model is merely the same as fitting a straight line to a scatterplot. It is assumed that deviations from this line are simply a result of random variation. \nNow, let’s go through the formal definition of a linear regression model. Let \\(Y\\) be a dependent variable with measurements \\(y_1 \\ldots, y_n\\), and let \\(X_1, X_2, \\ldots, X_k\\) be predictor variables with measurements \\(x_{1i}, \\ldots, x_{ki}\\) for all \\(i = 1,\\ldots,n\\) where \\(i\\) refers to an individual measurement. Then, the regression equation is \\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki} + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2), \\quad i = 1,\\ldots,n\n\\] where we have the regression coefficients \\(\\beta_0, \\beta_1, \\ldots \\beta_k\\) and the error variance \\(\\sigma^2\\). The first parameter \\(\\beta_0\\) is called the intercept that models the conditional expectation of \\(Y\\) when all the predictors have the value \\(0\\). From the regression equation, several assumptions become apparent. First, as the name of the model suggests, a linear relationship is assumed between the response and the predictors. Next, the variance \\(\\sigma^2\\) of the errors \\(\\varepsilon_i\\) is constant, and does not depend on the values of the predictors (homoscedasticity). The errors are also assumed independent. The predictor variables are assumed fixed and their values perfectly measured without error.\nLet’s fit a linear regression model that predicts the final grade with the number of Moodle events of different types. To simplify the exposition, we will only use three types of Moodle events as predictors. We use the lm() function which has the same formula interface that we are already familiar with. First, we must define the dependent variable on the left-hand side of the formula, followed by the predictors on the right-hand side separated by a + sign. We must also supply the data argument, which tells the function where the actual values of the variables can be accessed.\n\nfit <- lm(\n  Final_grade ~ Frequency.Applications + Frequency.Assignment +\n    Frequency.La_types,\n  data = all_combined\n)\nsummary(fit)\n\n\nCall:\nlm(formula = Final_grade ~ Frequency.Applications + Frequency.Assignment + \n    Frequency.La_types, data = all_combined)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0382 -0.8872  0.3665  1.2372  3.4422 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.800211   0.405963  14.288  < 2e-16 ***\nFrequency.Applications  0.076516   0.022294   3.432 0.000811 ***\nFrequency.Assignment   -0.005049   0.005734  -0.881 0.380225    \nFrequency.La_types      0.088252   0.027314   3.231 0.001574 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.914 on 126 degrees of freedom\nMultiple R-squared:  0.2559,    Adjusted R-squared:  0.2382 \nF-statistic: 14.45 on 3 and 126 DF,  p-value: 3.798e-08\n\n\nThe summary() function provides a compact overview of the model fit for lm objects. First, a summary of the residuals (i.e., the differences between the observed and predicted values) is provided under Residuals. Next, a summary of the regression coefficients \\(\\beta\\) is provided under Coefficients, including their estimates (Estimate), standard errors (Std. Error), test statistics for t-tests that test whether the coefficients are significantly different from zero (t value), p-values of the tests (Pr(>|t|)) and statistical significance (indicated by the asterisks). For instance, we see that the number of group work events is statistically significant. The notation used for the significance levels of the tests is described following Signif. codes. Estimate of the square root of the error variance \\(\\sigma^2\\) is reported following Residual standard error. The two R-squared values are estimates of the proportion of variance of the data that is explained by the model. Finally, the F-statistic reports the results of ANOVA when applied with the same model formula that was used for the linear regression model.\nThe report() function provides a more comprehensive summary of the model fit and the regression coefficients:\n\nreport(fit)\n\nWe fitted a linear model (estimated using OLS) to predict Final_grade with\nFrequency.Applications, Frequency.Assignment and Frequency.La_types (formula:\nFinal_grade ~ Frequency.Applications + Frequency.Assignment + Frequency.La_types).\nThe model explains a statistically significant and moderate proportion of variance\n(R2 = 0.26, F(3, 126) = 14.45, p < .001, adj. R2 = 0.24). The model's intercept,\ncorresponding to Frequency.Applications = 0, Frequency.Assignment = 0 and\nFrequency.La_types = 0, is at 5.80 (95% CI [5.00, 6.60], t(126) = 14.29, p < .001).\nWithin this model:\n\n  - The effect of Frequency Applications is statistically significant and positive\n(beta = 0.08, 95% CI [0.03, 0.12], t(126) = 3.43, p < .001; Std. beta = 0.32, 95%\nCI [0.13, 0.50])\n  - The effect of Frequency Assignment is statistically non-significant and negative\n(beta = -5.05e-03, 95% CI [-0.02, 6.30e-03], t(126) = -0.88, p = 0.380; Std. beta =\n-0.08, 95% CI [-0.26, 0.10])\n  - The effect of Frequency La types is statistically significant and positive (beta\n= 0.09, 95% CI [0.03, 0.14], t(126) = 3.23, p = 0.002; Std. beta = 0.31, 95% CI\n[0.12, 0.49])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were computed\nusing a Wald t-distribution approximation.\n\n\nAgain, the report output has condensed the information about the model fit into a format that can be read in a straightforward manner.\nThe assumption of normality of the residuals can be assessed with a quantile-quantile plot, or q-q plot for short. The residuals of the model fit can be accessed with the function resid(). The function qqnorm() draws the quantiles of the residuals against the quantiles of the normal distribution. The function qqline() adds a straight line through the plot that passes through the second and third quantiles, by default. Ideally, the residuals should fall on this line, and large deviations indicate that the normality assumption may not hold.\n\n## Draw the quantiles of the residuals and the theoretical quantiles\nqqnorm(resid(fit))\n## Add a line through the theoretical quantiles\nqqline(resid(fit))\n\n\n\n\n\n\n\n\nThe vast majority of residuals fall nicely onto the line for our model. Besides the q-q plot, we can obtain more model diagnostics with the help of the performance [25] package. This package provides a wide array of tools to assess how well models fit to the data. The general-purpose function check_model( provides a visual overview of the model fit using several metrics.\n\nlibrary(\"performance\")\ncheck_model(fit, theme = see::theme_lucid(base_size = 10))\n\n\n\n\nThe functions performs various tests related to the assumptions of the linear regression model. For example, the bottom right panel contains the same q-q plot that we previously constructed using the qqnorm() and qqline() functions. We refer the reader to the documentation of the performance package for more information on the remaining tests."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#logistic-regression",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#logistic-regression",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "6 Logistic regression",
    "text": "6 Logistic regression\nLogistic regression is a similar tool to linear regression, but with a binary outcome instead of a continuous one. Instead of modeling the outcome variable directly, a linear model is constructed for the logarithmic odds of the probability of “success” for the binary outcome, e.g., obtaining a passing grade. There is also no explicit error term \\(\\varepsilon\\) in the model, as the uncertainty in the outcome is already captured by the success probability. Formally, the model is \\[\n\\mbox{logit}\\left(P(y_i = 1)\\right) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki}, \\quad i = 1,\\ldots,n,\n\\] where the logit-function is defined as \\(\\mbox{logit}(x) = \\log(x/(1-x))\\). Here, the logit-function serves as the so-called link function that connects the expected value of the response to the predictors.\nWe fit a logistic regression model where the outcome variable is the level of achievement (AchievingGroup) and the predictors are the Moodle event counts of each type. The logistic regression model is a generalized linear model: a class of models that extend the linear regression model and that can be fitted in R with the function glm(). The syntax of glm() is analogous to lm(), but we must also specify the distribution of the outcome and the link function via the family argument. We use the function binomial() and supply the argument link = \"logit\" to define that the model should be a logistic regression model (in this case the link argument is optional, as \"logit\" is the default value). Because AchievingGroup is a factor, we must first convert it into a binary response that attains values 1 and 0 (or TRUE and FALSE). We can do this within the formula via the I() function, so that we do not have to modify our data. When used in a formula, this function will first compute its argument expression when evaluated, so that the expression is not mistaken for a variable name in the data (that does not exist). We select the high achievers as the “success” category for the outcome.\n\nfit_logistic <- glm(\n  ## Use the I() function to construct a binary response in the formula\n  I(AchievingGroup == \"High achiever\") ~ Frequency.Applications + \n    Frequency.Assignment + Frequency.La_types,\n  data = all_combined,\n  ## Our response is binary, so we use the binomial family with logit link\n  family = binomial(link = \"logit\")\n)\nsummary(fit_logistic)\n\n\nCall:\nglm(formula = I(AchievingGroup == \"High achiever\") ~ Frequency.Applications + \n    Frequency.Assignment + Frequency.La_types, family = binomial(link = \"logit\"), \n    data = all_combined)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -0.66272    0.62914  -1.053  0.29217    \nFrequency.Applications  0.30443    0.07778   3.914 9.07e-05 ***\nFrequency.Assignment   -0.04477    0.01402  -3.193  0.00141 ** \nFrequency.La_types      0.12245    0.04710   2.600  0.00933 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 180.22  on 129  degrees of freedom\nResidual deviance: 120.21  on 126  degrees of freedom\nAIC: 128.21\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe summary of a glm() function output is very similar to the output of a lm() summary. First, the Call is reported, which simply restates how the model was fitted. Next, Coefficients reports the estimates of the regression coefficients \\(\\beta\\), and their standard errors and statistical significance. Lastly, two deviance measures and their degrees of freedom are reported. The null deviance is twice the difference between the log-likelihood of the saturated model and the null model, and residual deviance is twice the difference between the saturated model and the model that was fitted. In simpler terms, the saturated model is a perfect model in a sense that there is a parameter for each observation. Conversely, the null model only has the intercept term. The deviance serves as a generalization of the residual sum of squares of the linear regression model, and it can be used to assess the quality of the model fit [26].\nThe report() function is applicable to models fitted with glm().\n\nreport(fit_logistic)\n\nWe fitted a logistic model (estimated using ML) to predict AchievingGroup with\nFrequency.Applications, Frequency.Assignment and Frequency.La_types (formula:\nI(AchievingGroup == \"High achiever\") ~ Frequency.Applications +\nFrequency.Assignment + Frequency.La_types). The model's explanatory power is\nsubstantial (Tjur's R2 = 0.38). The model's intercept, corresponding to\nFrequency.Applications = 0, Frequency.Assignment = 0 and Frequency.La_types = 0, is\nat -0.66 (95% CI [-1.94, 0.56], p = 0.292). Within this model:\n\n  - The effect of Frequency Applications is statistically significant and positive\n(beta = 0.30, 95% CI [0.17, 0.48], p < .001; Std. beta = 1.62e-14, 95% CI\n[-73784.14, 73784.14])\n  - The effect of Frequency Assignment is statistically significant and negative\n(beta = -0.04, 95% CI [-0.07, -0.02], p = 0.001; Std. beta = -9.05e-16, 95% CI\n[-71374.92, 71374.92])\n  - The effect of Frequency La types is statistically significant and positive (beta\n= 0.12, 95% CI [0.04, 0.22], p = 0.009; Std. beta = -2.52e-17, 95% CI [-75547.24,\n75547.24])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were computed\nusing a Wald z-distribution approximation.\n\n\nThe output describes succinctly the model that was fitted, and the effects of the predictors on the response. The performance package is also applicable to models fitted with the glm() function.\n\ncheck_model(fit_logistic)\n\n\n\n\nWe note that a different set of diagnostic checks is carried out for the logistic regression model compared to the linear regression model. For example, there is no assumption of homoscedasticity of variance as there is no explicit error term \\(\\varepsilon\\) in the model. Again, we refer the reader to the documentation of the performance package for more details on these checks."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#conclusion",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#conclusion",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nBasic statistics are an essential component of learning analytics. Learning analytics involves the collection, analysis, and interpretation of data related to the learning process, and statistical methods are used to identify patterns and trends in this data and to draw conclusions. Basic descriptive statistics such as measures of central tendency, variability and correlation are crucial for analyzing, interpreting, and visualizing data. Understanding these concepts is important for anyone involved in conducting research with quantitative data in the field of learning analytics. Moreover, mastery of basic statistics can facilitate the comprehension of more advanced statistical methods that are commonly used in learning analytics, such as logistic regression and cluster analysis. Table 10.1 contains a summary of the statistical tests that were introduced in this chapter.\n\n\nTable 1. Summary of the statistical tests, their null hypotheses, and the number of groups they compare simultaneously.\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis\nGroups\nR function\n\n\n\n\nStudent’s t-test\nEqual means\nOne, two or paired\nt.test()\n\n\nChi-squared test\nIndependence\nOne\nchisq.test()\n\n\nFisher’s test\nIndependence\nOne\nfisher.test()\n\n\nANOVA\nEqual means\nTwo or more\naov()\n\n\nKruskal-Wallis test\nEqual medians\nTwo or more\nkruskal.test()\n\n\nLevene’s test\nHomoscedasticity\nTwo or more\nleveneTest()\n\n\nShapiro-Wilk test\nNormality\nOne\nshapiro.test()\n\n\n\n\nWe emphasize that when using any statistical test or a statistical model, it is important to keep the various assumptions related to the chosen method in mind, and to assess them beforehand whenever possible. If the assumptions are violated, the results of the method may not be reliable, and thus suitable alternatives should be considered."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#further-reading",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#further-reading",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "8 Further reading",
    "text": "8 Further reading\nThis chapter scratched the surface of the full features of packages such as correlation, report and performance that can streamline the statistical analysis and reporting process. We refer the reader to the documentation of these packages to gain a more thorough understanding of their features. These packages are part of a package collection called easystats [27] (). There are several other packages in this collection that were not discussed in this chapter that can be useful for R users working with learning analytics. The book “Learning Statistics with R” by Danielle Navarro is freely available online and provides a comprehensive introduction to statistics using R (). For a general introductory text to statistical methods and inference, see e.g., [2]."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html",
    "href": "chapters/ch06-data-visualization/ch6-viz.html",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#introduction",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#introduction",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nData visualization can be defined as “the representation and presentation of data that exploits our visual perception abilities in order to amplify cognition” [1]. It has the power to transform complex information into compelling and insightful stories that enlighten, inform, and inspire action. Data visualization is an effective tool for learning analytics, as it helps to present learners’ data in a way that is easily understandable and intuitive for students, teachers, researchers, and other stakeholders. Through the use of graphs, charts, and other visual aids, it is possible to quickly identify patterns, trends, and relationships within data that may not be immediately apparent through traditional data analysis methods.\nVisualization in learning analytics has two distinct applications. On the one hand, the use of visual dashboards has become the main vehicle for putting learning analytics into practice. By presenting data in visually appealing and intuitive ways, visualization can help promote data literacy among students and other stakeholders, encouraging greater engagement with data and fostering a culture of continuous improvement. On the other hand, learning analytics scientific production heavily relies on data visualization to present research findings in a clear and accessible manner, making it easier for readers from different scholarly backgrounds to understand and act upon research insights. Regardless of the context, the power of visualization in learning analytics lies in its ability to take complex data and turn it into meaningful insights that support better decision-making and drive improvement. By leveraging the power of visualization, learning analytics professionals can unlock the full potential of data to enhance the quality of education and promote evidence-based practice.\nIn this chapter, the reader will be guided through the process of generating meaningful and aesthetically pleasing visualizations of different types of datasets using well-known R packages. Relevant plots and plot types will be demonstrated with an explanation of their usage and usage cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. Other examples of common research scenarios in which learners’ data are visualized will be illustrated throughout the chapter. In addition to creating compelling plots, readers will also be able to generate professional-looking tables with summary statistics to report descriptive statistics."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#visualization-in-learning-analytics",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#visualization-in-learning-analytics",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "2 Visualization in learning analytics",
    "text": "2 Visualization in learning analytics\nDeveloping visualizations is a challenging task of balancing the cognitive load of users while not compromising on conveying specific insights from data [2]. Visualizations for practice in learning analytics are mostly developed for two main stakeholders: learners and instructors. Depending on the target group, a visualization or a dashboard (i.e., a collection of visualizations depicting multiple indicators) have different goals.\nLearner-facing visualizations are meant to make learners aware of their own learning and to provide them with actionable feedback on their learning. Visualizations display learners’ performance on a specific metric and compare it with a reference frame: other peers, desirable learning achievement, or their own progress over time [3]. Sense-making questions triggering reflection can be added to a visualization [4, 5], or some elements of the visualizations can be highlighted and described in words using layered storytelling [6, 7]. Another option is to gamify a dashboard, for example, by using badges [8]. To provide feedback to learners, visualizations can be augmented with links to recommended resources [9], information about specific topics to review to close the achievement gap [6], or explanations of the meaning of visualizations and their implications for the learner [10]. Current learner-facing dashboards mostly show resource use and assessment data [11], compare learners to their peers [12], display descriptive analytics rather than predictive or prescriptive analytics [10], and use self-regulated learning theory as their framework [12, 13]. Some reviews found a positive effect on student outcomes [10], while others reported mixed results [11, 14]. Showing visualizations to learners can change their behavior. For example, social network analysis visualizations have resulted in fewer cross-group commenting [15], while a visualization comparing individual submission patterns with the top 25% of students in a class led to earlier homework submissions [16].\nIn comparison, the goal of instructor-facing visualizations is to support teachrs and their decision-making process by tracking student progress. Two main types can be distinguished. Mirroring or descriptive visualizations provide insights about the learners on an aggregated or an individual level using either descriptive or comparative data. Advising or prescriptive visualizations show not only information about the learners but also alert the instructor to undertake a pedagogical action [17, 18]. Current instructor-facing visualizations mostly display course-wide information about the learners or track group work [14]. These visualizations can support teachers in facilitating student collaboration [19], planning and collecting student feedback on learning activities [20], or obtaining insights into student interactions within an online environment, such as simulations, virtual labs or online games [21, 22]. However, interpreting dashboard information is a challenging task for instructors. Although some teachers use dashboards as complementary sources of information, others act based only on the dashboard information without further investigation [23].\nA common point of criticism of learning analytics dashboards is that most of them are not grounded in learning theories [13, 14]. Data-driven evaluations of dashboards focused on dashboard acceptance, usefulness, or usability are more prevalent than pedagogically-focused evaluations [24]. Some approaches were developed to mitigate these issues. The model of user-centered learning analytics systems (MULAS) presents a set of recommendations on four interconnected dimensions: theory, design, evaluation, and feedback, and can be used to guide dashboard development [14]. Another approach is an iterative five-stage Learning Awareness Tools – User eXperience (LATUX) workflow, including problem identification, low-fidelity prototyping, high-fidelity prototyping, pilot studies, and classroom use, that can be used to develop visual analytics [25]. Finally, open learner model research could be used as a source of insights while developing learning analytics visualizations, such as dashboards [9]."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#generating-plots-with-ggplot2",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#generating-plots-with-ggplot2",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "3 Generating plots with ggplot2",
    "text": "3 Generating plots with ggplot2\nIn the previous section, we have seen how central visualization is to learning analytics. In the remainder of the chapter, we will learn how to create different types of visualizations that are relevant to different types of data related to teaching and learning. We will mostly rely on ggplot2, a popular data visualization package in R that was developed by Hadley Wickham [26]. It is based on the grammar of graphics [27], which is a systematic way of thinking about and constructing visualizations. The ggplot2 library provides a flexible and intuitive framework for creating a wide range of graphics, from basic scatter plots to complex visualizations with multiple layers. It is known for its ability to produce visually appealing and informative graphics with relatively few lines of code. It enables users to define aesthetics, such as color and size, and add layers, such as points and lines, to create customized and interactive plots. In addition, ggplot2 allows for easy customization of plot features, such as titles, axis labels, and legends.\nOverall, ggplot2 is a powerful and versatile tool for data visualization in R, and is widely used by data scientists, statisticians, and researchers in a variety of fields. In this chapter, we will cover the fundamental concepts and techniques of ggplot2, including how to create basic plots, and customize their appearance. We will start by introducing the building blocks of a ggplot2 plot, including aesthetics, layers, and scales. Then, we will create a plot from scratch step by step, showing how to customize the appearance of the plots, including how to change theme, colors, and scales. We will then explore the different types of plots that you can create with ggplot2, such as scatter plots, bar charts, and histograms.\nThroughout this chapter section, we will use datasets of students’ learning data to demonstrate how to create effective visualizations for learning analytics with ggplot2. Please, refer to Chapter 2 of this book [28] to learn more about the datasets used. By the end of this section, you will have a solid foundation in ggplot2 and be able to create basic, yet compelling visualizations to explore your data."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#the-ggplot2-grammar",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#the-ggplot2-grammar",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "4 The ggplot2 grammar",
    "text": "4 The ggplot2 grammar\nThe ggplot2 library is based on Wilkinson’s grammar of graphics [27]. The main idea is that every plot can be broken down into a set of components, each of which can be customized and combined in a flexible way. These components are:\n\nData: This is the data we want to visualize. It can be in the form of a dataframe, tibble or any other structured data format.\nAesthetic mapping (aes): It defines how variables in the data are mapped to visual properties of the plot, such as position, color, shape, size, and transparency.\nGeometric object (geom): It represents the actual visual elements of the plot, such as points, lines, bars, and polygons.\nStatistical transformation (stat): It summarizes or transforms the data in some way, such as by computing means, medians, or proportions, or by smoothing or summarizing data, or grouping them into bins.\nScale (scale): It maps values in the data to visual properties of the plot, such as color, size, or position.\nCoordinate system (coord): It defines the spatial or geographic context in which the plot is displayed, such as Cartesian coordinates, polar coordinates, or maps.\nFacet (facet) : It allows to split the data into subsets and display each subset in a separate panel. It often useful for visualizing data with multiple categories or groups.\n\nBy combining and customizing these components, we can create a wide variety of complex and informative visualizations in ggplot2. The idea behind the graphics grammar is to provide a consistent and intuitive framework for constructing plots, allowing users to focus on the data and the message they want to convey, rather than on the technical details of the visualization. In the following section, we will create a plot from scratch step by step to become familiar with the most relevant components."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#creating-your-first-plot",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#creating-your-first-plot",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "5 Creating your first plot",
    "text": "5 Creating your first plot\nWe are now going to create our first plot using ggplot2. Our example deals with a widely studied matter in learning analytics, which is the relationship between online activity and achievement. We will use a bar chart to represent the number of students that have low, moderate and high activity levels in each achievement group (high achievers vs. low achievers). In order to become familiar with the syntax of ggplot2, we will recreate the plot step by step, explaining each of the elements in the plot. Below is the final result we aim at accomplishing:\n\n\n\n\n\nFigure 1: First plot with ggplot2.\n\n\n\n\n\n5.1 Installing ggplot2\nOur first step is installing the ggplot2 library. This is usually the first step in any R script that makes use of external libraries.\n\ninstall.packages(\"ggplot2\")\n\nTo import ggplot2 we just need to use the library command and specify the ggplot2 library:\n\nlibrary(ggplot2)\n\n\n\n5.2 Downloading the data\nNext, we need to import the data that we are going to plot. For this chapter, we are using synthetic data from a blended course on learning analytics. For more details about this dataset, refer to Chapter X in this book. The data is in Excel format. We can use the library rio since it makes it easy to read data in several formats. We first install the library:\n\ninstall.packages(\"rio\")\n\nAnd import it so we can use its functions:\n\nlibrary(rio)\n\nNow we can download the data using the import function from rio and assign it to a variable named df (short for dataframe).\n\ndemo_url = \"https://github.com/sonsoleslp/labook-data/raw/main/1_moodleLAcourse/AllCombined.xlsx\"\ndf <- import(demo_url)\n\nWe can use the head command to get an idea of what the dataset looks like. To recreate the plot above we will need the AchievingGroup column —which indicates whether students’ are high achievers (to 50%) or low achievers (bottom 50%), according to their final grade— and the ActivityGroup column —which indicates whether students have a high level of activity (top 33%), moderate activity (middle 33%), or low activity (bottom 33%), according to their total number of events in the LMS.\n\nhead(df)\n\n\n\n# A tibble: 130 × 37\n   User      Name   Gender ActivityGroup AchievingGroup Surname Origin Birthdate\n   <chr>     <chr>  <chr>  <chr>         <chr>          <chr>   <chr>  <chr>    \n 1 00a05cc62 Wan    M      Low activity  Low achiever   Tan     Malay… 12.12.19…\n 2 042b07ba1 Daniel M      High activity Low achiever   Tromp   Aruba  28.5.1999\n 3 046c35846 Sarah  F      Low activity  Low achiever   Schmit  Luxem… 25.4.1997\n 4 05b604102 Lian   F      Low activity  Low achiever   Abdull… Yemen  19.11.19…\n 5 0604ff3d3 Nina   F      Low activity  Low achiever   Borg    Malta  13.6.1994\n 6 077584d71 Moham… M      High activity High achiever  Gamal   Egypt  13.7.1998\n 7 081b100cf Maxim… M      Moderate act… High achiever  Gruber  Austr… 20.12.19…\n 8 0857b3d8e Hugo   M      High activity High achiever  Pérez   Spain  22.12.19…\n 9 0af619e4b Aylin  F      Low activity  Low achiever   Barat   Kazak… 14.8.1995\n10 0ec99ce96 Polina F      Moderate act… Low achiever   Novik   Belar… 9.10.1996\n# ℹ 120 more rows\n# ℹ 29 more variables: Location <chr>, Employment <chr>,\n#   Frequency.Applications <dbl>, Frequency.Assignment <dbl>,\n#   Frequency.Course_view <dbl>, Frequency.Feedback <dbl>,\n#   Frequency.General <dbl>, Frequency.Group_work <dbl>,\n#   Frequency.Instructions <dbl>, Frequency.La_types <dbl>,\n#   Frequency.Practicals <dbl>, Frequency.Social <dbl>, …\n\n\n\n\n5.3 Creating the aesthetic mapping\nNow that we have our data, we can pass it on to ggplot2 as follows:\n\nggplot(df)\n\n\n\n\nFigure 2: Empty plot\n\n\n\n\nWe still do not see anything because we have not selected the type of chart or the variables of the data that we want to plot. First, let us specify that we want to plot the AchievingGroup column (high vs. low achievers) on the x-axis. Assigning columns of our dataset to different elements of the plot is called constructing an aesthetic mapping. We can do it by calling the aes function from ggplot2, specifying that we want to map the AchievingGroup column to the x-axis, and then passing this call to aes to our plot using the second argument of ggplot:\n\n\n5.4 Add the geometry component\n\nggplot(df, aes(x = AchievingGroup)) \n\n\n\n\nFigure 3: Empty plot with AchievingGroup in x-axis labels\n\n\n\n\nWe now see that the x-axis has the two possible values of AchievingGroup: “High achiever” and “Low achiever”. We still need to tell ggplot2 the type of chart we want to use to plot the number of students of each type. To do that we need to add a geometrical (geom) component to our plot in which we specify that we want a bar chart. We do it by adding a + sign after our call to ggplot and calling geom_bar() (the name of the geometry that represents a bar chart).\n\nggplot(df, aes(x = AchievingGroup)) + geom_bar() \n\n\n\n\nFigure 4: Basic bar plot showing students by achievement group\n\n\n\n\nNow the plot can actually be called a plot. Notice that we have not specified what we want to plot in the y-axis. When not specified, ggplot2 assumes that we want to use the count of rows.\nWe also notice that the bars are in the wrong order. By default, ggplot2 orders the values in an ascending way (alphabetically in the case of text values). If we want to enforce our own order, we need to convert the AchievingGroup column of df into a factor and provide the ordered list of values to the levels argument.\n\ndf$AchievingGroup = factor(df$AchievingGroup,\n                           levels = c(\"Low achiever\", \"High achiever\"))\n\nIf we generate our plot again, we see that the bars are now in the order we want them to be:\n\nggplot(df, aes(x = AchievingGroup)) + geom_bar() \n\n\n\n\nFigure 5: Basic bar plot showing students by achievement group after transforming the x-axis variable into a factor\n\n\n\n\n\n\n5.5 Adding the color scale\nWe still need to color our bar chart according to students’ activity level. We do that by mapping the fill aesthetic to the ActivityLevel column inside the aes. When we provide the fill property, ggplot will automatically create the appropriate legend.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + geom_bar()\n\n\n\n\nFigure 6: Basic bar plot showing students’ activity level by achievement group and colored by activity level\n\n\n\n\nAgain, we need to change the order of our legend so that it follows the logical semantic order for the activity levels (low-moderate-high):\n\ndf$ActivityGroup = factor(df$ActivityGroup,\n                          levels = c(\"Low activity\", \"Moderate activity\", \"High activity\"))\n\nIf we generate the plot again, we see now that the legend is in the right order:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + geom_bar()\n\n\n\n\nFigure 7: Basic bar plot showing students’ activity level by achievement group and colored by activity level after ordering the legend\n\n\n\n\nHowever, the stacks are still not in the right order, being the low activity students at the top of the bar, and the high activity students at the bottom, which might be counter-intuitive. To change this, we need to reverse the position of the bar using position = position_stack(reverse = TRUE) inside geom_bar:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE))\n\n\n\n\nFigure 8: Basic bar plot showing students’ activity level by achievement group and colored by activity level after ordering the stacks\n\n\n\n\nWe are getting closer but the color scheme does not quite match our intended result. To add a color scheme to our plot we need to add a scale layer. In this case, the scale is for the fill property, which is the color of the bars in our chart. There are many ways to specify the color scheme. One option is to use sequential colors from the same palette. For that we add a new layer to our plot named scale_fill_brewer and we pass the palette that we want as an argument. For example, palette number 15 would look like this:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_brewer(palette = 15)\n\n\n\n\nFigure 9: Bar plot showing students’ activity level by achievement group with sequential color scale\n\n\n\n\nAnother option is to provide a manual scale with the colors of our choice. For that we use scale_fill_manual and specify a values vector as an argument. You need to specify as many colors as unique elements in your scale. In this case we have three activity groups (for low, moderate or high activity), so we must provide three colors. There are tons of resources online where you can find or create your own palettes (e.g., Coolors, Adobe Color or Lospec). You have to provide the hexadecimal code of each color or the official color name recognized by R. Below is an example:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_manual(values = c(\"#ef6461\", \"#7AE7C7\", \"#8E518D\"))  \n\n\n\n\nFigure 10: Bar plot showing students’ activity level by achievement group with manual color scale\n\n\n\n\nLastly, a very common color scale used is Viridis. It is designed to be perceived by viewers with common forms of color blindness. To use it in our plot we just add scale_fill_viridis_d().\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d() \n\n\n\n\nFigure 11: Bar plot showing students’ activity level by achievement group with viridis color scale\n\n\n\n\nViridis is the palette we need to replicate our target plot. However, the order of the color needs to be reversed so the most dense color represents the higher activity level. We do this by reversing the direction of the palette as follows:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) \n\n\n\n\nFigure 12: Bar plot showing students’ activity level by achievement group with viridis color scale\n\n\n\n\n\n\n5.6 Working with themes\nNow that the geometry and color scheme of the bars looks like our initial plot, we notice that there are still some differences. An important one is the grey background of the plot. To change the general appearance of our plot, we may use the ggplot2 themes. Below are some examples:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + theme_dark()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + theme_classic()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1) + theme_void()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1) + theme_minimal()\n\n\n\n\n\n\nFigure 13: Bar plot using different themes: theme_dark (top left), theme_classic (top right), theme_void (bottom left), and theme_minimal (bottom right)\n\n\n\n\nWe have theme_dark with a dark background and border, theme_classic with thick axes and no grid lines, theme_void which is completely empty, and theme_minimal with a minimalistic look. There are more available in the ggplot2 documentation and even more third-party implementations. To recreate our goal plot, we select the theme_minimal. To avoid having to add the theme to all of our plots from now on, we can set a default theme for our whole project by using theme_set:\n\ntheme_set(theme_minimal())\n\nNotice how now we get theme_minimal even when we do not specify it in our code:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1)\n\n\n\n\nFigure 14: Bar plot with theme minimal by default\n\n\n\n\n\n\n5.7 Changing the axis ticks\nYou may have not noticed that another difference with our goal plot is the ticks in our y-axis. In the goal plot we count 10 by 10, whereas in our last plot we do so 20 by 20. Just like we modified the scale of the fill aesthetic when we changed the color of our bars, we can also modify the y aesthetic to adjust to our needs. We use the scale_y_continuous layer and we try different number of breaks (n.breaks), until we find what we like best:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 15)\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 3)\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 7)\n\n\n\n\n\n\nFigure 15: Bar plot with different numbers of y.axis breaks: 15 (left), 3 (middle), and 7 (right)\n\n\n\n\nWe choose 7 breaks to obtain our desired result.\n\n\n5.8 Titles and labels\nOur plot is still missing some slight modifications to be 100% equal to the original one. For instance, the axes’ titles are not the same. To specify the y-axis label, we add a new layer to our plot named ylab and we pass a string with our desired label “Number of students”:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") \n\n\n\n\nFigure 16: Bar plot with y-axis label\n\n\n\n\nWe do the same for the x-axis using xlab, and for the legend using labs:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\")\n\n\n\n\nFigure 17: Bar plot with all labels\n\n\n\n\nMore importantly, we are missing the overall title of the plot. To add it we use ggtitle and we pass our intended plot title “Activity level by achievement group”. Keep in mind that, whenever possible, it is better to add a caption to the image rather than a title on the plot. A caption is more accessible for visually impaired users since it is compatible with screen readers. In scientific papers, it is also more common to have a Figure caption than a title within the plot. In social media, it is frequent to see the title on the plot as images are often shared without context. However, many social media platforms allow to provide an alternative text which is what screen readers will read as a substitute for the image, and that is also the case in learning analytics dashboards.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") +\n  ggtitle(\"Activity level by achievement group\") \n\n\n\n\nFigure 18: Bar plot with title\n\n\n\n\n\n\n5.9 Other cosmetic modifications\nLastly, we need to do some slight modifications to the overall appearance of the plot. We do this through the generic theme function of ggplot2. We first modify the position of the legend by setting legend.position to “bottom”. We then increase the size of the axes titles, by setting axis.title to element_text(size = 12). Finally, we make the plot title bigger as well and put it in bold by setting plot.title to element_text(size = 15, face = \"bold\")). With these last changes, we have an exact replica of our original plot.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") +\n  ggtitle(\"Activity level by achievement group\") + \n  theme(legend.position = \"bottom\", \n        axis.title = element_text(size = 12), \n        plot.title = element_text(size = 15, face = \"bold\"))\n\n\n\n\nFigure 19: Bar plot with theme modifications\n\n\n\n\n\n\n5.10 Saving the plot\nSince we have obtained the desired result, we may now save it as an image to be able to use it elsewhere. For that, we first need to assign the plot to a variable (e.g., myplot).\n\nmyplot <- ggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") + \n  ggtitle(\"Activity level by achievement group\") + \n  theme(legend.position = \"bottom\", axis.title = element_text(size = 12), \n        plot.title = element_text(size = 15, face = \"bold\"))\n\nWe then use ggsave to save the plot to our filesystem. We need to specify the file path (including the extension, such as PNG, JPEG, etc.) where we want to save the plot (e.g., “bar.png”) as the first argument and pass the variable where we saved our plot (myplot) as a second argument. If we do not do this, ggplot2 assumes we want to save the latest plot that we created. Lastly, we may specify the width, height and resolution (dpi) of our plots. If we are submitting our figure to a scientific journal, we probably need a high resolution image. If we are using the figure in social media, we do not want the resolution to be so high as it would take a long time to load.\n\nggsave(\"bar.png\", myplot, width = 10000, height = 5000, units = \"px\", dpi = 900)\n\nThroughout this section, we have learned how we can create a plot from scratch using only the ggplot2 library and a simple dataset. We have seen the many customization possibilities (theme, scales, titles) that we can achieve using the different plot components without needing to rely on external tools for retouching our final graph. In the next section we will learn about new types of plots that might be more suitable for other types of data and their customization possibilities."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#types-of-plots",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#types-of-plots",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "6 Types of plots",
    "text": "6 Types of plots\nThe ggplot2 library offers many types of plots (or geoms) that you can choose from to visualize your data in several ways. In this section, we go over some of the most common types and present examples using students’ learning data.\n\n6.1 Bar plot\nWe have seen how to construct a bar plot in the previous section as an example of how to use ggplot2. But when should we use a bar plot? Bar plots are useful when we want to represent counts or any numerical variable broken down by categories. The y-axis would represent the count (or other continuous numerical variable) and the x-axis would represent the categories. Keep in mind that if the categories follow a natural order, the x-axis should respect it (for example: “Morning”, “Afternoon”, “Evening”; or “Children”, “Adults”, “Elders”). Otherwise, you can just order the x-axis alphabetically or from highest to lowest value in the y-axis.\n\nggplot(df, aes(x = AchievingGroup)) +  geom_bar(position = position_stack(reverse = TRUE)) \n\n\n\n\nFigure 20: Basic bar plot of students by achievement group\n\n\n\n\nRemember that you can add a “third dimension” to the plot by using the fill property. This is known as a ‘stacked’ bar chart and helps highlight the proportion of, in this case, students’ activity level (ActivityGroup).\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  scale_fill_viridis_d(direction = -1) + \n  geom_bar(position = position_stack(reverse = TRUE)) \n\n\n\n\nFigure 21: Basic bar plot of students by achievement group filled by activity level\n\n\n\n\nIf we care more about the actual number rather than the proportion of students with each activity level, instead of a stacked bar chart we can keep each ‘stack’ as a whole bar of their own. This plot is very useful to compare values among categories. We accomplish this by passing the position argument with the value “dodge” to the geom_bar component:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  scale_fill_viridis_d(direction = -1) + geom_bar(position = \"dodge\") \n\n\n\n\nFigure 22: Basic bar plot of students by achievement group filled by activity level with position dodge instead of stacked\n\n\n\n\nWe can now see that the highest group is represented by the low achievers with low activity, followed by the high achievers with high activity.\n\n\n6.2 Histogram\nHistograms allow us to represent the distribution of a single continuous variable. It is inherently a bar chart, but instead of each bar representing the count of a single category, it represents the count of a range of values in the x-axis (what is known as a bin). Let us, for example, create a histogram for students’ online activity. Specifically, let us see the distribution of the number of accesses to the course main page online.\nIf we look at our dataset, we can see that the name of the variable that we are interested in is Frequency.Course_view:\n\nhead(df)\n\n\n\n# A tibble: 130 × 37\n   User      Name       Surname  Origin     Gender Birthdate Location Employment\n   <chr>     <chr>      <chr>    <chr>      <chr>  <chr>     <chr>    <chr>     \n 1 00a05cc62 Wan        Tan      Malaysia   M      12.12.19… Remote   None      \n 2 042b07ba1 Daniel     Tromp    Aruba      M      28.5.1999 Remote   None      \n 3 046c35846 Sarah      Schmit   Luxembourg F      25.4.1997 On camp… None      \n 4 05b604102 Lian       Abdullah Yemen      F      19.11.19… On camp… None      \n 5 0604ff3d3 Nina       Borg     Malta      F      13.6.1994 On camp… None      \n 6 077584d71 Mohamed    Gamal    Egypt      M      13.7.1998 On camp… Part-time \n 7 081b100cf Maximilian Gruber   Austria    M      20.12.19… On camp… None      \n 8 0857b3d8e Hugo       Pérez    Spain      M      22.12.19… On camp… None      \n 9 0af619e4b Aylin      Barat    Kazakhstan F      14.8.1995 On camp… None      \n10 0ec99ce96 Polina     Novik    Belarus    F      9.10.1996 On camp… None      \n# ℹ 120 more rows\n# ℹ 29 more variables: Frequency.Applications <dbl>,\n#   Frequency.Assignment <dbl>, Frequency.Course_view <dbl>,\n#   Frequency.Feedback <dbl>, Frequency.General <dbl>,\n#   Frequency.Group_work <dbl>, Frequency.Instructions <dbl>,\n#   Frequency.La_types <dbl>, Frequency.Practicals <dbl>,\n#   Frequency.Social <dbl>, Frequency.Ethics <dbl>, Frequency.Theory <dbl>, …\n\n\nTo create a histogram for this variable we may use the geom_histogram feature of ggplot2. We just pass our dataset and map the Frequency.Course_view variable to the x axis, and we add the geometry geom_histogram:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + geom_histogram()\n\n\n\n\nFigure 23: Histogram of students’ course page views\n\n\n\n\nWe can provide our own value to the bins argument in geom_histogram to personalize how many bins we want in our plot:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + \n  geom_histogram(bins = 50)\n\n\n\n\nFigure 24: Histogram of students’ course page view with 50 bins\n\n\n\n\nWe can also personalize the color scheme using fill for the background of the bars:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + \n  geom_histogram(bins = 20,  fill = \"deeppink\" ) +  \n  scale_x_continuous(n.breaks = 10) \n\n\n\n\nFigure 25: Histogram of students’ course page view with color, fill and linewidth\n\n\n\n\nThe histogram allows us to acknowledge that most students had around 400-500 events, with another peak around 900-1000. Students with more than 1000 events were rare.\n\n\n6.3 Line plot\nAnother very widely used type of plot is the line plot. Like the histogram, it is also appropriate when we have both a numerical continuous x-axis and y-axis but it gives us a bit more liberty of what we plot and it is suitable for when we want to plot several series of data together. A very common scenario for a line plot is when we deal with timelines and we wish to visualize the evolution of a certain variable over time. Let us, for instance, plot the students’ daily events in the LMS throughout the course, a common plot in learning analytics dashboards. In the dataset that we have been using, we have the total count of events per user but not the timestamp of each event. We need to import the original event data from the dataset:\n\nev_url <- \"https://github.com/sonsoleslp/labook-data/raw/main/1_moodleLAcourse/Events.xlsx\"\nevents <- import(ev_url)\n\nThe Events.xlsx file contains all the actions that the students enrolled in this course performed in the LMS (Action) with their corresponding timestamp (timecreated): clicking on a lecture file, viewing the assignment instructions, etc.\n\nhead(events)\n\n\n\n# A tibble: 95,626 × 7\n   Event.context     user  timecreated         Component Event.name Log   Action\n   <chr>             <chr> <dttm>              <chr>     <chr>      <chr> <chr> \n 1 Assignment: Fina… 9d74… 2019-10-26 09:37:12 Assignme… Course mo… Assi… Assig…\n 2 Assignment: Fina… 9148… 2019-10-26 09:09:34 Assignme… The statu… Assi… Assig…\n 3 Assignment: Fina… 278a… 2019-10-18 12:05:28 Assignme… Course mo… Assi… Assig…\n 4 Assignment: Fina… 53d6… 2019-10-19 13:28:37 Assignme… The statu… Assi… Assig…\n 5 Assignment: Fina… aab7… 2019-10-15 23:38:13 Assignme… Course mo… Assi… Assig…\n 6 Assignment: Fina… 82ed… 2019-10-18 17:51:43 Assignme… Course mo… Assi… Assig…\n 7 Assignment: Fina… 4178… 2019-10-18 15:22:56 Assignme… Course mo… Assi… Assig…\n 8 Assignment: Fina… 82ed… 2019-10-22 13:46:51 Assignme… The statu… Assi… Assig…\n 9 Assignment: Fina… f2e9… 2019-10-15 14:58:17 Assignme… Submissio… Assi… Assig…\n10 Assignment: Fina… 53d6… 2019-10-19 13:28:38 Assignme… Course mo… Assi… Assig…\n# ℹ 95,616 more rows\n\n\nInstead of mapping timecreated directly to the x aesthetic, we can plot the timeline of the number of events per day by using as.Date(timecreated) and the geom_line geometry from ggplot2. Notice that, unlike geom_bar, if we do not provide a y aesthetic and want ggplot2 to count the number of events per day for us, we need to make it explicit by passing the stat argument with value \"count\" to geom_line.\n\nggplot(events, aes(x = as.Date(timecreated) )) + geom_line(stat = \"count\")\n\n\n\n\nFigure 26: Line plot of number of events per day\n\n\n\n\nThe line plot of students’ events allows us to identify periods of increased activity. We can see that it was low at the very beginning of the course, with some peaks corresponding to the assignment deadlines and one last peak for the final project. When the course is over, activity begins to decrease.\nTo make our plot more aesthetically pleasing, we can customize the color and line width. We do so by tweaking the color and linewidth properties of the geom_line. We can also fix the axes’ titles as we learned before. For example:\n\nggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 2) + \n  xlab (\"Date\") + ylab(\"Number of events\")\n\n\n\n\nFigure 27: Line plot of number of events per day with color, linewidth, and custom labels\n\n\n\n\nWe can also add a point to mark each date using geom_point:\n\nggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 1.5)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 28: Line plot of number of events per hour with points every hour\n\n\n\n\nBesides visualizing the events for all the students of the course, we can pinpoint specific students to follow their progress and offer them personalized support. To do this, we would need to filter our data before handing it over to ggplot2. We can filter the data using the filter function from dplyr, as we learned in Chapter 4 [29]. We first install dplyr if we do not have it:\n\ninstall.packages(\"dplyr\")\n\nThen, we import it as usual:\n\nlibrary(dplyr)\n\nWe can now filter the data and pass it on to ggplot2:\n\nevents |> filter(user == \"9d744e5bf\") |> ggplot(aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 2)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 29: Line plot of number of events per date for a single student\n\n\n\n\n\n\n6.4 Jitter plots\nIn the previous plots we have seen aggregated information for all the cohort of students as well as information for a single student. However, in some occasions, it is very useful to see the general picture while accounting for possible individual differences. For example, using our original df dataset, we can plot the number of events on the LMS, differentiating between high achievers and low achievers.\nOne option is to use geom_point to represent each students’ count of events as a single point. To do this, we map the Event column to the x aesthetic, the Frequency column to the y aesthetic, and the User column to the group aesthetic:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + \n  geom_point() +   \n  xlab(\"Achieving group\") + \n  ylab(\"Number of events\") +\n  theme(legend.position = \"bottom\", \n        legend.text = element_text(size = 7), \n        legend.title = element_blank())\n\n\n\n\nFigure 30: Jitter plot of number of events per achievement group using geom_point\n\n\n\n\nHowever, there are many points that overlap. If we use geom_jitter instead, we take advantage of the horizontal gap between the event names to spread the points and avoid the overlap:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + \n  geom_jitter() + \n  xlab(\"Achieving group\") + \n  ylab(\"Number of events\") +\n  theme(legend.position = \"bottom\", \n        legend.text = element_text(size = 7), \n        legend.title = element_blank())\n\n\n\n\nFigure 31: Jitter plot of number of events per achievement group using geom_jitter\n\n\n\n\nThe plot shows that students that are high achievers generally have a higher number of events than low achievers.\n\n\n6.5 Box plot\nWhen we have too many data points, it is often more useful to visualize summary statistics instead of all the points. Box plots are very useful in summarizing data distributions. We can create a box plot for the number of events per achievement group using geom_boxplot:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + geom_boxplot() + \n  xlab(\"Achieving group\") + ylab(\"Number of events\") \n\n\n\n\nFigure 32: Box plot of activity per achievement group\n\n\n\n\nThe lower hinge of each box indicates the 25% percentile, the thick middle line is the median, and the top hinge is the top 75% percentile. The upper whisker extends from the hinge up to the maximum value within 1.5 * IQR (inter-quantile range), whereas the lower whisker extends to the minimum value within 1.5 * IQR of the hinge. The points outside the whisker represent outliers in the distribution (i.e., values outside of the 1.5 * IQR range). As the jitter plot already hinted, the median number of events is higher in the high achieving group.\n\n\n6.6 Violin plot\nWe can also visualize the distribution of the number of events for each group using violin plots (geom_violin), but these are recommended when we have a large amount of data:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + geom_violin() + \n  xlab(\"Achieving group\") + ylab(\"Number of events\") \n\n\n\n\nFigure 33: Violin plot of total activity per achievement group\n\n\n\n\n\n\n6.7 Scatter plots\nThe examples we have seen so far have dealt with plotting a single variable alone or divided in categories. Another common scenario is to investigate the direct relationship between two or more variables. Scatter plots are used to visualize how two numerical variables relate to each other. For example, we can use them to see how LMS activity relates to grades.\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade)) + \n  geom_point() + \n  ylab(\"Final grade\") + xlab(\"Number of events\") \n\n\n\n\nFigure 34: Scatter plot of number of events vs. final grade\n\n\n\n\nIn the plot, each point represents a student. Students at the right side of the plot represent students with higher activity, while students closer to the left side of the plot, represent students with lower activity. At the same time, students with low grades are closer to the bottom of the plot, while students with high grades are closer to the top. Overall, se see an upward trend whereby students with higher activity indeed obtain better grades.\nWe can add another dimension by coloring points according to another variable. For example, we can color the points according to high vs. low achievers, so we can now where the division between the two groups is:\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade, color = AchievingGroup)) + \n  geom_point() + \n  ylab(\"Final grade\") + xlab(\"Number of events\") +\n  labs(color = \"Achievement\")\n\n\n\n\nFigure 35: Scatter plot of number of events vs. final grade colored by achievement group\n\n\n\n\nWe can add yet another dimension by mapping the size aesthetic to another variable, for example Frequency.Group_work which represents the number of events related to group work.\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade, \n               fill = AchievingGroup, size = Frequency.Group_work)) +\n  geom_point(color = \"black\", pch = 21) + \n  scale_size_continuous(range = c(1, 7)) +\n  ylab(\"Final grade\") + xlab(\"Number of events\") +\n  labs(size = \"Group work\", fill = \"Achievement\")\n\n\n\n\nFigure 36: Scatter plot of number of events vs. final grade colored by achievement group and sized by frequency of group work"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#advanced-features",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#advanced-features",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "7 Advanced features",
    "text": "7 Advanced features\n\n7.1 Plot grids\nSometimes, adding all the information in a single plot can be overwhelming and hard to interpret. For example, take a look at the following line plot that shows the number of events per day for each of the course online components:\n\nggplot(events, aes(x = as.Date(timecreated), color = Action )) + \n  scale_fill_viridis_d() +  \n  geom_line(stat = \"count\") +\n  xlab(\"Date\") + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 37: Multiple series line plot\n\n\n\n\nIf we had only a few (2-5) lines, the plot would probably look very good, but as the number of categories grow, the plot becomes unintelligible. Instead of showing all the lines together, the plot would be easier to understand if each component had their own plot. To do this, instead of mapping the Action column to the color aesthetic, we add a new component to our plot using facet_wrap and we pass the name of the column as a character string (\"Action\"). We can change the geom_line to a geom_area to enhance the visualization.\n\nggplot(events, aes(x = as.Date(timecreated))) +\n  geom_area(stat = \"count\", fill = \"turquoise\", color = \"black\") + \n  facet_wrap(\"Action\") +  \n  xlab(\"Date\") + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 38: Grid of multiple plots\n\n\n\n\n\n\n7.2 Combining multiple plots\nIn the previous example, we saw how to split a plot into multiple plots. But what happens if we want to combine multiple independent plots? For that purpose, we can use the library patchwork. Install it first if you do not have it already:\n\ninstall.packages(\"patchwork\")\n\nWe import the patchwork library:\n\nlibrary(patchwork)\n\nWe have to create the plots that we want to combine and assign each of them to a different variable. We can use previous examples from this chapter and assign them to variables named p1, p2, and p3.\n\np1 <- ggplot(df, aes(x = Frequency.Total, y = Final_grade)) + \n  geom_point() + ylab(\"Grade\") + \n  xlab(\"Total number of events\")\n\np2 <- ggplot(df, aes(x = AchievingGroup, fill = ActivityGroup )) + geom_bar(position = position_fill(reverse = T)) + \n  scale_fill_viridis_d(direction = -1) + \n  xlab(\"Achievement group\") + \n  ylab(\"Number of events\") + \n  labs(fill = \"Activity level\")\np3 <- ggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 1.5)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\nNow, if we add the three variables together separated by the + sign, the plots will be placed horizontally next to each other:\n\np1 + p2 + p3\n\n\n\n\nFigure 39: Multiple plots stacked horizontally\n\n\n\n\nIf we use the / character side instead, we lay them out vertically:\n\np1 / p2 / p3\n\n\n\n\nFigure 40: Multiple plots stacked vertically\n\n\n\n\nWe can use combinations of both signs and even leave blank spaces as follows:\n\n(p1 + p2) / ( p3 + plot_spacer())\n\n\n\n\nFigure 41: Multiple plots in a grid\n\n\n\n\nPutting plots side by side can be very useful to compare datasets and discuss the differences. Some publication venues limit the number of figures or pages of their articles, so combining several plots together can be very useful to overcome this limitation."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#creating-tables-with-gt",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#creating-tables-with-gt",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "8 Creating tables with gt",
    "text": "8 Creating tables with gt\nWe have seen earlier in this chapter multiple types of visualizations that are suitable for diverse scenarios in learning analytics. However, we must not forget the other main way of reporting results or metrics, i.e., tables. When we display a data frame in Rstudio, it is by default presented as a table, but we need to be able to extract this table and display it in a dashboard, a report or a scientific article. The library gt can help us with this endeavor. First, install it if you do not have it yet:\n\ninstall.packages(\"gt\")\n\nWe then import it, as usual:\n\nlibrary(gt)\n\nLet us create a table, for example, to display the descriptive statistics of students’ events in the LMS. Using the events dataset, we first count the number of events of each type (Event.name) per student (user) using group_by and count from dplyr. We then group by Event.name only and use the summarize function, also from dplyr, to create the mean, and standard deviation of the number of events of each type per student, as we learned in Chapter 5 [30].\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n))\n\n# A tibble: 12 × 3\n   Action        Mean     SD\n   <chr>        <dbl>  <dbl>\n 1 Applications  11.1   9.83\n 2 Assignment    56.7  34.1 \n 3 Course_view  195.  152.  \n 4 Ethics        11.7  10.7 \n 5 Feedback      24.7  16.2 \n 6 General       25.7  21.4 \n 7 Group_work   252.  163.  \n 8 Instructions  49.8  40.3 \n 9 La_types      14.5   7.58\n10 Practicals    77.1  33.8 \n11 Social        18.1  19.0 \n12 Theory        11.1   6.92\n\n\nNow that we have a data frame with the shape that we like, we can use gt to create the formatted table by simply adding gt to the pipeline of operations:\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n)) |>\n  gt()\n\n\n\n\n\nTable 1:  Table created with gt \n  \n    \n    \n      Action\n      Mean\n      SD\n    \n  \n  \n    Applications\n11.07143\n9.825022\n    Assignment\n56.68462\n34.129492\n    Course_view\n194.56154\n151.656947\n    Ethics\n11.68182\n10.669050\n    Feedback\n24.71429\n16.243082\n    General\n25.73846\n21.390991\n    Group_work\n251.90769\n162.899810\n    Instructions\n49.80000\n40.272213\n    La_types\n14.54615\n7.583245\n    Practicals\n77.07692\n33.751627\n    Social\n18.10744\n19.034093\n    Theory\n11.10484\n6.922120\n  \n  \n  \n\n\n\n\n\nWe might add some tweaks by forcing the numerical columns to have two decimals and the first column to be aligned left. You can also apply themes to the table using the library gtExtras.\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n)) |>\n  gt() |> \n  fmt_number(decimals = 2, columns = where(is.numeric)) |>\n  cols_align(align = \"left\", columns = 1)\n\n\n\n\n\nTable 2:  Table created with gt with formatting \n  \n    \n    \n      Action\n      Mean\n      SD\n    \n  \n  \n    Applications\n11.07\n9.83\n    Assignment\n56.68\n34.13\n    Course_view\n194.56\n151.66\n    Ethics\n11.68\n10.67\n    Feedback\n24.71\n16.24\n    General\n25.74\n21.39\n    Group_work\n251.91\n162.90\n    Instructions\n49.80\n40.27\n    La_types\n14.55\n7.58\n    Practicals\n77.08\n33.75\n    Social\n18.11\n19.03\n    Theory\n11.10\n6.92"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#discussion",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#discussion",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "9 Discussion",
    "text": "9 Discussion\nThe use of data visualization in the context of learning analytics has the potential to greatly enhance our understanding of student behavior and performance. By leveraging the power of tools such as ggplot2, instructors and researchers can create informative and visually appealing plots that highlight important patterns and trends in student activity, providing insights into factors that may be impacting student success. Whether through exploring patterns in LMS activity, tracking completion rates of online assignments, or monitoring engagement with course materials, data visualization can provide valuable insights that can inform instructional decisions and improve student outcomes. Ultimately, the use of data visualization in learning analytics can help to create a more data-driven and evidence-based approach to education, enabling educators to more effectively support their students’ success.\nAs we have already seen throughout the chapter, we often do not use the same plots when we are dealing with categorical variables or numerical variables; when we are plotting a single variable or two or more, etc. Moreover, on some occasions when we need very detailed information, a table might be needed instead of or in addition to a figure. Table 6.3 summarizes the most commonly used visualization types that we have seen throughout this chapter according to the number of variables and the data type. It also points to the ggplot2 geometry that is used to create each visualization.\n\n\nTable 3: Summary of the types of visualization for each data type and number of variables\n\n\n\n\n\n\n\n\nNumber of variables\nVariable types\nType of visualization\nggplot2 geometry\n\n\n\n\nOne variable\nContinuous\nHistogram\ngeom_hist()\n\n\n\nDiscrete\nBar chart\ngeom_bar()\n\n\nTwo or more variables\nBoth continuous\nScatter plot\ngeom_point()\n\n\n\nOne discrete time and\nLine chart\ngeom_line()\n\n\n\none continuous\nArea chart\ngeom_area()\n\n\n\nOne discrete and one\nBar chart\ngeom_bar()\n\n\n\ncontinuous\nBox plot\ngeom_boxplot()\n\n\n\n\nJitter plot\ngeom_jitter()\n\n\n\n\nViolin plot\ngeom_violin()\n\n\n\nBoth discrete\nStacked bar chart\ngeom_bar()\n\n\n\n\nAnother way to decide which visualization to use is to think what kind of story we want to tell or which aspect of our data we want to highlight. Figure 6.42 shows a flowchart that can help choose the most suitable visualization for your data. There are many other decision charts online made for this purpose. For example, “From Data to Viz”1 leads you to the most appropriate graph for your data and also links to the code to build it and lists common caveats you should avoid.\n\n\n\nFigure 42: Flowchart to decide the most appropriate visualization for your data\n\n\nThroughout the rest of the book, we will see other forms of data visualization that are inherent to specific learning analytics methods. For example, in Chapter 15 [31], we will learn how to represent students’ discussions in the form of social networks, and in Chapter 10 [32], we will represent students’ sequences of activities using sequence analysis. The foundations learned in this chapter are key to understanding more complex visualizations in learning analytics and are, of course, transferable to other fields as well. We encourage readers to further their knowledge of data visualization by referring to the recommended resources in the next section. Especially readers that would like to take their visualizations to the next step should consider using shiny2, a web framework for R that allows creating fully interactive web apps for data analyses such as dashboards. By adding interactivity, instead of creating a static data report, final users can switch between datasets, filter the data in different ways, customize visualizations, etc."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#additional-material",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#additional-material",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "10 Additional material",
    "text": "10 Additional material\n\nWilke, Claus. 2019. Fundamentals of Data Visualization. O’Reilly. https://clauswilke.com/dataviz/.\nRahlf, Thomas. 2019 Data visualisation with R: 111 Examples. Springer. https://doi.org/10.1007/978-3-030-28444-2.\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2019. ggplot2: Elegant Graphics for Data Analysis (Use R) https://ggplot2-book.org/index.html.\nSahin, Muhittin and Dirk Ifenthaler. 2021. Visualizations and Dashboards for Learning Analytics. Springer. https://doi.org/10.1007/978-3-030-81222-5.\nDougherty, Jack and Ilya Ilyankou. 2021. Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code https://handsondataviz.org/spreadsheet.html.\nFrom Data to Viz. https://www.data-to-viz.com/about.html\nWickham, Hadley. 2021. Mastering shiny. O’Reilly. https://mastering-shiny.org/."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html",
    "href": "chapters/ch07-prediction/ch7-pred.html",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#introduction",
    "href": "chapters/ch07-prediction/ch7-pred.html#introduction",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nPrediction of students’ performance has been a central theme within the field of learning analytics (LA) since the early days [1]. In fact, the initial conceptualization of the field has highlighted the use of digital data collected from learners to predict their success —among other usages. Such predictions hold the promise to help identify those who are at risk of low achievement, in order to proactively offer early support and appropriate intervention strategies based on insights derived from learners’ data [1, 2]. Nevertheless, the prediction of students’ performance is not unique to LA and was an important theme in related fields even before LA, e.g., academic analytics [3], educational data mining [4], and even far earlier in education research at large [5].\nSuch widespread, longstanding and continuous centrality of early and accurate prediction of students’ performance lends itself to the premise that detection of early signs could allow a timely prevention of, e.g., dropout, low achievement, or undesired outcomes in general [6]. More importantly, identifying the predictors could help inform interventions, explain variations in outcomes and inform educators of why such outcomes happened —a form of predictive modelling that is often referred to as explanatory modelling [7]. Noticeable is the famous example of the Signal system at the University of Purdue, where predictions were based on digital data collected from an online learning platform [8]. Signal produced predictions and classified students into three categories according to “safety” and presented the students with traffic-light-inspired dashboard signs, where at-risk students get a red light. However, the influence of Signal on retention rates is unclear and often debated [9]. Several other systems were designed, built and applied in practice, e.g., OU analyse at the Open University, where the system offers informative dashboards to students and teachers as well as predictive models to forecast students' performance [10].\nSuccessful prediction of students’ performance has been demonstrated repeatedly in several LA studies across the years [11]. In general, the majority of the published studies used features extracted from logged learning trace data (i.e., data about students’ interactions with online learning activities and resources) and achieved accurate predictions for a considerable number of students. Yet, most of such studies examined a single course or what is often referred to as a convenience sample (i.e., a course with a sufficiently large and accessible dataset) [11]. Studies that attempted to apply predictive modelling across several courses have not found similar success [12–15]. For instance, Finnegan et al. [15] examined 22 courses across three academic domains using student log-trace data recorded from the learning management system. The authors found considerable differences among predictive models developed for individual courses regarding their predictive power as well as the significance of features. Similar results were reported by Gašević et al. [12] who used data from nine undergraduate courses in different disciplines to examine how instructional variations affected the prediction of academic success. Gašević et al. [12] found that predictors were remarkably different across courses with no consistent pattern that would allow for having one model applicable across all courses. Similarly, Conijn et al. [13] examined 17 courses across several subjects and confirmed the considerable variability of the indicators and predictive models across courses.\nStudies within the same domain have also found significant differences in predictors and predictive models. For instance, a recent study [14] examined 50 courses with a similar course design and homogeneous pedagogical underpinning. The authors found variations among different offerings of the same course, that is, the same predictor was statistically significantly correlated with performance in one course offering, but not in the same course offered to similar students in the next year. Furthermore, some predictors were more consistent than others e.g., the frequency of online sessions was more consistent than the frequency of lectures. In a similar vein, Jovanović et al. [16] applied mixed-effect linear modelling to data from fifty combined courses and developed several predictive models with different combinations of features. All predictive models in the work by Jovanović et al. [16] were able to explain only a limited proportion of variations in students’ grades. The intraclass correlation coefficient (a measure of source of variability) of all models revealed that the main source of variability were students themselves, that is, students’ specific features not captured in the logged data, pointing to the importance of taking students’ international conditions into account.\nThe goal of this chapter is to introduce the reader to predictive LA. The next section is a review of the existing literature, including the main objectives, indicators and algorithms that have been operationalized in previous works. The remainder of the chapter is a step-by-step tutorial of how to perform predictive LA using R. The tutorial describes how to predict student success using students’ online trace log data extracted from a learning management system. The reader is guided through all the required steps to perform prediction, including the data preparation and exploration, the selection of the relevant indicators (i.e., feature engineering) and the actual prediction of student success."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#sec-2",
    "href": "chapters/ch07-prediction/ch7-pred.html#sec-2",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "2 Predictive modelling: objectives, features, and algorithms",
    "text": "2 Predictive modelling: objectives, features, and algorithms\nExtensive research in the LA field has been devoted to the prediction of different measures of student success, as proven by the existence of multiple reviews and meta-analyses on the topic [17–20]. Among the measures of student success that have been examined in the literature are student retention [21], grades [22], and course completion [23]. Predicting lack of success has also been a common target of predictive analytics, mostly in the form of dropout [24], with special interest in the early prediction of at-risk students [25, 26].\nTo predict student success, numerous indicators from varying data sources have been examined in the literature. Initially, indicators were derived from students’ demographic data and/or academic records. Some examples of such indicators are age, gender, and previous grades [27]. More recent research has focused on indicators derived from students’ online activity in the learning management system (LMS) [17, 20]. Many of such indicators are derived directly from the raw log data such as the number of total clicks, number of online sessions, number of clicks on the learning materials, number of views of the course main page, number of assignments completed, number of videos watched, number of forum posts [13, 14, 28–31]. Other indicators are related to time devoted to learning, rather than to the mere count of clicks, such as login time, login frequency, active days, time-on-task, average time per online session, late submissions, and periods of inactivity [13, 14, 32–35]. More complex indicators are often derived from the time, frequency, and order of online activities, such as regularity of online activities, e.g., regularity of accessing lecture materials [16, 36, 37], or regularity of active days [14, 16]. Network centrality measures derived from network analysis of interactions in collaborative learning settings were also considered, as they compute how interactions relate to each other and their importance [38]. Research has found that predictive models with generic indicators are only able to explain just a small portion of the overall variability in students’ performance [36]. Moreover, it is important to take into account learning design as well as quality and not quantity of learning [17, 20].\nThe variety of predictive algorithms that have been operationalized in LA research is also worth discussing. Basic algorithms, such as linear and logistic regression, or decision trees, have been used for their explainability, which allows teachers to make informed decisions and interventions related to the students “at risk” [37]. Other machine learning algorithms have also been operationalized such as kNN or random forest [39, 40], although their interpretability is less straightforward. Lastly, the most cutting-edge techniques in the field of machine learning have also made their way to LA, such as XGBoost [41] or Neural Networks [42]. Despite the fact that the accuracy achieved by these complex algorithms is often high, their lack of interpretability is often pointed out as a reason for teachers to avoid making decisions based on their outcomes [7, 43].\nIt is beyond the scope of this review to offer a comprehensive coverage of the literature. Interested readers are encouraged to read the cited literature and the literature reviews on the topics [11, 14, 17–20]"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#predicting-students-course-success-early-in-the-course",
    "href": "chapters/ch07-prediction/ch7-pred.html#predicting-students-course-success-early-in-the-course",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "3 Predicting students’ course success early in the course",
    "text": "3 Predicting students’ course success early in the course\n\n3.1 Prediction objectives and methods\nThe overall objective of this section is to illustrate predictive modelling in LA through a typical LA task of making early-in-the-course predictions of the students’ course outcomes based on the logged learning-related data (e.g., making predictions of the learners’ course outcomes after log data has been gathered for the first 2-3 weeks). The course outcomes will be examined and predicted in two distinct ways: i) as success categories (high vs. low achievement), meaning that the prediction task is approached with classification models; ii) as success score (final grades), in which case the development of regression models is required.\nTo meet the stated objectives, the following overall approach will be applied: create several predictive models, each one with progressively more learning trace data (i.e., logged data about the learners’ interactions with course resources and activities), as they become available during the course. In particular, the first model will be built using the learning traces available at the end of the first week of the course; the second model will be built using the data available after the completion of the second week of the course (i.e., the data logged over the first two weeks); then, the next one will be built by further accumulating the data, so that we have learning traces for the first three weeks, and so on. In all these models, the outcome variable will be the final course outcome (high/low achievement for classification models, that is, the final grade for regression models). We will evaluate all the models on a small set of properly chosen evaluation metrics and examine when (that is, how early in the course) we can make reasonably good predictions of the course outcome. In addition, we will examine which learning-related indicators (i.e., features of the predictive models) had the highest predictive power.\n\n\n3.2 Context\nThe context of the predictive modelling presented in this chapter is a postgraduate course on learning analytics (LA), taught at University of Eastern Finland. The course was 6 weeks long, though some assignments were due in the week after the official end of the course. The course covered several LA themes (e.g., Introductory topics, Learning theories, Applications, Ethics), and each theme was covered roughly in one week of the course. Each theme had a set of associated learning materials, mostly slides, and reading resources. The course reading resources included seminal articles, book chapters, and training materials for practical work. The course also contained collaborative project work (referred to as group projects). In the group project, students worked together in small groups to design an LA system. The group project was continuous all over the course and was designed to align with the course themes. For instance, when students learned about LA data collection, they were required to discuss the data collection of their own project. The group project has two grades, one for the group project as a whole and another for the individual contribution to the project. It is important to note here that the dataset is based on a synthetic anonymized version of the original dataset and was augmented to three times the size of the original dataset. For more details on the course and the dataset, please refer to the dataset chapter [44] of the book.\n\n\n3.3 An overview of the required tools (R packages)\nIn addition to a set of tidyverse packages that facilitate general purpose data exploration, wrangling, and analysis tasks (e.g., dplyr, tidyr, ggplot2, lubridate), in this chapter, we will also need a few additional R packages relevant for the prediction modelling tasks:\n\nThe caret (Classification And REgression Training) package [45] offers a wide range of functions that facilitate the overall process of development and evaluation of prediction models. In particular, it includes functions for data pre-processing, feature selection, model tuning through resampling, estimation of feature importance, and the like. Comprehensive documentation of the package, including tutorials, is available online1.\n\n\nThe randomForest package [46] provides an implementation of the Random Forest prediction method [47] that can be used both for the classification and regression tasks.\nThe performance package [48] offers utilities for computing indices of model quality and goodness of fit for a range of regression models. In this chapter, it will be used for estimating the quality of linear regression models. The package documentation, including usage examples, is available online2.\n\n\nThe corrplot package [49] allows for seamless visual exploration of correlation matrices and thus facilitates understanding of connections among variables in high dimensional datasets. A detailed introduction to the functionality the package offers is available online3.\n\n\n\n3.4 Data preparation and exploration\nThe data that will be used for predictive modelling in this chapter originates from the LMS of a blended course on LA. The dataset is publicly available in a GitHub repository4, while its detailed description is given in the book’s chapter on datasets [44]. In particular, we will make use of learning trace data (stored in the Events.xlsx file) and data about the students’ final grades (available in the Results.xlsx file).\nWe will start by familiarising ourselves with the data through exploratory data analysis.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rio)\n\nAfter loading the required packages, we will load the data from the two aforementioned data files:\n\nevents = import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\")\nresults = import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Results.xlsx\")\n\nWe will start by exploring the events data, and looking first into its structure:\n\nevents\n\n\n\n  \n\n\n\nSince we intend to build separate predictive models for each week of the course, we need to be able to organise the events data into weeks. Therefore, we will extend the events data frame with additional variables that allow for examining temporal aspects of the course events from the weekly perspective. To that end, we first order the events data based on the events’ timestamp (timecreated) and then add three auxiliary variables for creating the course_week variable: weekday of the current event (wday), weekday of the previous event (prev_wday), and indicator variable for the start of a new week (new_week). The assumption applied here is that each course week starts on Monday and the beginning of a new week (new_week) can be identified by the current event being on Monday (wday==\"Mon\") while the previous one was on any day other than Monday (prev_wday!=\"Mon\") :\n\nevents |>\n  arrange(timecreated) |>\n  mutate(wday = wday(timecreated, \n                     label = TRUE, \n                     abbr = TRUE, \n                     week_start = 1)) |>\n  mutate(prev_wday = lag(wday)) |>\n  mutate(new_week = ifelse((wday == \"Mon\") & (is.na(prev_wday) | prev_wday != \"Mon\"), \n                           yes = TRUE, no = FALSE)) |>\n  mutate(course_week = cumsum(new_week)) -> events\n\nHaving created the variable that denotes the week of the course (course_week), we can remove the three auxiliary variables, to keep our data frame tidy:\n\nevents |> select(-c(wday, prev_wday, new_week)) -> events\n\nWe can now explore the distribution of the events across the course weeks. The following code will give us the count and proportion of events per week (with proportions rounded to the 4th decimal):\n\nevents |>\n  count(course_week) |>\n  mutate(prop = round(n/nrow(events), 4))\n\nThe output of the above lines show that we have data for seven weeks: six weeks of the course plus one more week, right after the course officially ended but students were still able to submit assignments. We can also observe that the level of students’ interaction with course activities steadily increased up until week 5 and then started going down.\nLet us now move to examining the factor variables that represent different types of actions and logged events. First, we can check how many distinct values each of these variables has:\n\nevents |>\n  summarise(across(c(Event.context, Component:Action), n_distinct))\n\n\n\n  \n\n\n\nWe can also examine unique values of each of the four variables, but it is better to examine them together, that will help us better understand how they relate to one another and get a better idea of the semantics of events they denote. For example, we can examine how often distinct Component, Event, and Action values co-occur:\n\nevents |>\n  count(Component,Event.name, Action) |> \n  arrange(Component, desc(n))  \n\n\n\n  \n\n\n\n\n\n\nLikewise, we can explore how Action and Log values are related (i.e., co-occur):\n\nevents |>             \n  count(Action, Log) |> \n  arrange(Action)         \n\n\n\n  \n\n\n\n\n\n\nHaving explored the four categorical variables that capture information about the students’ interactions with course resources and activities, we will select the Action variable as the most suitable one for further analysis. The reason for choosing the Action variable is twofold: i) it is not overly granular (it has 12 distinct values), and thus allows for the detection of patterns in the learning trace data; ii) it captures sufficient information about the distinct kinds of interaction the events refer to. In fact, the Action variable was manually coded by the course instructor to offer a more nuanced way of analysis. The coding was performed to group actions that essentially indicate the same activities under the same label. For instance, logs of viewing feedback from the teacher were grouped under the label feedback. Practical activities (Social network analysis or Process mining) were grouped under the label practicals. In the same way, accessing the group work forums designed for collaboration, browsing, reading others’ comments, or writing were all grouped under the label group_work [50].\nWe will rename some of the Action values to make it clear that they refer to distinct topics of the course materials:\n\ntopical_action <- c(\"General\", \"Applications\", \"Theory\",  \"Ethics\", \"Feedback\", \"La_types\")\n\nevents |>\n  mutate(action = ifelse(test = Action %in% topical_action, \n                         yes = str_glue(\"Materials_{Action}\"), \n                         no = Action), \n         .keep = \"unused\") -> events\n\nLet us now visually examine the distribution of events across different action types and course weeks:\n\n# Compute event counts across action types and course weeks\nevents |>\n  count(course_week, action) |>\n  arrange(course_week, desc(n)) -> action_dist_across_weeks\n\n# Visualise the event distribution\naction_dist_across_weeks |>\n  mutate(Action = as.factor(action)) |>\n  ggplot(aes(x = course_week, y = n, fill = action)) +\n  geom_col(position = position_fill()) +\n  scale_fill_brewer(palette = 'Paired') +\n  scale_x_continuous(breaks = seq(1,7)) +\n  labs(x = \"\\nCourse week\", y = \"Proportion\\n\") +\n  theme_minimal()\n\n\n\n\nFigure 1. Distribution of action types across the course weeks\n\n\n\n\nFrom the plot produced by the above lines of code (Figure 21.1), we can observe, for example, that group work (Group_work) was the most represented type of actions from week 2 till the end of the course (week 6). It is followed by browsing the main page of the course containing the course materials, announcements and updates (Course_view) and working on practical tasks (Practicals). We can also note that the assignment-related actions (Assignment) are present mostly towards the end of the course.\nNow that we have familiarised ourselves with the events data and done some initial data preparation steps, we should do some final ‘polishing’ of the data and store it to have it ready for further analysis.\n\n# Keep only the variables to be used for further analysis and \n# Rename some of the remaining ones to keep naming consistent\nevents |> \n  select(user, timecreated, course_week, action) |>\n  rename(week = course_week, ts = timecreated) -> events\n\n# Save the prepared data in the R native format\ndir.create(\"preprocessed_data\")\nsaveRDS(events, \"preprocessed_data/events.RDS\")\n\nThe next step is to explore the grades data that we previously loaded into the results data frame\n\nresults\n\n\n\n  \n\n\n\nEven though the results dataset includes the students’ grades on individual assignments, we will be able to use just the final grade (Final_grade) since we do not have information when during the course the individual assignment grades became available.\nTo get an overall understanding of the final grade distribution, we will compute the summary statistics and plot the density function for the Final_grade variable:\n\nsummary(results$Final_grade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   5.666   7.954   7.254   9.006  10.000 \n\n\n\nggplot(results, aes(x = Final_grade)) +\n  geom_density() +\n  labs(x = \"Final grade\", \n       title = \"Distribution of the final grade\") +\n  theme_minimal()\n\n\n\n\nFigure 2. Distribution of the final course grade\n\n\n\n\nWe can clearly notice both in the summary statistics and the distribution plot (Figure 20.2) that the final grade is not normally distributed, but skewed towards higher grade values.\nAs noted in Section 7.3.1, we will build two kinds of prediction models: models that predict the final grade (regression models) as well as models that predict whether a student belongs to the group of high or low achievers (classification models). For the latter group of models, we need to create a binary variable (e.g., Course_outcome) indicating if a student is in the high or low achievement group. Students whose final grade is above the 50th percentile (i.e., above the median) will be considered as being high achievers in this course (High), the rest will be considered as having low course achievement (Low):\n\nresults |>\n  mutate(Course_outcome = ifelse(test = Final_grade > median(Final_grade),\n                                 yes = \"High\", no = \"Low\")) |>\n  mutate(Course_outcome = factor(Course_outcome)) -> results\n\nNow that we have prepared the outcome variables both for regression and classification models (Final_grade and Course_outcome, respectively), we can save them for later use in model building:\n\nresults |>\n  select(user, Final_grade, Course_outcome) |>\n  saveRDS(\"preprocessed_data/final_grades.RDS\")\n\n\n\n3.5 Feature engineering\nAfter the data has been preprocessed, we can focus on feature engineering, that is, the creation of new variables (features) to be used for model development. This step needs to be informed by the course design and any learning theory that underpins the course design, so that the features we create and use for predictive modelling are able to capture relevant aspects of the learning process in the given learning settings. In addition, we should consult the literature on predictive modelling in LA (see Section 7.2), to inform ourselves about the kinds of features that were good predictors in similar learning settings. Following such an approach, we have identified the following event-based features as potentially relevant:\n\nFeatures based on learning action counts\n\nTotal number of each type of learning actions\nAverage number of actions (of any type) per day\nEntropy of action counts per day\n\nFeatures based on learning sessions:\n\nTotal number of learning sessions\nAverage (median) session length (time)\nEntropy of session length\n\nFeatures based on number of active days (= days with at least one learning session)\n\nNumber of active days\nAverage time distance between two consecutive active days\n\n\nIn addition to the course specific features (A1), the feature set includes several course-design agnostic (i.e., not directly related to a specific course design) features (e.g., A2 and A3) that proved as good predictors in similar (blended) learning settings [14, 16, 36, 51]. Furthermore, the chosen features allow for capturing both the amount of engagement with the course activities (features A1, A2, B1, B2, C1) and regularity of engagement (features A3, B3, C2) at different levels of granularity (actions, sessions, days).\nTo compute features based on action counts per day (group A), we need to extend the events dataset with date as an auxiliary variable:\n\nevents |> mutate(date = as.Date(ts)) -> events\n\nTo compute features based on learning sessions, we need to add sessions to the events data. It is often the case that learning management systems and other digital learning platforms do not explicitly log beginning and end of learning sessions. Hence, LA researchers have used heuristics to detect learning sessions in learning events data. An often used approach to session detection consists of identifying overly long periods of time between two consecutive learning actions (of the same student) and considering them as the end of one session and beginning of the next one [14, 16, 36]. To determine such overly long time periods that could be used as “session delimiters”, LA researchers would examine the distribution of time periods between consecutive events in a time-ordered dataset, and set the delimiter to the value corresponding to a high percentile (e.g., 85th or 90th percentile) of the time distance distribution. We will rely on this approach to add sessions to the event data.\nFirst, we need to compute time distance between any two consecutive actions of each student:\n\nevents |>\n  group_by(user) |>\n  arrange(ts) |>\n  mutate(ts_diff = ts - lag(ts)) |>\n  ungroup() -> events\n\nNext, we should examine the distribution of time differences between any two consecutive actions of each student, to set up a threshold for splitting action sequences into sessions:\n\nevents |> pull(ts_diff) -> ts_diff\n\nts_diff_hours = as.numeric(ts_diff, units = 'hours')\n\nsummary(ts_diff_hours)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n  0.00000   0.00028   0.00028   1.40238   0.01694 307.05028       130 \n\n\nAs summary statistics is not sufficiently informative, we should examine the upper percentiles:\n\nquantile(ts_diff_hours, probs = seq(0.8, 1, 0.01), na.rm = TRUE) |> round(3)\n\n    80%     81%     82%     83%     84%     85%     86%     87%     88%     89% \n  0.017   0.017   0.034   0.034   0.034   0.050   0.067   0.084   0.117   0.167 \n    90%     91%     92%     93%     94%     95%     96%     97%     98%     99% \n  0.234   0.350   0.617   1.000   1.834   3.367   7.434  14.300  22.035  39.767 \n   100% \n307.050 \n\n\nConsidering the computed percentile values, on one hand, and the expected length of learning activities in the online part of the course (which included long forums discussions), we will set 1.5 hours (value between 93th and 94th percentile) as the threshold for splitting event sequences into learning sessions:\n\nevents |>\n  mutate(ts_diff_hours = as.numeric(ts_diff, units = 'hours')) |>\n  group_by(user) |>\n  arrange(ts) |>\n  mutate(new_session = (is.na(ts_diff_hours)) | (ts_diff_hours >= 1.5)) |>   \n  mutate(session_nr = cumsum(new_session))|> \n  mutate(session_id = paste0(user,\"_\", \"session_\",session_nr)) |>\n  ungroup() -> events_with_sessions\n\nWe will also add session length variable, which can be computed as the difference between the first and last action in each session, as it will be required for the computation of some of the features (B2 and B3):\n\nevents_with_sessions |>\n  group_by(session_id) |>\n  mutate(session_len = as.numeric(max(ts) - min(ts), units = \"secs\")) |>\n  ungroup() -> events_with_sessions\n\nAfter adding the necessary variables for feature computation, we can tidy up the dataset before proceeding to the feature computation. In particular, we will keep only the variables required for feature computation:\n\nevents_with_sessions <- events_with_sessions |>\n  select(user, ts, date, week, action, session_nr, session_id, session_len)\n\nAll functions for computing the event-based features outlined above are given in the feature_creation R script. In the following, we give a quick overview of those functions, while the script with further explanations is available at the book’s GitHub repository:\n\nthe total_counts_per_action_type function computes the set of features labelled as A1, that is, counts of each type of learning action, up to the current week\nthe avg_action_cnt_per_day function computed feature A2, that is, average (median) number of learning actions per day, up to the current week\nthe daily_cnt_entropy function computes feature A3, namely entropy of action counts per day, up to the current week\nthe session_based_features function computes all session-based features up to the current week: total number of sessions (B1), average (median) session length (B2), and entropy of session length (B3)\nthe active_days_count function computes the number of active days (C1), up to the current week\nthe active_days_avg_time_dist function computes avg. (median) time distance between two consecutive active days (C2), up to the current week\nfinally, the create_event_based_features function makes use of the above functions to compute all event-based features, up to the given week\n\nHaving defined the feature set and functions for feature computation, cumulatively for each week of the course, we can proceed to the development of predictive models. In the next section (Section 7.3.6), we will present the creation and evaluation of models for predicting the overall course success (high / low), whereas Section 7.3.7 will present models for predicting the final course grade.\n\n\n3.6 Predicting success category\nTo build predictive (classification) models, we will use Random Forest [47]. This decision was motivated by the general high performance of this algorithm on a variety of prediction tasks [52] as well as its high performance on prediction tasks specific to the educational domain [43].\nRandom forest (RF) is a very efficient machine learning method that can be used both for classification and regression tasks. It belongs to the group of ensemble methods, that is, machine learning methods that build and combine multiple individual models to do the prediction. In particular, RF builds and combines the output of several decision or regression trees, depending on the task at hand (classification or regression). The way it works can be briefly explained as follows: the method starts by creating a number of bootstrapped training samples to be used for building a number of decision trees (e.g. 100). When building each tree, each time a split in a tree is to be made, instead of considering all predictors, a random sample of predictors is chosen as split candidates from the full set of predictors (typically the size of the sample is set to be equal to the square root of the number of predictors). The reason for choosing a random sample of predictors is to make a diverse set of trees, which has proven to increase the performance. After all the trees have been built, each one is used to generate a prediction, and those predictions are then aggregated into the overall prediction of the RF model. In case of a classification task, the aggregation of predictions is done through majority vote, that is, the class voted (i.e., predicted) by the majority of the classification trees is the final prediction. In case of a regression task, the aggregation is done by averaging predictions of individual trees. For a thorough explanation of the RF method (with examples in R), an interested reader is referred to Chapter 8 of [53].\nWe will first load the additional required R packages as well as R scripts with functions for feature computation and model building and evaluation:\n\nlibrary(caret)\nlibrary(randomForest)\n\nsource(\"feature_creation.R\")\nsource(\"model_develop_and_eval.R\")\n\nThe following code snippet shows the overall process of model building and evaluation, one model for each week of the course, starting from week 1 to week 5. Note that week 5 is set as the last week for prediction purposes since it is the last point during the course when some pedagogical intervention, informed by the model’s output, can be applied by the course instructors.\n\nmodels <- list()\neval_measures <- list()\n\nfor(k in 1:5) {\n  \n  ds <- create_dataset_for_course_success_prediction(events_with_sessions, \n                                                  k, results)\n  set.seed(2023)\n  train_indices <- createDataPartition(ds$Course_outcome, \n                                       p = 0.8, list = FALSE)\n  train_ds <- ds[train_indices,] |> select(-user)\n  test_ds <- ds[-train_indices,] |> select(-user)\n\n  rf <- build_RF_classification_model(train_ds)\n  eval_rf <- get_classification_evaluation_measures(rf, test_ds)\n  \n  models[[k]] <- rf\n  eval_measures[[k]] <- eval_rf\n}\n\nThe process consists of the following steps, each of which will be explained in more detail below:\n\nCreation of a dataset for prediction of the course outcomes, based on the logged events data (events_with_sessions) up to the given week (k) and the available course outcomes data (results)\nSplitting of the dataset intro the part for training the model (train_ds) and evaluating the model’s performance (test_ds)\nBuilding a RF model based on the training portion of the dataset\nEvaluating the model based on the test portion of the dataset\n\nAll built models and their evaluation measures are stored (in models and eval_measures lists) so that they can later be compared.\nGoing now into details of each step, we start with the creation of a dataset to be used for predictive modelling in week k. This is done by first computing all features based on the logged events data (events_data) up to the week k, and then adding the course outcome variable (Course_outcome) from the dataset with course results (grades):\n\ncreate_dataset_for_course_success_prediction <- function(events_data, \n                                                      current_week, \n                                                      grades) {\n  features <- create_event_based_features(events_data, current_week)\n  grades |>\n    select(user, Course_outcome) |>\n    inner_join(features, by = \"user\")\n} \n\nNext, to be able to properly evaluate the performance of the built model, we need to test its performance on a dataset that the model “has not seen”. This requires the splitting of the overall feature set into two parts: one for training the model (training set) and the other for testing its performance (test set). This is done in a way that a larger portion of the dataset (typically 70-80%) is used for training the model, whereas the rest is used for testing. In our case, we use 80% of the feature set for training (train_ds) and 20% for evaluation purposes (test_ds). Since observations (in this case, students) are randomly selected for the training and test sets, to assure that we can replicate the obtained results, we initiate the random process with an (arbitrary) value (set.seed).\nIn the next step, we use the training portion of the dataset to build a RF model, as shown in the code snippet below. We train a model by tuning its mtry hyper-parameter and choose the model with optimal mtry value based on the Area under the ROC curve (AUC ROC) metric. The mtry hyper-parameter defines the number of features that are randomly chosen at each step of tree branching, and thus controls how much variability will be present among the trees that RF will build. It is one of the key hyper-parameters for tuning RF models and its default value (default_mtry) is equal to the square root of the number of features (n_features). Hence, we create a grid that includes the default value and a few values around it.\n\nbuild_RF_classification_model <- function(dataset) {\n  \n  #defining the model hyperparameter (mtry) that we want to tune \n  n_features <- ncol(dataset)-1\n  default_mtry <- round(sqrt(n_features))\n  grid <- expand.grid(.mtry = (default_mtry-1):(default_mtry+1))\n  \n  #setting that we want to train the model through 10-fold cross-validation\n  ctrl <- trainControl(method = \"CV\", \n                       number = 10,\n                       classProbs = TRUE,\n                       summaryFunction = twoClassSummary)\n  \n  # initiating the training process and setting the evaluation measure \n  # (ROC) for choosing the best value of the tuned hyperparameter \n  rf <- train(x = dataset |> select(-Course_outcome),\n              y = dataset$Course_outcome,\n              method = \"rf\",\n              metric = \"ROC\",\n              tuneGrid = grid,\n              trControl = ctrl)\n  \n  rf$finalModel\n}\n\nThe parameter tuning is done through 10-fold cross-validation (CV). K-fold CV is a widely used method for tuning parameters of machine learning models. It is an iterative process, consisting of k iterations, where the training dataset is randomly split into k folds of equal size, and in each iteration, k-1 folds are used for training the model whereas the k-th fold is used for evaluating the model on the chosen performance measure (e.g., ROC AUC, as in our case). In particular, in each iteration, a different fold is used for evaluation purposes, whereas the remaining k-1 folds are used for training. When this iterative process is finished, the models’ performance, computed in each iteration, are averaged, thus giving a more stable estimate of the performance for a particular value of the parameter being tuned. CV is often done in 10 iterations, hence the name 10-fold CV.\nThe final step is to evaluate each model based on the test data. To that end, we compute four standard evaluation metrics for classification models —Accuracy, Precision, Recall, and F1— as shown in the code snippet below. These four metrics are based on the so-called confusion matrix, which is, in fact, a cross-tabulation of the actual and predicted counts for each value of the outcome variable (i.e., class).\n\nget_classification_evaluation_measures <- function(model, test_data) {\n  \n  # use the model to make predictions on the test set\n  predicted_vals <- predict(model, \n                            test_data |> select(-Course_outcome))\n  actual_vals <- test_data$Course_outcome\n  \n  # create the confusion matrix (see Figure 3)\n  cm <- table(actual_vals, predicted_vals)\n  \n  TP <- cm[2,2] \n  TN <- cm[2,2]\n  FP <- cm[1,2]\n  FN <- cm[2,1]\n  \n  # compute evaluation measures based on the confusion matrix\n  accuracy = sum(diag(cm)) / sum(cm)\n  precision <- TP / (TP + FP)\n  recall <- TP / (TP + FN)\n  F1 <- (2 * precision * recall) / (precision + recall)\n  \n  c(Accuracy = accuracy, \n    Precision = precision, \n    Recall = recall, \n    F1 = F1)\n}\n\n\n\n\nFigure 3. Confusion matrix for the prediction of the students’ overall course success\n\n\nIn our case, the confusion matrix has the structure as shown on Figure 7.3. In rows, it has the counts of the actual number of students in the high and low achievement groups, whereas the columns give the predicted number of high and low achievers. We consider low course achievement as the positive class, since we are primarily interested in spotting those students who might benefit from a pedagogical intervention (to prevent a poor course outcome). Hence, TP (True Positive) is the count of students who had low course achievement and were predicted by the model as such. TN (True Negative) is the count of those who were high achieving in the course and the model predicted they would be high achievers. FP (False Positive) is the count of those who were high achievers in the course, but the model falsely predicted that they would have low achievement. Finally, FN (False Negative) is the count of students who were predicted to have high achievement in the course, but actually ended up in the low achievement group. These four count-based values forming the confusion matrix serve as the input for computing the aforementioned standard evaluation measures (Accuracy, Precision, Recall, and F1) based on the formuli given in the code snippet above.\nAfter the predictive models for weeks 1 to 5 are built and evaluated, we combine and compare their performance measures:\n\neval_df <- bind_rows(eval_measures)\neval_df |>\n  mutate(week = 1:5) |>\n  mutate(across(Accuracy:F1, \\(x) round(x, digits = 4))) |>\n  select(week, Accuracy:F1) \n\n\n\n  \n\n\n\n\n\n\nThe results above shows the resulting comparison of the built models. According to all measures, models 2 and 3, that is, models with the data from the first two and first three weeks of the course, are the best. In other words, the students' interactions with the course activities in the first two-three weeks are the most predictive of their overall course success. In particular, the accuracy of these models is 84%, meaning that for 84 out of 100 students, the models will correctly predict if the student would be a high or low achiever in this course. These models have precision of 75%, meaning that out of all the students for whom the models predict will be low achievers in the course, 75% will actually have low course achievement. In other words, the models will underestimate students’ performance in 25% of predictions they make, by wrongly predicting that students would have low course achievement. The two best models have perfect recall (100%), meaning that the models would identify all the students who will actually have low course performance. These models outperform the other three models also in terms of the F1 measure, which was expected considering that this measure combines precision and recall giving them equal relevance. Interestingly, the studies exploring predictive models on weekly basis have found similar high predictive power for models developed around the second week of the course [54]\nRF allows for estimating the relevance of features used for model building. In a classification task, RF estimates feature relevance as the total decrease in the impurity (measured by the Gini index) of leaf nodes from splitting on a particular feature, averaged over all the trees that a RF model builds [46]. We will use this RF’s functionality to compute and plot the importance of features in the best model. The function that does the computation and plotting is given below.\n\ncompute_and_plot_variable_importance <- function(rf_model) {\n  importance(rf_model, type = 2) |> \n    as.data.frame() |> \n    (function(x) mutate(x, variable = rownames(x)))() -> var_imp_df\n  \n  row.names(var_imp_df) <- 1:nrow(var_imp_df)\n  colnames(var_imp_df) <- c(\"importance\",\"variable\")\n  \n  ggplot(var_imp_df, \n         aes(x = reorder(variable, importance), y = importance)) +\n    geom_col(width = 0.35) +\n    labs(x = \"\", y = \"\", title = \"Feature importance\") +\n    coord_flip() +\n    theme_minimal()\n}\n\nThe plot produced by this function for one of the best models (Model 2) is given in Figure 20.4.\n\ncompute_and_plot_variable_importance(models[[2]])\n\n\n\n\nFigure 4. The importance of features in the best course outcome prediction model, as estimated by the RF algorithm\n\n\n\n\nAs Figure 20.4 shows, features denoting the overall level of activity (avg_session_len, session_cnt) are those with the highest predictive power. They are followed by entropy-based features, that is, features reflective of the regularity of study. These findings are in line with the LA literature (e.g., [14, 36, 37]. It should be also noted that the feature reflective of the level of engagement in the group work (Group_work_cnt) is among the top 5 predictors, which can be explained by the prominent role of group work in the course design.\n\n\n3.7 Predicting success score\nTo predict the students’ final grades, we will first try to build linear regression models, since linear regression is one of the most often used regression methods in LA [43]. To that end, we will first load a few additional R packages:\n\nlibrary(performance)\nlibrary(corrplot)\n\nConsidering that a linear regression model can be considered valid only if it satisfies a set of assumptions that linear regression, as a statistical method, is based upon (linearity, homogeneity of variance, normally distributed residuals, and absence of multicollinearity and influential points), we will first examine if our data satisfies these assumptions. In particular, we will compute the features based on the events data from the 1st week of the course, build a linear regression model using the computed features, and examine if the resulting model satisfies the assumptions. Note that we limit our initial exploration to the logged events data over the 1st week of the course since we aim to employ a regression method that can be applied to any number of course weeks; so, if the data from the first course week allow for building a valid linear regression model, we can explore the same method further; otherwise, we need to choose a more robust regression method, that is, method that is not so susceptible to imperfections in the input data.\nHaving created the dataset for final grade prediction based on the week 1 events data, we will split it into training and test sets (as done for the prediction of the course outcome, Section 7.3.6), and examine correlations among the features. The latter step is due to the fact that one of the assumptions of linear regression is the absence of high correlation among predictor variables. In the code below, we use the corrplot function to visualise the computed correlation values (Figure 20.5), so that highly correlated variables can be easily observed.\n\nds <- create_dataset_for_grade_prediction(events_with_sessions, 1, results)\n  \nset.seed(2023)\ntrain_indices <- createDataPartition(ds$Final_grade, p = 0.8, list = FALSE)\ntrain_ds <- ds[train_indices,] |> select(-user)\ntest_ds <- ds[-train_indices,] |> select(-user)\n\n# examine correlations among the variables: for a linear regression model, \n# they must not be highly mutually correlated\ncorrplot(train_ds |> select(-Final_grade) |> cor(), \n         method = \"number\", type = \"lower\",\n         diag = FALSE, order = 'hclust',\n         tl.cex = 0.75, tl.col = 'black', tl.srt = 30, number.cex = 0.65)\n\n\n\n\nFigure 5. Correlations among variables in the feature set\n\n\n\n\nFigure 20.5 indicates that there are a couple of features that are highly mutually correlated. These will be removed before proceeding with the model building. While there is no universal agreement on the correlation threshold above which features should be considered overly correlated, correlation coefficients of 0.75 and -0.75 are often used as the cut-off values [53].\n\ntrain_ds |> select(-c(session_cnt, Course_view_cnt, \n                      active_days_cnt, entropy_daily_cnts)) -> train_ds_sub\n\nWe can now build a model and check if it satisfies the assumptions of linear regression:\n\nlr <- lm(Final_grade ~ ., data = train_ds_sub)\ncheck_model(lr)\n\n\n\n\nFigure 6. The output of the check_model function enables visual verification of the assumptions that linear regression is based upon\n\n\n\n\nThe check_model function from the performance R package [48] allows for seamless, visual verification of whether the assumptions are met. The output of this function when applied to our linear regression model (lr) is shown on Figure 20.6. As the figure shows, two important assumptions of linear models are not met, namely linearity and homoscedasticity (i.e. homogeneity of variance). Therefore, linear regression cannot be used with the given feature set. Instead, we have to use a regression method that does not impose such requirements on the data distribution. Since Random forest is such a method and it has already proven successful with our dataset on the classification task (Section 7.3.6), we will use it to build regression models that predict students’ final grades.\nBefore moving to regression with Random forest, it is worth noting that, in addition to checking all model assumptions at once, using the check_model function, one can also check each assumption individually using appropriate functions from the performance R package. For example, from Figure 20.6, one can not clearly see the X-axis of the collinearity plot and might want to explore this assumption more closely. That can be easily done using the check_collinearity function:\n\ncheck_collinearity(lr)\n\n\n\n  \n\n\n\nFrom the function’s output, we can clearly see the VIF (Variance Inflation Factor) values for all the features and a confirmation that the assumption of the absence of multicollinearity is satisfied. The documentation of the performance 5 package provides the whole list of functions for different ways of checking regression models.\nTo build and compare regression models in each course week, we will follow a similar procedure to the one applied when building classification models (Section 7.3.6); the code that implements it is given below. The differences are in the way that the dataset for grade prediction is built (create_dataset_for_grade_prediction), the way that regression models are built (build_RF_regression_model) and evaluated (get_regression_evaluation_measures), and these will be explained in more detail below.\n\nregression_models <- list()\nregression_eval <- list()\nfor(k in 1:5) {\n  print(str_glue(\"Starting computations for week {k} as the current week\"))\n  \n  ds <- create_dataset_for_grade_prediction(events_with_sessions, k, results)\n  \n  set.seed(2023)\n  train_indices <- createDataPartition(ds$Final_grade, p = 0.8, list = FALSE)\n  train_ds <- ds[train_indices,] |> select(-user)\n  test_ds <- ds[-train_indices,] |> select(-user)\n\n  rf <- build_RF_regression_model(train_ds)\n  eval_rf <- get_regression_evaluation_measures(rf, train_ds, test_ds)\n  \n  regression_models[[k]] <- rf\n  regression_eval[[k]] <- eval_rf\n}\n\nTo create a dataset for final grade prediction in week k, we first compute all features based on the logged events data (events_data) up to the week k, and then add the final grade variable (Final_grade) from the dataset with course results (grades):\n\ncreate_dataset_for_grade_prediction <- function(events_data, current_week, grades) {\n  features <- create_event_based_features(events_data, current_week)\n  grades |> \n    select(user, Final_grade) |>\n    inner_join(features, by = \"user\")\n} \n\nAs can be observed in the code snippet below, building a RF regression model is very similar to building a RF classification model. The main difference is in the evaluation measure that is used for selecting the optimal mtry value in the cross-validation process - here, we are using RMSE (Root Mean Squared Error), which is a standard evaluation measure for regression models [53]. As its name suggests, RMSE is the square root of the average squared differences between the actual and predicted values of the outcome variable on the test set.\n\nbuild_RF_regression_model <- function(dataset) {\n  \n  n_features <- ncol(dataset)-1\n  default_mtry <- round(sqrt(n_features))\n  grid <- expand.grid(.mtry = (default_mtry-1):(default_mtry+1))\n  \n  ctrl <- trainControl(method = \"CV\", \n                       number = 10)\n  \n  rf <- train(x = dataset |> select(-Final_grade),\n              y = dataset$Final_grade,\n              method = \"rf\",\n              metric = \"RMSE\",\n              tuneGrid = grid,\n              trControl = ctrl)\n  \n  rf$finalModel\n}\n\nFinally, to evaluate each model on the test data, we compute three standard evaluation metrics for regression models, namely RMSE, MAE (Mean Absolute Error), and R2. MAE is the average value of the absolute differences between the actual and predicted values of the outcome variable (final grade) on the test set. Finally, R2 (R-squared) is a measure of variability in the outcome variable that is explained by the given regression model. The computation of the three evaluation measures is shown in the code below.\n\nget_regression_evaluation_measures <- function(model, train_ds, test_data) {\n  \n  predicted_vals <- predict(model, \n                            test_data |> select(-Final_grade))\n  actual_vals <- test_data$Final_grade\n  \n  # R2 = 1 - RSS/TSS\n  # RSS - Residual Sum of Squares\n  RSS <- sum((predicted_vals - actual_vals)^2)\n  # TSS - Total Sum of Squares\n  TSS <- sum((median(train_ds$Final_grade) - actual_vals)^2)\n  R2 <- 1 - RSS/TSS\n  \n  # RMSE = sqrt(RSS/N)\n  RMSE <- sqrt(RSS/nrow(test_ds))\n  \n  # MAE = avg(abs(predicted - actual))\n  MAE <- mean(abs(predicted_vals - actual_vals))\n  \n  c(R2 = R2, RMSE = RMSE, MAE = MAE)\n}\n\nAfter the regression models for weeks 1 to 5 are built and evaluated, we combine and compare their performance measures, with the results reported below.\n\nregression_eval_df <- bind_rows(regression_eval)\nregression_eval_df |>\n  mutate(WEEK = 1:5) |>\n  mutate(across(R2:MAE, \\(x) round(x, digits = 4))) |>\n  select(WEEK, R2, RMSE, MAE)\n\n\n\n  \n\n\n\n\n\n\nAs shown above, in this case, we do not have a clear situation as it was with the classification task, since the three evaluation measures point to different models as potentially the best ones. In particular, according to R2, the best model would be model 5 (i.e, the model based on the data from the first 5 weeks), whereas the other two measures point to the 2nd or 3rd model as the best. Considering that i) RMSE and MAE measures are considered more important than R2 when evaluating the predictive performance of regression models [13] and ii) RMSE and MAE values for models 2 and 3 are very close, while the 2nd model is better in terms of R2, we will conclude that the 2nd model, that is, the model based on the logged event data from the first two weeks of the course is the best model. This model explains 84.23% of variability in the outcome variable (final grade), and predicts it with an average absolute error of 0.4531, which can be considered a small value with respect to the value range of the final grade [0-10].\nTo estimate the importance of features in the best regression model, we will again leverage the RF’s ability. We will use the same function as before (compute_and_plot_variable_importance) to estimate and plot feature importance. The only difference will be that the importance function (from the randomForest package) will internally use residual sum of squares as the measure of node impurity when estimating the features importance. Figure 7.7 shows that, as in the case of predicting the overall course success (Figure 20.4), regularity of study features (avg_aday_dist, entropy_daily_cnts, session_len_entropy) are among the most important ones. In addition, the number of learning sessions (session_cnt), as an indicator of overall activity in the course, is also among the top predictors.\n\ncompute_and_plot_variable_importance(regression_models[[2]])\n\n\n\n\nFigure 7. The importance of features in the best final grade prediction model, as estimated by the RF algorithm"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#concluding-remarks",
    "href": "chapters/ch07-prediction/ch7-pred.html#concluding-remarks",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "4 Concluding remarks",
    "text": "4 Concluding remarks\nThe results of predictive modelling presented in the previous section show that, in the examined postgraduate course on LA, we can make fairly accurate predictions of the students’ course outcomes already in the second week of the course. In fact, both classification and regression models, that is, prediction of the students’ overall course success and final grades, proved to be the most accurate when based on the logged learning events data from the first two or three course weeks. That students’ learning behaviour in the first part of the course is highly predictive of their course performance, which is in line with related research on predictive modelling (e.g., [54–56]). It should be also noted that the high performance of the presented predictive models can be partially explained by the well chosen feature set and the used algorithm (Random forest) that generally performs well on prediction tasks. However, it may also be due to the relatively large dataset. As noted in Section 7.3.1, we used a synthetic anonymized version of the original dataset that is three times larger than the original dataset.\nConsidering the features that proved particularly relevant for predicting the students’ course performance, we note that in both kinds of predictive tasks —-course success and final grade prediction— features reflective of regularity of study stand out. In addition, features denoting the overall level of engagement with online learning activities and resources also have high predictive power. It is also worth noting that the highly predictive features are session level features, suggesting that learning session is the right level of granularity (better than actions or active days) for predictive modelling in the given course. In fact, this finding is in line with earlier studies that examined predictive power of a variety of features derived from learning traces [13, 14, 36] . Note that due to the purpose of this chapter to serve as introductory reading to predictive modelling in LA, we based the feature set on relatively simple features and used only one data source for feature creation. For more advanced and diverse feature creation options, interested readers are referred to, for example [57–59].\nThe algorithm used for building predictive models, namely Random forest, offers the advantage of flexibility in terms of the kinds of data it can work with (unlike, for example, linear regression which is based on several assumptions about data distribution) as well as fairly good prediction results it tends to produce. On the other hand, the algorithm is not as transparent as simpler algorithms are (e.g., linear regression or decision trees) and thus its use might raise issues of teachers’ trust and willingness to rely on the models’ output. On the positive side, the algorithm offers an estimate of feature importance thus shedding some light on the underlying “reasoning” process that led to its output (i.e., predictions).\nTo sum up, predictive modelling, as applied in LA, can bring about important benefits in the form of early in the course detection of students who might be struggling with the course and pointing out indicators of learning behaviour that are associated with poor course outcomes. With such insights available, teachers can make better informed decisions as to the students who need support and the kind of support they might benefit from. However, predictive modelling is also associated with challenges, especially practical challenges related to the development and use of such models, including availability and access to the data, interpretation of models and their results, and the associated issue of trust in the models’ output."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#suggested-readings",
    "href": "chapters/ch07-prediction/ch7-pred.html#suggested-readings",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "5 Suggested readings",
    "text": "5 Suggested readings\n\nMax Kuhn & Julia Silge (2022). Tidy Modeling with R: A Framework for Modeling in the Tidyverse. O’Reilly. https://www.tmwr.org/\nBradley Boehmke & Brandon Greenwell (2020). Hands-On Machine Learning with R. Taylor & Francis. https://bradleyboehmke.github.io/HOML/\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. 2nd Edition. Springer US. https://doi.org/10.1007/978-1-0716-1418-1"
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html",
    "title": "9  An introduction and tutorial to model-based clustering in education via latent profile analysis",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#literature-review",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#literature-review",
    "title": "9  An introduction and tutorial to model-based clustering in education via latent profile analysis",
    "section": "1 Literature review",
    "text": "1 Literature review\nExamples of mixture models being applied in educational research settings are relatively few compared to other methods of clustering. Some notable examples exist which address patterns in students’ online learning [9] or patterns in students’ disposition [10] or collaborative roles [11]. Most studies in education research that applied mixture models used latent profile analysis (LPA) to identify students’ profiles from self-reported data. For example, [10] performed LPA on a data set of 318 students’ survey responses about emotional self-efficacy, motivation, self-regulation, and academic performance, and identified four profiles: “low”, “average”, “above average with a low ability to handle the emotions of others”, and “high emotional self-efficacy”. In the work by [12], the authors analyzed 615 vocational education students’ achievement emotions in online learning environments, and found three profiles: “blends of negative emotions”, “nonemotional”, and “pure positive emotion”. [13] employed LPA on self-report data on classroom engagement from 413 first-year university students in Vietnam and found three profiles: “highly engaged”, “moderately engaged”, and “minimally engaged”. [14] collected survey responses from 2,339 engineering undergraduates about 28 noncognitive and affective factors using a survey instrument and using Gaussian mixture models found four very distinct profiles of students.\nThe analysis of online trace log data —which is at the core of learning analytics data— with mixture models is even less common. In the study by [15], the authors applied LPA to variables related to debugging derived from students’ programming problems submission traces. They found a profile with higher debugging accuracy and coding speed, another profile with lower debugging performance in runtime and logic errors, and a third profile with lower performance in syntactic errors who tended to make big changes in every submission. Studies covering online collaborative learning are even more scarce. A rare example is the study by [11], in which the authors used latent profile analysis to identify students’ roles in collaboration based on their centrality measures. The mixture models identified three collaborative roles that represented a distinct pattern of collaboration: leaders, who contribute the most to the discussion, whose arguments are more likely to spread; mediators, who bridge others and moderate the discussions; as well as isolates, who are socially idle and do not contribute to the discussion.\nA considerable number of studies that applied mixture models further investigate the association between profile membership and academic achievement. For example, in the aforementioned study by [10], students with high emotional self-efficacy had higher academic performance than the other profiles. In the study by [15], the authors found that higher debugging accuracy was related to higher scores in all exams, whereas there were no differences between the two other identified profiles. By the same token, researchers have attempted to find reasons why a certain profile emerged, or what are the variables that are more associated with one profile more than the other. For example, [13] found that peer support, provision of choice, and task relevance are the factors more likely to predict classroom engagement profile membership. [10] found that self-regulation and motivation played significant roles in determining profile membership.\nClearly, there are plenty of opportunities for further exploration and investigation in this area that could augment our knowledge of learning, learners’ behavior, and the variabilities of learning processes [2]. This is especially true given the numerous advantages of the MBC paradigm over more traditional, heuristic clustering algorithms, which we imminently describe. Subsequently, in the rest of this chapter we elaborate on the theoretical underpinnings of the family of Gaussian parsimonious clustering models implemented in the mclust R package and additionally explore some advanced features of the package, which we employ in an analysis of a real educational research application thereafter. Finally, we conclude with a brief discussion.\n##Model-based clustering As stated above, clustering methods, in a general sense, are used to uncover group structure in heterogeneous populations and identify patterns in a data set which may represent distinct subpopulations. While there is no universally applicable definition of what constitutes a cluster [16], it is commonly assumed that clusters should be well separated from each other and cohesive in an ideal analysis [17]. Conversely, objects within a cluster should be more similar to each other in some sense, in such a way that an observation has a defined relationship with observations in the same cluster, but not with observations from other clusters.\nTraditional clustering approaches, like the aforementioned \\(k\\)-means algorithm, and agglomerative hierarchical clustering, use dissimilarity-based heuristics to ultimately produce a “hard” partition of cases into groups, such that each observation is associated with exactly one cluster only. As such approaches are not underpinned by a statistical model, assessment of the optimal number of clusters is often a fraught task, lacking the guidance of principled statistical model selection criteria. However, we note that \\(k\\)-means can be recasted as a clustering model assuming a Gaussian mixture with equal proportions and diagonal equal covariance matrix across groups. Moreover, some (but not all) agglomerative hierarchical clustering models can be rooted in a statistical model also, as discussed in [18].\nConversely, the MBC paradigm typically assumes that data arise from a (usually finite) mixture of probability distributions, whereby each observation is assumed to be generated from a specific cluster, characterised by an associated distribution in the mixture [19]. Ideally, mixtures of distributions are supposed to provide a good model for the heterogeneity in a data set; that is, once an observation has been assigned to a cluster, it is assumed to be well-represented by the associated distribution. As such, MBC methods are based on a formal likelihood and seek to estimate parameters (e.g., means, variances, and covariances, which may or may not differ across groups) which best characterise the different distributions. Rather than yielding only a “hard” partition, each observation is assigned a probability of being associated with each mixture component —such that observations can have non-negative association with more than one cluster— from which a hard partition can be constructed. These probabilities are treated as weights when estimating the component parameters, which brings the advantages of minimising the effect of observations lying near the boundary of two natural clusters (e.g., a student with an ambiguous learning profile) and being able to quantity the uncertainty in the cluster assignment of a particular observation to provide a sense of cases for which further investigation may be warranted. Compared to other approaches, the other main advantages of this statistical modelling framework are its ability to use statistical model selection criteria and inferential procedures for evaluating and assessing the results obtained.\nInference for finite mixture models is routinely achieved by means of the expectation-maximisation (EM) algorithm [20], under which each observation’s component membership is treated as a “missing” latent variable which must be estimated. This formulation assumes that the data are conditionally independent and identically distributed, where the conditioning is with respect to a latent variable representation of the data in which the latent variable indicates cluster membership. Given the relative familiarity of latent class and latent profile terminology in the social sciences, we now explicitly cast MBC methods in the framework of latent variable modelling.\n\n1.1 Latent variable models\nLatent variable models are statistical models that aim to explain the relationships between observed variables by introducing one or more unobserved or latent variables. The idea behind latent variable models is that some of the underlying constructs or concepts we are interested in cannot be measured directly, but only through their effects on observable variables. Latent variable modelling has a relatively long history, dating back from the measure of general intelligence by factor analysis [21], to the structural equation modelling approach [22], from topic modelling, such as the latent Dirichlet allocation algorithm [23], to hidden Markov models for time series [24] and longitudinal data [25]. Latent variable models are widely used in various fields, including psychology, sociology, economics, and biology, to name a few. They are particularly useful when dealing with complex phenomena that cannot be easily measured or when trying to understand the underlying mechanisms that drive the observed data.\nWhen discussing latent variable modelling, it is useful to consider the taxonomy presented by [26]. This can be particularly helpful, as the same models are sometimes referred to by different names in different scientific disciplines. [26, Table 1.3] considered a cross-classification of latent variable methods based on the type of variable (manifest or latent) and its nature (metrical or categorical). If both the manifest and latent variables are metrical, the model is called a factor analysis model. If the manifest variables are categorical and the latent variables are metrical, the model is called a latent trait model or item response theory model. If the manifest variables are metrical and the latent variables are categorical, the model is called a latent profile analysis model. If both the manifest and latent variables are categorical, the model is called a latent class model.\nIn this scheme, finite Gaussian mixture models described in this chapter assume that the observed variables are continuous and normally distributed, while the latent variable, which represents the cluster membership of each observation, is categorical. Therefore, Gaussian mixtures belong to the family of latent profile analysis models. This connection is made apparent by the tidyLPA R package [27], which leverages this equivalence to provide an interface to the well-known mclust R package [7] used throughout this chapter, using tidyverse syntax and terminology which is more familiar in the LPA literature.\n\n\n1.2 Finite Gaussian mixture models\nAs described above, finite mixture models (FMMs) provide the statistical framework for model-based clustering and allow for the modelling of complex data by combining simpler distributions. Specifically, a FMM assumes that the observed data are generated from a finite mixture of underlying distributions, each of which corresponds to a distinct subgroup or cluster within the data. Gaussian mixture models (GMMs) are a particularly widespread variant of FMMs which specifically assume that each of the underlying distributions is a (multivariate) Gaussian distribution. This means that the data within each cluster are normally distributed, but with potentially different means and covariance matrices. The relevance of GMMs stems from the well-established fact that mixtures of Gaussians can provide an accurate approximation to any continuous density.\nIn FMMs, the latent variable represents the cluster assignment for each observation in the data. It is a categorical variable assuming one of a finite set of possible values that correspond to different clusters. Alternatively, it can be encoded using a set of indicator variables, which take the value of 1 for the cluster to which the observation belongs, and 0 for all other clusters.\nTo estimate the parameters of a GMM with the associated latent variable for cluster membership, a likelihood-based approach is typically used. The likelihood function expresses the probability of observing the data, given the parameter values and the latent variable. The maximum likelihood estimation (MLE) method is commonly used to estimate the parameters and the latent variable which maximise the likelihood function. Usually, the number of clusters in a GMM is also unknown, and it is determined through a process known as model selection, which involves comparing models with different numbers of clusters and parameterisations and selecting the one which best fits the data.\nIn summary, model-based clustering from the perspective of latent variable modelling assumes that the data is generated from a probabilistic model with a specific number of clusters. A likelihood-based approach can be used to estimate the parameters of the model and the latent variable that represents the cluster assignment for each observation in the data, and guide the selection of the number of clusters. A GMM is a common framework for model-based clustering which assumes the data in each cluster is generated from a Gaussian distribution."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#gaussian-parsimonious-clustering-models",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#gaussian-parsimonious-clustering-models",
    "title": "9  An introduction and tutorial to model-based clustering in education via latent profile analysis",
    "section": "2 Gaussian parsimonious clustering models",
    "text": "2 Gaussian parsimonious clustering models\nFor a continuous feature vector \\(\\boldsymbol{x}\\in \\mathbb{R}^{d}\\), the general form of the density function of a Gaussian mixture model (GMM) with \\(K\\) components can be written as \\[\nf(\\boldsymbol{x}) = \\sum_{k=1}^K \\pi_k \\phi_d(\\boldsymbol{x};\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k),\n\\tag{1}\\] where \\(\\pi_k\\) represents the mixing probabilities, i.e., the marginal probability of belonging to the \\(k\\)-th cluster, such that \\(\\pi_k > 0\\) and \\(\\sum_{k=1}^K\\pi_k=1\\), and \\(\\phi_d(\\cdot)\\) is the \\(d\\)-dimensional multivariate Gaussian density with parameters \\((\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\) for \\(k=1,\\ldots,K\\). Clusters described by such a GMM are ellipsoidal, centered at the means \\(\\boldsymbol{\\mu}_k\\), and with other geometric characteristics (namely volume, shape, and orientation) determined by the covariance matrices \\(\\boldsymbol{\\Sigma}_1, \\ldots, \\boldsymbol{\\Sigma}_K\\). Parsimonious parameterisations of covariance matrices can be controlled by imposing some constraints on the covariance matrices through the following eigen-decomposition [28, 29]: \\[\n\\boldsymbol{\\Sigma}_k = \\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k,\n\\tag{2}\\] where \\(\\lambda_k = \\lvert\\boldsymbol{\\Sigma}_k\\rvert^{1/d}\\) is a scalar which controls the volume, \\(\\boldsymbol{\\Delta}_k\\) is a diagonal matrix whose entries are the normalised eigenvalues of \\(\\boldsymbol{\\Sigma}_k\\) in decreasing order, such that \\(\\lvert\\boldsymbol{\\Delta}_k\\rvert = 1\\), which controls the shape of the density contours, and \\(\\boldsymbol{U}_k\\) is an orthogonal matrix whose columns are the eigenvectors of \\(\\boldsymbol{\\Sigma}_k\\), which controls the orientation of the corresponding ellipsoid. The size of a cluster is distinct from its volume and is proportional to \\(\\pi_k\\) [29].\nGMMs with unconstrained covariance matrices are quite flexible, but require the estimation of several parameters. To obtain a balance between model complexity and accuracy of parameter estimates, a parsimonious model parameterisation can be adopted. Constraining the geometric characteristics of cluster covariances to be equal across clusters can greatly reduce the number of estimable parameters, and is the means by which GMMs obtain intermediate covariance matrices between homoscedasticity and heteroscedasticity. A list of the 14 resulting parameterisations available in the mclust package [7] for R [8] is included in Table 2.1 of [30]. Of particular note is the nomenclature adopted by mclust whereby each model has a three-letter name with each letter pertaining to the volume, shape, and orientation, respectively, denoting whether the given component is equal (E) or free to vary (V) across clusters. Some model names also use the letter I in the third position to indicate that the covariance matrices are diagonal and two particularly parsimonious models have the letter I in the second position to indicate that the covariance matrices are isotropic. Thus, as examples, the fully unconstrained VVV model is one for which the volume, shape, and orientation are all free to vary across clusters, the EVE model constrains the clusters to have equal volume and orientation but varying shape, and the VII model assumes isotropic covariance matrices with cluster-specific volumes. The flexibility to model clusters with different geometric characteristics by modelling correlations according to various parameterisations represents another advantage over heuristic clustering algorithms. Taking the \\(k\\)-means algorithm as an example, a larger number of circular, Euclidean distance-based clusters may be required to fit the data well, rather than a more parsimonious and easily interpretable mixture model with fewer non-spherical components.\nGiven a random sample of observations \\(\\{ \\boldsymbol{x}_1, \\boldsymbol{x}_2, \\ldots, \\boldsymbol{x}_n \\}\\) in \\(d\\) dimensions, the log-likelihood of a GMM with \\(K\\) components is given by \\[\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log\\left\\{ \\sum_{k=1}^K \\pi_k \\phi_d(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right\\},\n\\tag{3}\\] where \\(\\boldsymbol{\\theta}= (\\pi_1, \\ldots, \\pi_{K-1}, \\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K, \\boldsymbol{\\Sigma}_1, \\ldots, \\boldsymbol{\\Sigma}_K)\\) denotes the collection of parameters to be estimated.\nMaximizing the log-likelihood function in Equation 9.3 directly is often complicated, so maximum likelihood estimation (MLE) of \\(\\boldsymbol{\\theta}\\) is usually performed using the EM algorithm [20] by including the component membership as a latent variable. The EM algorithm consists of two steps: the E-step (Expectation step) and the M-step (Maximisation step). In the E-step, the algorithm calculates the expected membership probabilities of each data point to each of the mixture components based on the current estimates of the model parameters. Thus, though the latent variable indicating cluster membership is assumed to be categorical and represented by indicator variables taking the values \\(0\\) or \\(1\\), the model estimates assignment probabilities in the range \\([0,1]\\) at this step. In the M-step, the algorithm updates the model parameters by maximizing the likelihood of the observed data given the estimated membership probabilities. These two steps are repeated until convergence or a maximum number of iterations is reached. Details on the use of EM algorithm in finite mixture modelling is provided by [19], while a thorough treatment and further extensions can be found in [31]. For the GMM case, see Sec. 2.2 of [30].\nFollowing the fitting of a GMM and the determination of the MLEs of parameters, the maximum a posteriori (MAP) procedure can be used to assign the observations into the most likely cluster and recover a “hard” partition. For an observation \\(\\boldsymbol{x}_i\\) the posterior conditional probability of coming from the mixture component \\(k\\) is given by \\[\n\\widehat{p}_{ik} = \\frac{\\widehat{\\pi}_k \\phi_d(\\boldsymbol{x}_i; \\widehat{\\boldsymbol{\\mu}}_k, \\widehat{\\boldsymbol{\\Sigma}}_k)}{\\displaystyle\\sum_{g=1}^K \\widehat{\\pi}_g \\phi_d(\\boldsymbol{x}; \\widehat{\\boldsymbol{\\mu}}_g, \\widehat{\\boldsymbol{\\Sigma}}_g)}.\n\\tag{4}\\] Then, an observation is assigned to the cluster with the largest posterior conditional probability, i.e., \\(\\boldsymbol{x}_i \\in \\mathcal{C}_{k^\\star}\\) with \\(k^\\star = \\mathop{\\mathrm{arg\\,max}}_k \\widehat{p}_{ik}\\).\n\n2.1 Model selection\nGiven that a wide variety of GMMs in Equation 9.1 can be estimated by varying the number of mixture components and the covariance decompositions in Equation 9.2, selecting the appropriate model represents a crucial decision. A popular option consists in choosing the “best” model using the Bayesian information criterion [BIC, 32], which, for a given model \\(\\mathcal{M}\\), is defined as \\[\n\\text{BIC}_{\\mathcal{M}} = 2\\ell_{\\mathcal{M}}(\\widehat{\\boldsymbol{\\theta}}) - \\nu_{\\mathcal{M}} \\log(n),\n\\] where \\(\\ell_{\\mathcal{M}}(\\widehat{\\boldsymbol{\\theta}})\\) stands for the maximised log-likelihood of the data sample of size \\(n\\) under model \\(\\mathcal{M}\\), and \\(\\nu_{\\mathcal{M}}\\) for the number of independent parameters to be estimated. Another option available in clustering is the Integrated Complete Likelihood [ICL, 33] criterion given by \\[\n\\text{ICL}_{\\mathcal{M}} = \\text{BIC}_{\\mathcal{M}} + 2 \\sum_{i=1}^n\\sum_{k=1}^K c_{ik} \\log(\\widehat{p}_{ik}),\n\\] where \\(\\widehat{p}_{ik}\\) is the conditional probability that \\(\\boldsymbol{x}_i\\) arises from the \\(k\\)-th mixture component from Equation 9.4, and \\(c_{ik} = 1\\) if the \\(i\\)-th observation is assigned to cluster \\(\\mathcal{C}_k\\) and 0 otherwise.\nBoth criteria evaluate the fit of a GMM to a given set of data by considering both the likelihood of the data given the model and the complexity of the model itself, represented by the number of parameters to be estimated. Compared to the BIC, the ICL introduces a further entropy-based penalisation for the overlap of the clusters. For this reason, the ICL tends to select models with well-separated clusters.\nWhereas there is no consensus of a standard criteria for choosing the best model, there are guidelines that the researcher could rely on. To decide on the optimal model, examining the fit indices (such as the BIC and ICL), model interpretability, and conformance to theory can be of great help. The literature recommends estimating a 1-cluster solution for each model that serves as a comparative baseline and then increasing the number of clusters one by one, evaluating if adding another cluster yields a better solution in both statistical and conceptual terms [34]. Among all fit indices, lower BIC values seems to be the preferred method for selecting the best model. However, examining other indices (e.g., AIC, ICL) is also useful. Oftentimes, fit indices do not converge to a certain model. In such cases, the interrelation between the selected models, such as whether one model is an expanded version of another, should also be taken into consideration, as well as the stability of the different models, including the relative sizes of the emergent profiles (each profile should comprise more than 5-8% of the sample) [34]. Furthermore, the elbow method could be helpful in cases where no clear number of clusters can be easily determined from the fit indices (e.g., the BIC continues to decrease consistently when increasing the number of clusters). This entails plotting the BIC values and finding an elbow shape where a drop in BIC is less noticeable with increasing numbers of clusters or roughly an elbow followed by a relatively flat line. The choice of the best number of clusters can and probably should be guided by theory; that is, in cases where previous research reported a certain number of clusters or profiles, it is recommended to take this guidance into account. For instance, research on engagement has repeatedly reported three levels of engagement. Once we have chosen the most suitable model, it is suggested to compute model diagnostics (e.g., entropy and average posterior probability) to evaluate the selected model. These diagnostics are covered in Section 9.2.3.3.\n\n\n2.2 mclust R package\nmclust is an R package [8] for model-based cluster analysis, classification, and density estimation using Gaussian finite mixture models [30, 35]. It is widely used in statistics, machine learning, data science, and pattern recognition. One of the key features of mclust is its flexibility in modelling quantitative data with several covariance structures and different numbers of mixture components. Additionally, the package provides extensive graphical representations, model selection criteria, initialisation strategies for the EM algorithm, bootstrap-based inference, and Bayesian regularisation, among other prominent features. mclust also represents a valuable tool in educational settings because it provides a powerful set of models that allows students and researchers to quickly and easily perform clustering and classification analyses on their data. We focus here on the use of mclust as a tool for unsupervised model-based clustering, though the package does also provide functions for supervised model-based classification.\nThe main function implementing model-based clustering is called Mclust(), which requires a user to provide at least the data set to analyze. In the one-dimensional case, the data set can be a vector, while in the multivariate case, it can be a matrix or a data frame. In the latter case, the rows correspond to observations, and the columns correspond to variables.\nThe Mclust() function allows for further arguments, including the optional argument G to specify the number of mixture components or clusters, and modelNames to specify the covariance decomposition. If both G and modelNames are not provided, Mclust() will fit all possible models obtained using a number of mixture components from 1 to 9 and all 14 available covariance decompositions, and it will select the model with the largest BIC. Notably, if the data set is univariate, only 2 rather than 14 models governing the scalar variance parameters are returned; that they are equal or unequal across components. Finally, computing the BIC and ICL criteria can be done by invoking the functions mclustBIC() and mclustICL(), respectively.\n\n\n2.3 Other practical issues and extensions\nPrior to commencing the cluster analysis of a data set on school engagement, academic achievement, and self-regulated learning measures, we first provide some theoretical background on some extensions of practical interest which will be explored in the analysis.\n\n2.3.1 Bayesian regularisation\nIncluding a prior distribution over the mixture parameters is an effective way to avoid singularities and degeneracies in maximum likelihood estimation. Furthermore, this can help to prevent overfitting and improve model performance. In situations where the variables of interest are discrete or take on only a few integer values, including a prior distribution can help to regularise the model.\n[36] proposed using weekly informative conjugate priors to regularise the estimation process. The EM algorithm can still be used for model fitting, but maximum likelihood estimates (MLEs) are replaced by maximum a posteriori (MAP) estimates. A slightly modified version of BIC can be used for model selection, with the maximised log-likelihood replaced by the log-likelihood evaluated at the MAP or posterior mode.\nThe prior distributions proposed by [36] are:\n\na uniform prior on the simplex for the mixture weights \\((\\pi_1, \\ldots, \\pi_K)\\);\na Gaussian prior on the mean vector (conditional on the covariance matrix), i.e., \\[\\begin{align}\n\\boldsymbol{\\mu}\\mid \\boldsymbol{\\Sigma}\n& \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Sigma}/\\kappa_{\\scriptscriptstyle \\mathcal{P}}) \\\\\n& \\propto\n\\left|\\boldsymbol{\\Sigma}\\right|^{-1/2}\n\\exp\\left\\{ -\\frac{\\kappa_{\\scriptscriptstyle \\mathcal{P}}}{2}\n            \\mathop{\\mathrm{tr}}\\left(\\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right){}^{\\!\\top}\n                     \\boldsymbol{\\Sigma}^{-1}\n                     \\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right)\n                \\right)\n    \\right\\},\n\\label{eqn:multivariatePriorMean}\n\\end{align}\\] with \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) and \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}\\) being the hyperparameters controlling, respectively, the mean vector and the amount of shrinkage applied;\nan inverse Wishart prior on the covariance matrix, i.e., \\[\\begin{align}\n\\boldsymbol{\\Sigma}\n& \\sim \\mathcal{IW}(\\nu_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}})\n\\nonumber\\\\\n& \\propto\n\\left|\\boldsymbol{\\Sigma}\\right|^{-(\\nu_{\\scriptscriptstyle \\mathcal{P}}+d+1)/2}\n\\exp\\left\\{ -\\frac{1}{2}\n            \\mathop{\\mathrm{tr}}\\left(\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}^{-1}\n               \\right)\n    \\right\\},\n\\label{eqn:multivariatePriorVar}\n\\end{align}\\] with the hyperparameters \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}\\) and the matrix \\(\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}\\) controlling the degrees of freedom and scale of the prior distribution, respectively.\n\nAdding a prior to GMMs estimated using the mclust R package is easily obtained by adding an optional prior argument when calling some of the fitting functions, such as mclustBIC() and Mclust(). Specifically, setting prior = priorControl(functionName = \"defaultPrior\") allows to adopt the conjugate priors described above with the following default values for the hyperparameters:\n\nmean vector \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}= \\bar{\\boldsymbol{x}}\\), the sample mean of each variable;\nshrinkage \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}= 0.1\\);\ndegrees of freedom \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}= d+2\\);\nscale matrix \\(\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}= \\S/(K^{2/d})\\), where \\(\\S\\) is the sample covariance matrix.\n\nRationale for the above default values for the prior hyperparameters, together with the corresponding MAP estimates of the GMM parameters, can be found in [36, Table 2]. These values should suffice for most applications, but experienced users who want to tune the hyperparameters can refer to the documentation available in the help pages for priorControl and defaultPrior(). Further details about specifying different hyperparameter values can be found in [30].\n\n\n2.3.2 Bootstrap inference\nLikelihood-based inference in mixture models is complicated because asymptotic theory applied to mixture models require a very large sample size [19], and standard errors derived from the expected or the observed information matrix tend to be unstable [37]. For these reasons, resampling approaches based on the bootstrap are often employed [38].\nThe bootstrap [39] is a general, widely applicable, powerful technique for obtaining an approximation to the sampling distribution of a statistic of interest. The bootstrap distribution is approximated by drawing a large number of samples, called bootstrap samples, from the empirical distribution. This can be obtained by resampling with replacement from the observed data (nonparametric bootstrap), or from a parametric distribution with unknown parameters substituted by the corresponding estimates (parametric bootstrap). A Bayesian version of the bootstrap, introduced by [40], allows posterior samples to be obtained by resampling with weights for each observation drawn from a uniform Dirichlet distribution. A strictly related technique is the weighted likelihood bootstrap [41], where a statistical model is repeatedly fitted using weighted maximum likelihood with weights obtained as in Bayesian bootstrap.\nLet \\(\\widehat{\\boldsymbol{\\theta}}\\) be the estimate of a set of GMM parameters \\(\\boldsymbol{\\theta}\\) for a given model \\(\\mathcal{M}\\), determined by the adopted covariance parameterisation and number of mixture components. The bootstrap distribution for the parameters of interest is obtained as follows:\n\ndraw a bootstrap sample of size \\(n\\) using one of the resampling techniques described above to form the bootstrap sample \\((\\boldsymbol{x}^\\star_1, \\ldots, \\boldsymbol{x}^\\star_n)\\);\nfit a the GMM \\(\\mathcal{M}\\) to get the bootstrap estimates \\(\\widehat{\\boldsymbol{\\theta}}^\\star\\);\nreplicate the previous steps a large number of times, say \\(B\\).\n\nThe bootstrap distribution for the parameters of interest, \\(\\widehat{\\boldsymbol{\\theta}}^\\star_1, \\widehat{\\boldsymbol{\\theta}}^\\star_2, \\ldots, \\widehat{\\boldsymbol{\\theta}}^\\star_B\\), can then be used for computing the bootstrap standard errors (as the square root of the diagonal elements of the bootstrap covariance matrix) or the bootstrap percentile confidence intervals. More details can be found in [30].\nFrom a practical point of view, bootstrap resampling can be conducted in mclust by means of the function MclustBootstrap(). This function takes as arguments the fitted model object returned from e.g., Mclust() or mclustBIC(), the optional argument type, which allows to specify the type of bootstrap samples to draw (\"bs\" for nonparametric bootstrap, \"pb\" for parametric bootstrap, and \"wlbs\" for weighted likelihood bootstrap), and the optional argument nboot, which sets the number of bootstrap samples. At least \\(999\\) samples should be drawn if confidence intervals are needed.\n\n\n2.3.3 Entropy and average posterior probabilities\nThe definition of entropy in information theory [42] refers to the average amount of information provided by a random variable. Following this definition, [43] defines the entropy of a finite mixture model as follows \\[\nE_{\\text{FMM}} = - \\sum_{i=1}^n \\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik}),\n\\] where \\(\\widehat{p}_{ik}\\) is the estimated posterior probability of case \\(i\\) to belong to cluster \\(k\\) (see Equation 9.4). If the mixture components are well separated, \\(\\widehat{p}_{ik} \\approx 1\\) for the assigned cluster \\(\\mathcal{C}_k\\) and \\(0\\) otherwise. Consequently, the entropy of the mixture model in this case is \\(E_{\\text{FMM}} = 0\\) (note that \\(0\\log(0)=0\\) by convention). On the contrary, in the case of maximal assignment uncertainty, \\(\\widehat{p}_{ik} = 1/K\\) for all clusters \\(\\mathcal{C}_k\\) (\\(k=1,\\ldots,K\\)). As a result, the entropy of the mixture model is \\(E_{\\text{FMM}} = n\\log(K)\\).\nIn latent class and latent profile analysis, a slightly different definition of entropy is used as a diagnostic statistic to assess how well the fitted model assigns individuals to the identified clusters based on their response patterns. Thus, a normalised version of the entropy is defined as follows \\[\nE = 1 - \\frac{E_{\\text{FMM}}}{n \\log(K)} = 1 + \\dfrac{\\sum_{i=1}^n \\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik})}{n \\log(K)}.\n\\] Entropy takes values on the range \\([0,1]\\), with higher entropy values indicating that the model has less uncertainty in assigning cases to their respective latent classes/profiles. Thus, high entropy values typically indicate a better model which is able to distinguish between the latent components and that the components are relatively distinct. An entropy value close to 1 is ideal, while values above \\(0.6\\) are considered acceptable, although there is no agreed upon optimal cutoff for entropy.\nThe contribution of each observation to the overall total entropy can be defined as \\[\nE_i = 1 + \\frac{\\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik})}{\\log(K)},\n\\] so that the overall total entropy is obtained by averaging over the individual contributions, i.e., \\(E = \\sum_{i=1}^n E_i/n\\). The individual contributions \\(E_i\\) can also be used to compute the average entropy of each latent component, which indicates how accurately the model defines components. Average posterior probabilities (AvePP) are a closely related performance assessment measure, given by the average posterior membership probabilities \\(\\widehat{p}_{ik}\\) for each component for the observations most probably assigned to that component, for which a cutoff of \\(0.8\\) has been suggested to indicate acceptably high assignment certainty and well-separated clusters [34]. The analysis below presents the necessary code to calculate entropies and average posterior probabilities thusly from a fitted mclust model."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#application-school-engagement-academic-achievement-and-self-regulated-learning",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#application-school-engagement-academic-achievement-and-self-regulated-learning",
    "title": "9  An introduction and tutorial to model-based clustering in education via latent profile analysis",
    "section": "3 Application: School engagement, academic achievement, and self-regulated learning",
    "text": "3 Application: School engagement, academic achievement, and self-regulated learning\nA group of 717 primary school students from northern Spain were evaluated in terms of their school engagement, self-regulation, and academic performance through the use of various measures. The school engagement measure (SEM) was employed to assess their engagement, while their self-regulation was evaluated with the self-regulation strategy inventory—self-report. The measure for academic achievement was based on the students’ self-reported grades in Spanish and mathematics, which were rated on a scale of 1 to 5. This data set can be used to identify clusters of students based on their engagement and self-regulation. These clusters would represent distinct patterns or “profiles” of engagement. Finding such profiles would allow us to understand individual differences but more importantly to stratify support according to different engagement profiles.\n\n3.1 Preparing the data\nWe start by loading the packages required for the analysis. We note in particular that version 6.0.0 of mclust is employed here, the latest release at the time of writing.\n\nlibrary(ggplot2)\nlibrary(ggridges)\nlibrary(mclust)\nlibrary(rio)\nlibrary(tidyverse)\n\nThen, we read the data set from an online comma-separated-value (CSV) file, followed by some data cleaning and formatting to prepare the data for subsequent analysis. Note that the CSV file to be read is not in standard format, so we have to explicitly set the separator field using the optional argument sep = \";\".\n\n\n\n\n\n# read the data\ndata <- import(\"https://github.com/lamethods/data/raw/main/3_engSRLach/\n               Manuscript_School%20Engagment.csv\", \n               sep = \";\")\n\n\n# select the variables to be analyzed\nvars <- c(\"PRE_ENG_COND\", \"PRE_ENG_COGN\", \"PRE_ENG_EMOC\")\nx <- select(data, all_of(vars)) |> \n  as_tibble() |>\n  rename(\"BehvEngmnt\" = \"PRE_ENG_COND\",  # Behavioral engagement\n         \"CognEngmnt\" = \"PRE_ENG_COGN\",  # Cognitive engagement\n         \"EmotEngmnt\" = \"PRE_ENG_EMOC\")  # Emotional engagement\n\n# print the data set used in the subsequent analysis\nx\n## # A tibble: 717 × 3\n##    BehvEngmnt CognEngmnt EmotEngmnt\n##         <dbl>      <dbl>      <dbl>\n##  1       3.75       3.14        4.4\n##  2       4          3.71        2  \n##  3       4.25       3.86        4  \n##  4       3.75       2.57        3  \n##  5       4.25       3           4  \n##  6       4          3.71        3.8\n##  7       3.5        2.14        3.2\n##  8       4.75       3.57        1.6\n##  9       3.25       2.71        3  \n## 10       5          4.43        4.8\n## # ℹ 707 more rows\n\nA table of summary statistics for the data set can be obtained as follows:\n\nx |> pivot_longer(cols = colnames(x),\n                  names_to = \"Variable\",\n                  values_to = \"Value\") |>\n  group_by(Variable) |>\n  summarize(N = n(),\n            Nunq = n_distinct(Value),\n            Mean = mean(Value),\n            SD = sd(Value),\n            Min = min(Value),\n            Median = median(Value),\n            Max = max(Value))\n## # A tibble: 3 × 8\n##   Variable       N  Nunq  Mean    SD   Min Median   Max\n##   <chr>      <int> <int> <dbl> <dbl> <dbl>  <dbl> <dbl>\n## 1 BehvEngmnt   717    17  4.17 0.627     1   4.25     5\n## 2 CognEngmnt   717    30  2.92 0.771     1   2.92     5\n## 3 EmotEngmnt   717    22  3.61 0.911     1   3.61     5\n\n\n\n3.2 Model estimation and model selection\nTo begin our latent profile analysis, we first fit a number of candidate GMMs with different numbers of latent components and covariance parameritations, and compute the Bayesian Information Criterion (BIC) to select the “optimal” model. This model selection criterion jointly takes into account both the covariance decompositions and the number of mixture components in the model.\nAs mentioned earlier, given the characteristics of the data, which consists of a small number of unique values relative to the number of observations, a prior is used for regularisation. We invoke the default priors described above, summarise the BIC values of the three best models, and visualise the BIC values of all fitted models.\n\nBIC <- mclustBIC(x, prior = priorControl())\nsummary(BIC)\n## Best BIC values:\n##              VVI,3        VVI,4       VVV,3\n## BIC      -4521.213 -4526.905884 -4533.57166\n## BIC diff     0.000    -5.693183   -12.35896\nplot(BIC)\n\n\n\n\nBIC traces for the estimated GMMs with default priors.\n\n\n\n\nThe selected model is a three-component GMM with diagonal covariance matrices of varying volume and shape, with axis-aligned orientation, indicated as (VVI,3). Thus, the variables are independent within each cluster.\n\n\n3.3 Examining model output\nThe fit of the optimal model is obtained using:\n\nmod <- Mclust(x, modelNames = \"VVI\", G = 3, prior = priorControl())\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVI (diagonal, varying volume and shape) model with 3 components: \n## \n## Prior: defaultPrior() \n## \n##  log-likelihood   n df       BIC       ICL\n##       -2194.856 717 20 -4521.213 -4769.174\n## \n## Clustering table:\n##   1   2   3 \n## 184 119 414 \n## \n## Mixing probabilities:\n##         1         2         3 \n## 0.2895147 0.1620776 0.5484078 \n## \n## Means:\n##                [,1]     [,2]     [,3]\n## BehvEngmnt 3.704041 4.713234 4.257355\n## CognEngmnt 2.287057 3.699530 3.017293\n## EmotEngmnt 2.738969 4.733899 3.737286\n## \n## Variances:\n## [,,1]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.5022148  0.0000000  0.0000000\n## CognEngmnt  0.0000000  0.3909235  0.0000000\n## EmotEngmnt  0.0000000  0.0000000  0.7674268\n## [,,2]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.0737948  0.0000000 0.00000000\n## CognEngmnt  0.0000000  0.4150514 0.00000000\n## EmotEngmnt  0.0000000  0.0000000 0.05540526\n## [,,3]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.2048374  0.0000000  0.0000000\n## CognEngmnt  0.0000000  0.3327557  0.0000000\n## EmotEngmnt  0.0000000  0.0000000  0.2795838\n\nThe shown output reports some basic information about the fit, such as the maximised log-likelihood (log-likelihood), the number of observations (n), the number of estimated parameters (df), the BIC criterion (BIC), and the clustering table based on the MAP classification. The latter indicates that the clusters also vary in terms of size. The optional argument parameters = TRUE in the summary() function call additionally prints the estimated parameters. Observe that the VVI model allows variance to vary across components while fixing all covariance parameters to zero.\nA plot showing the classification provided by the estimated model can be drawn as follows:\n\nplot(mod, what = \"classification\") \n\n\n\n\nScatterplot matrix of engagement features with data points marked and coloured by the identified clusters, and ellipses corresponding to projections of the estimated cluster covariances.\n\n\n\n\nThe estimated model identifies three clusters of varying size. The third group (shown as filled green triangles) accounts for more than 50% of the observations, while the first (shown as blue filled points) and the second (shown as red open squares) account for approximately 29% and 16%, respectively. The smallest cluster is also the group with the largest engagement scores.\nThe different engagement behaviour of the three identified clusters can be shown using a latent profiles plot of the estimated means with point sizes proportional to the estimated mixing probabilities (see Figure 9.1):\n\n# collect estimated means\nmeans <- data.frame(Profile = factor(1:mod$G),\n                    t(mod$parameters$mean)) |>\n  pivot_longer(cols = -1,\n               names_to = \"Variable\",\n               values_to = \"Mean\")\n\n# convert variable names to factor\nmeans$Variable <- factor(means$Variable, \n                         levels = colnames(mod$data))\n\n# add mixing probabilities corresponding to profiles\nmeans <- means |> \n  add_column(MixPro = mod$parameters$pro[means$Profile])\nmeans\n## # A tibble: 9 × 4\n##   Profile Variable    Mean MixPro\n##   <fct>   <fct>      <dbl>  <dbl>\n## 1 1       BehvEngmnt  3.70  0.290\n## 2 1       CognEngmnt  2.29  0.290\n## 3 1       EmotEngmnt  2.74  0.290\n## 4 2       BehvEngmnt  4.71  0.162\n## 5 2       CognEngmnt  3.70  0.162\n## 6 2       EmotEngmnt  4.73  0.162\n## 7 3       BehvEngmnt  4.26  0.548\n## 8 3       CognEngmnt  3.02  0.548\n## 9 3       EmotEngmnt  3.74  0.548\n\n# plot the means of the latent profiles\nggplot(means, aes(x = Variable, y = Mean,\n                  group = Profile, \n                  shape = Profile, \n                  color = Profile)) +\n  geom_point(aes(size = MixPro)) +\n  geom_line(linewidth = 0.5) +\n  labs(x = NULL, y = \"Latent profiles means\") +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_size(range = c(1, 3), guide = \"none\") +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 1. Latent profiles plot showing estimated means with point sizes proportional to estimated mixing probabilities.\n\n\n\n\nThe smallest cluster (Profile 2) has the highest engagement scores for all three variables. All three scores are lower for the largest cluster (Profile 3), which are all in turn lower for Profile 1. All three profiles exhibit the lowest mean scores for the cognitive engagement attribute. For Profile 2, behavioural engagement and emotional engagement scores are comparable, whereas for the other two profiles, the mean scores for this attribute are lower than those for the behaviour engagement attribute. Taken together, we could characterise profiles 1, 3, and 2 as “low”, “medium”, and “high” engagement profiles, respectively.\nTo provide a more comprehensive understanding of the results presented in the previous graph, it would be beneficial to incorporate a measure of uncertainty for the estimated means of the latent profiles. This can be achieved by resampling using the function MclustBootstrap() as described above:\n\nboot <- MclustBootstrap(mod, type = \"bs\", nboot = 999)\n\nThe bootstrap distribution of the mixing weights can be visualised using histograms with the code\n\npar(mfcol = c(1, 3), mar = c(2, 4, 1, 1), mgp = c(2, 0.5, 0))\nplot(boot, what = \"pro\", xlim = c(0, 1))\n\n\n\n\nFigure 2. Bootstrap distribution of GMM mixture weights.\n\n\n\n\nwhile the bootstrap distribution of the components means can be plotted with the code\n\npar(mfcol = c(3, 3), mar = c(2, 4, 1, 1), mgp = c(2, 0.5, 0))\nplot(boot, what = \"mean\", conf.level = 0.95)\n\n\n\n\nFigure 3. Bootstrap distribution of GMM component means.\n\n\n\n\nThe resulting graphs are reported in Figure 9.2 and Figure 9.3, where the GMM estimates are shown as dashed vertical lines, while the horizontal segments represent the percentile confidence intervals at the 95% confidence level.\nNumerical output of the resampling-based bootstrap distributions is given by:\n\nsboot <- summary(boot, what = \"ci\")\nsboot\n## ---------------------------------------------------------- \n## Resampling confidence intervals \n## ---------------------------------------------------------- \n## Model                      = VVI \n## Num. of mixture components = 3 \n## Replications               = 999 \n## Type                       = nonparametric bootstrap \n## Confidence level           = 0.95 \n## \n## Mixing probabilities:\n##               1          2         3\n## 2.5%  0.1275702 0.09255033 0.4761326\n## 97.5% 0.3872065 0.23592891 0.6845753\n## \n## Means:\n## [,,1]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    3.479985   1.872176   2.082263\n## 97.5%   3.822700   2.441357   2.942343\n## [,,2]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    4.628745   3.532034   4.527568\n## 97.5%   4.898591   3.929510   4.878574\n## [,,3]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    4.097707   2.836241   3.533040\n## 97.5%   4.366235   3.145216   3.887315\n## \n## Variances:\n## [,,1]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.4090113  0.2286101  0.5467998\n## 97.5%  0.7845491  0.4913383  0.9885883\n## [,,2]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.0159611  0.3256681 0.01817132\n## 97.5%  0.1042114  0.5728663 0.14713752\n## [,,3]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.1587247  0.2842579  0.2297413\n## 97.5%  0.2788433  0.3974127  0.4077299\n\nThe information above can then be used to plot the latent profile means accompanied by 95% confidence intervals represented as vertical bars, as illustrated in Figure 9.4. The confidence intervals for the cognitive and emotional engagement attributes are noticeably wider for the “low” engagement profile.\n\nmeans <- means |> \n  add_column(lower = as.vector(sboot$mean[1,,]),\n             upper = as.vector(sboot$mean[2,,]))\nmeans\n## # A tibble: 9 × 6\n##   Profile Variable    Mean MixPro lower upper\n##   <fct>   <fct>      <dbl>  <dbl> <dbl> <dbl>\n## 1 1       BehvEngmnt  3.70  0.290  3.48  3.82\n## 2 1       CognEngmnt  2.29  0.290  1.87  2.44\n## 3 1       EmotEngmnt  2.74  0.290  2.08  2.94\n## 4 2       BehvEngmnt  4.71  0.162  4.63  4.90\n## 5 2       CognEngmnt  3.70  0.162  3.53  3.93\n## 6 2       EmotEngmnt  4.73  0.162  4.53  4.88\n## 7 3       BehvEngmnt  4.26  0.548  4.10  4.37\n## 8 3       CognEngmnt  3.02  0.548  2.84  3.15\n## 9 3       EmotEngmnt  3.74  0.548  3.53  3.89\n\nggplot(means, aes(x = Variable, y = Mean, group = Profile, \n                  shape = Profile, color = Profile)) +\n  geom_point(aes(size = MixPro)) +\n  geom_line(linewidth = 0.5) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                linewidth = 0.5, width = 0.1) +\n  labs(x = NULL, y = \"Latent profiles means\") +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_size(range = c(1, 3), guide = \"none\") +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 4. Latent profiles plot showing estimated means with 95% bootstrap confidence intervals.\n\n\n\n\nFinally, the entropy of the estimated partition, average entropy of each latent component, and average posterior probabilities are obtained via:\n\nprobs <- mod$z                    # posterior conditional probs\nprobs_map <- apply(probs, 1, max) # maximum a posteriori probs\nclusters <- mod$classification    # cluster assignment for each obs\nn <- mod$n                        # number of obs\nK <- mod$G                        # number of latent profiles\n\n# Entropy:\nE <- 1 + sum(probs * log(probs))/(n * log(K))\nE\n## [1] 0.6890602\n\n# Case-specific entropy contributions:\nEi <- 1 + rowSums(probs * log(probs))/log(K)\nsum(Ei)/n\n## [1] 0.6890602\n\ndf_entropy  <- data.frame(clusters = as.factor(clusters), entropy = Ei)\n\n\ndf_entropy |>\n  group_by(clusters) |>\n  summarise(count = n(),\n            mean = mean(entropy),\n            sd = sd(entropy),\n            min = min(entropy),\n            max = max(entropy))\n## # A tibble: 3 × 6\n##   clusters count  mean    sd   min   max\n##   <fct>    <int> <dbl> <dbl> <dbl> <dbl>\n## 1 1          184 0.740 0.239 0.369 1.00 \n## 2 2          119 0.690 0.225 0.187 0.993\n## 3 3          414 0.666 0.197 0.172 0.974\n\nggplot(df_entropy, aes(y = clusters, x = entropy,  fill = clusters)) +\n  geom_density_ridges(stat = \"binline\", bins = 21,\n                      scale = 0.9, alpha = 0.5) +\n  scale_x_continuous(breaks = seq(0, 1 ,by=0.1), \n                     limits = c(0, 1.05)) +\n  scale_fill_manual(values = mclust.options(\"classPlotColors\")) +\n  geom_vline(xintercept = E, lty = 2) +\n  labs(x = \"Case-specific entropy contribution\", \n       y = \"Latent profile\") +\n  theme_ridges(center_axis_labels = TRUE) +\n  theme(legend.position = \"none\",\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(size = 8))\n\n\n\n\nFigure 5. Entropy contributions by cluster and total entropy (dashed line).\n\n\n\n\n\n# Average posterior probabilities by cluster:\ndf_AvePP <- data.frame(clusters = as.factor(clusters), pp = probs_map)\n\ndf_AvePP |>\n  group_by(clusters) |>\n  summarise(count = n(),\n            mean = mean(pp),\n            sd = sd(pp),\n            min = min(pp),\n            max = max(pp))\n## # A tibble: 3 × 6\n##   clusters count  mean    sd   min   max\n##   <fct>    <int> <dbl> <dbl> <dbl> <dbl>\n## 1 1          184 0.864 0.160 0.513 1.00 \n## 2 2          119 0.858 0.146 0.468 0.999\n## 3 3          414 0.850 0.135 0.502 0.996\n\nggplot(df_AvePP, aes(y = clusters, x = pp,  fill = clusters)) +\n  geom_density_ridges(stat = \"binline\", bins = 21,\n                      scale = 0.9, alpha = 0.5) +\n  scale_x_continuous(breaks = seq(0, 1, by=0.1), \n                     limits = c(0, 1.05)) +\n  scale_fill_manual(values = mclust.options(\"classPlotColors\")) +\n  labs(x = \"MAP probabilities\", y = \"Latent profile\") +\n  theme_ridges(center_axis_labels = TRUE) +\n  theme(legend.position = \"none\",\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(size = 8)) \n\n\n\n\nFigure 6. Average posterior probabilities by cluster.\n\n\n\n\nWe note that, as shown in Figure 9.5 and Figure 9.6, all entropy and AvePP quantities appear satisfactory from the point of view of indicating reasonably well-separated clusters."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#discussion",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#discussion",
    "title": "9  An introduction and tutorial to model-based clustering in education via latent profile analysis",
    "section": "4 Discussion",
    "text": "4 Discussion\nUsing a person-centered method (finite Gaussian mixture model), the present analysis uncovered the heterogeneity within the SEM engagement data by identifying three latent or unobserved clusters: low, medium, and high engagement clusters. Uncovering the latent structure could help understand individual differences among students, identify the complex multidimensional variability of a construct —engagement in our case— and possibly help personalise teaching and learning. Several studies have revealed similar patterns of engagement which —similar to the current analysis— comprise three levels that can be roughly summarised as high, moderate, and low [9, 44, 45]. The heterogeneity of engagement has been demonstrated in longitudinal studies, in both face-to-face settings as well as online engagement [9]. Furthermore, the association between engagement and performance has been demonstrated to vary by achievement level, time of the year, as well as engagement state; that is, high achievers may at some point in their program descend to lower engagement states and still continue to have higher achievement [3]. Such patterns, variability, and individual differences are not limited to engagement, but has been reported for almost every major disposition in education psychology [2].\nOn a general level, heterogeneity has been a hot topic in recent educational literature. Several calls have been voiced to adopt methods that capture different patterns or subgroups within students’ behavior or functioning. Assuming that there is “an average” pattern that represents the entirety of student populations requires the measured construct to have the same causal mechanism, same development pattern, and affect students in exactly the same way. The average assumption is of course impossible and has been proven inaccurate across a vast number of studies (e.g., [2] and [4]). Since heterogeneity is prevalent in psychological, behavioral, and physiological human data, person-centered methods will remain a very important tool for researchers [46].\nPerson-centered methods can be grouped into algorithmic clustering methods on one hand and the model-based clustering paradigm on the other, with the former being more traditional and the latter being more novel in the learning analytics literature. The analysis of the SEM data centered here on the model-based approach, specifically the finite Gaussian mixture model framework. The mclust package enabled such models to be easily fitted and this framework exhibits many advantages over traditional clustering algorithms which rely on dissimilarity-based heuristics. Firstly, the likelihood-based underpinnings enable the selection of the optimal model using principled statistical model selection criteria. In particular, it is noteworthy in the present analysis that the model selection procedure was not limited to three-cluster solutions: mixtures with fewer or greater than three clusters were evaluated and the three-cluster solution —supported by previous studies in education research— was identified as optimal according to the BIC. Secondly, the parsimonious modelling of the covariance structures provides the flexibility to model clusters with different geometric characteristics. In particular, the clusters in the present analysis, whereby each group is described by a single Gaussian component with varying volume and shape, but the same orientation aligned with the coordinate axes are more flexible than the spherical, Euclidean distance-based clusters obtainable under the \\(k\\)-means algorithm. Thirdly, the models relax the assumption that each observation is associated with exactly one cluster and yields informative cluster-membership probabilities for each observation, which can be used to compute useful diagnostics such as entropies and average posterior probabilities which are unavailable under so-called “hard” clustering frameworks. Finally, the mclust package facilitates simple summaries and visualisations of the resulting clusters and cluster-specific parameter estimates.\nThat being said, there are a number of methodological limitations of the GMM framework to be aware of in other settings. Firstly, and most obviously, such models are inappropriate for clustering categorical or mixed-type variables. For clustering longitudinal categorical sequences, such as those in Chapter 10 [47], model-based approaches are provided by the mixtures of exponential-distance models framework of [48] (and related MEDseq R package) and the mixtures of hidden Markov models framework of [49] (and related seqHMM package; see Chapter 12 [50]). Regarding mixed-type variables, [51] provide a model-based framework (and the related clustMD package).\nSecondly, the one-to-one correspondence typically assumed between component distributions and clusters is not always the case [52]. This is only true if the underlying true component densities are Gaussian. When the assumption of component-wise normality is not satisfied, the performance of such models will deteriorate as more components are required to fit the data well. However, even for continuous data, GMMs tend to overestimate the number of clusters when the assumption of normality is violated. Two strategies for dealing with this are provided by the mclust package, one based on combining Gaussian mixture components according to an entropy criterion, and one based on a adding a so-called “noise component” —represented by a uniform distribution— to the mixture. The noise component captures outliers which do not fit the prevailing patterns of Gaussian clusters, which would otherwise be assigned to (possibly many) small clusters and minimises their deleterious effect on parameter estimation for the other, more defined clusters. Further details of combining components and adding a noise component can be found in [30]. Alternatively, mixture models which depart from normality have been an active area of research in model-based clustering in recent years. Such approaches —some of which are available in the R package mixture [53]— replace the underlying Gaussian component distributions with e.g., generalised hyperbolic distributions, the multivariate \\(t\\) distribution, and the multivarate skew-\\(t\\) distribution.\nA third main limitation of GMMs is their ineffectiveness in high-dimensional settings, when the data dimension \\(d\\) is comparable to or even greater than \\(n\\). Among the 14 parsimonious parameterisations available in mclust, only models with diagonal covariance structures are tractable when \\(n \\le p\\). Incorporating factor-analytic covariance decompositions in so-called finite Gaussian mixtures of factor analysers have been proposed for addressing this issue [54, 55]. Imposing constraints on the parameters of such factor-analytic structures in the component covariance matrices in the spirit of mclust leads to another family of parsimonious Gaussian mixture models [56], which are implemented in the R package pgmm. Model selection becomes increasingly difficult with such models, given the need to choose both the optimal number of mixture components and the optimal number of latent factors (as well as the covariance parameterisation, in the case of pgmm). Infinite mixtures of infinite factor analysers —implemented in the R package IMIFA— are a recent, Bayesian extension which enable automatic inference of the number of components and the numbers of cluster-specific latent factors [57].\nAnother recent extension, building directly on the 14 models from mclust, is the MoEClust model family of [58] and the associated MoEClust R package, which closely mimics its syntax. MoEClust effectively embeds Gaussian parsimonious clustering models in the mixtures of experts framework, enabling additional sources of heterogeneity in the form of covariates to be incorporated directly in the clustering model, to guide the construction of the clusters. Either, neither, or both the mixing proportions and/or component mean parameters can be modelled as functions of these covariates. The former is perhaps particularly appealing, given its analogous equivalence to latent profile regression [59]. Hypothetically, assuming information on the gender and age of the students in the present analysis was available, such covariates would influence the probabilities of cluster membership under such a model, while the correspondence thereafter between the parameters of the component distributions and the clusters would have the same interpretation as per standard LPA models."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#introduction",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#introduction",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "1 Introduction",
    "text": "1 Introduction\nPatterns exist everywhere in our life, from the sequence of genes to the order of steps in cooking recipes. Discovering patterns, variations, regularities, or irregularities is at the heart of scientific inquiry and, therefore, several data mining methods have been developed to understand patterns. Sequence analysis —or sequence mining— was developed almost four decades ago to address the increasing needs for pattern mining [1]. Ever since, a wealth of applications, algorithms, and statistical tools have been developed, adapted, or incorporated into the array of sequence analysis. Since sequence mining has been conceptualized, it has grown in scale of adoption and range of applications across life and social sciences [5] and education research was no exception (e.g., [7]). As a data mining technique, sequence mining has been commonly implemented to identify hidden patterns that would otherwise be missed using other analytical techniques and find interesting subsequences (parts of the sequence) that have practical significance or unexpected sequences that we didn’t know existed [9]. For instance, by mining sequences of collaborative dialogue, we could identify which sequences are followed by more conductive to more argumentative interactions, and what sequences are crucial to the collaborative process. A literature review of the common applications follows in the next section.\nLearning is a process that unfolds in time, a process that occurs in sequences of actions, in repeated steps, in patterns that have meanings and value for understanding learners’ behavior [10]. The conceptualization of learning as a process entails two important criteria: process as a sequence of states that unfold in time and process as a transformative mechanism that drives the change from one state to another [11]. Thereupon, methods such as sequence mining have gained increasing grounds and amassed a widening repertoire of techniques in the field of education to study the learning process. In particular, sequence mining has been used to harness the temporal unfolding of learners’ behavior using digital data, transcripts of conversations, or behavioral states [12]. Nevertheless, sequence mining can be used to study non-temporal sequences such as protein sequences and other types of categorical data [4].\nWhat makes sequences in education interesting is that they have patterns of repeated or recurrent sequences. Finding such patterns has helped typify learners’ behaviors and identify which patterns are associated with learning and which are associated with unfavorable outcomes [7]. Sequence mining can also describe a pathway or a trajectory of events, for example, how a student proceeds from enrolment to graduation [14], and help to identify the students who have a stable trajectory, who are turbulent, and who are likely to falter along their education [7]."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#review-of-the-literature",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#review-of-the-literature",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "2 Review of the literature",
    "text": "2 Review of the literature\nIn recent years, sequence analysis has become a central method in learning analytics research due to its potential to summarize and visually represent large amounts of student-related data. In this section we provide an overview of some of the most representative studies in the published literature. A summary of the studies reviewed in this section can be seen in Table 10.1. A common application of sequence analysis is the study of students’ log data extracted from their interactions with online learning technologies (mosty learning management systems, LMSs) throughout a learning session [15–17]. In some studies, the session is well-delimited, such as the duration of a game [18] or moving window [19], but in most cases it is inferred from the data, considering a session as an uninterrupted sequence of events [15, 16]. Few are the studies in which longer sequences are studied, covering a whole course or even a whole study program [7, 14, 20]. In such studies, the sequences are not composed of instantaneous interactions but rather of states that aggregate students’ information over a certain period, for example, we can study students’ engagement [14], learning strategies [7], or collaboration roles [20] for each course in a study program.\nMost of the existing research has used clustering techniques to identify distinct groups of similar sequences. Agglomerative Hierarchical Clustering (AHC) has been the most recurrently used technique, with a wealth of distance measures such as Euclidean [7, 15], Longest Common Subsequence [18], Longest Common Prefix [21], and Optimal Matching [16]. Other works have relied on Markovian Models [13, 20] or differential sequence mining [6]. Throughout the remainder of the book, we provide an introduction to sequence analysis as a method, describing in detail the most relevant concepts for its application to educational data. We provide a step-by-step tutorial of how to implement sequence analysis in a data set of student log data using the R programming language.\n\n\nTable 1. Summary of the reviewed articles about sequence analysis in learning analytics\n\n\n\n\n\n\n\n\n\n\nRef.\nContext\nTime scheme\nActor\nAlphabet\nClustering algorithm\n\n\n\n\n[6]\n40 students\nLearning activity (5 days)\nStudent\nLMS events\nDifferential sequence mining (core algorithm)\n\n\n[19]\n1 middle school class (40 students)\nLearning activity (5 days)\nStudent\nLMS events (e.g., Read, Linkadd)\nDifferential sequence mining (core algorithm, SPAMc)\n\n\n[16]\n1 university course (290 students)\nSession\nStudent-session\nLMS events\nAHC (Optimal matching)\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering\nAHC (Euclidean distance)\n\n\n[15]\n3 courses: one university course with 3 course offerings (1135 students), another university course with 2 course offerings (487 students), and a MOOC with a single offering (368 students)\nSession\nStudent-session\nLMS events (e.g., content_access, download)\nFirst Order Markov Model\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering\nAHC (Euclidean distance)\n\n\n[14]\n15 university courses (106 students)\nStudy program (15 courses)\nStudent\nEngagement state (e.g., Active, Average)\nHidden Markov Models\n\n\n[18]\n1 educational escape room game in a university course (96 students)\nEscape room game (1h 45m)\nTeam\nGame activity (e.g., hint obtained, puzzle solving)\nAHC (Longest Common Subsequence)\n\n\n[20]\n10 university courses (329 students)\nStudy program (10 courses)\nStudent\nRoles in the group (e.g., Leaders, Mediators)\nMixture Hidden Markov Models\n\n\n[7]\n10 university courses (135 students)\nSession\nStudent-session\nLMS event (e.g., Course main view, Forum consume)\nMixture Hidden Markov Models\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering (e.g., Lecture read, Forum read)\nAHC (Euclidean distance)\n\n\n\n\nStudy program (10 courses)\nStudent\nCourse-level strategies from previous clustering (e.g., Light interactive, Moderate interactive)\nAHC (Euclidean distance)\n\n\n[21]\n1 university courses, 4 Course offerings (200 students)\nWeek\nGroup of students\nInteraction type on forum (e.g., Discuss, Argue)\n\n\n\n\n\nSession\nStudent-session\nInteraction type on forum (e.g., Discuss, Argue)\nAHC (Longest Common Prefix)"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#basics-of-sequences",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#basics-of-sequences",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "3 Basics of sequences",
    "text": "3 Basics of sequences\nSequences are ordered lists of discrete elements (i.e., events, states or categories). Such elements are discrete (in contrast to numerical values such as grades) and are commonly organized chronologically. Examples include sequence of activities, sequence of learning strategies, or sequence of behavioral states [23]. A sequence of learning activities may include (play video - solve exercise - taking quiz - access instructions) [13], other examples include sequence of game moves e.g., (solve puzzle - request hint - complete game) [18], or collaborative roles, for instance, (leader - mediator - isolate) [20].\nBefore going into sequence analysis, let’s discuss a basic example of a sequence inspired by [14]. Let’s assume we are tracking the engagement states of students from a course to the next and for a full year that has five courses. The engagement states can be either engaged (when the student is fully engaged in their learning), average (when the student is moderately engaged), and disengaged (when the student is barely engaged). Representing the sequence of engagement states of two hypothetical students may look like the example on Table 10.2.\n\n\n\nTable 2. An example sequence\n\n\n\n\n\n\n\n\n\n\nActor\nCourse 1\nCourse 2\nCourse 3\nCourse 4\nCourse 5\n\n\n\n\nStudent 1\nAverage\nEngaged\nEngaged\nEngaged\nEngaged\n\n\nStudent 2\nAverage\nDisengaged\nDisengaged\nDisengaged\nDisengaged\n\n\n\n\n\nThe first student starts in course 1 with an Average engagement state, in Course 2, the student is engaged, and so in all the subsequent courses Course 3, Course 4, and Course 5. The student in row 2 has a Disengaged state in course 2 onwards. As we can see from the two sequences here, there is a pattern that repeats in both sequences (both students stay 4 consecutive courses in the same state). In real-life examples, sequences are typically longer and in larger numbers. For instance, the paper by [14] contains 106 students for a sequence of 15 courses. Finding repeated patterns of engaged states similar to the first student or repeated patterns of disengaged states like the other student would be interesting and helpful to understand how certain subgroups of students proceed in their education and how that relates to their performance.\n\n3.1 Steps of sequence analysis\nSeveral protocols exist for sequence analysis that vary by discipline, research questions, type of data, and software used. In education, sequence analysis protocol usually follows steps that include preparing the data, finding patterns, and relating these patterns to other variables e.g., performance e.g., [16]. The protocol which will be followed in this manual includes six steps:\n\nIdentifying (or coding) the elements of the sequence, commonly referred to as alphabet\nSpecifying the time window or epoch (time scheme) or sequence alignment scheme\nDefining the actor and building the sequence object\nVisualization and descriptive analysis\nFinding similar groups or clusters of sequences,\nAnalyzing the groups and/or using them in subsequent analyses.\n\n\n3.1.1 The alphabet\nThe first step of sequence analysis is defining the alphabet which are the elements or the possible states of the sequence [23]. This process usually entails “recoding” the states to optimize the granularity of the alphabet. In other words, to balance parsimony versus granularity and detail of the data. Some logs are overly detailed and therefore would require a careful recoding by the researcher [4]. For instance, the logs of Moodle (the LMS) include the following log entries for recoding students’ access to the quiz module: quiz_attempt, quiz_continue_attempt, quiz_close_attempt, quiz_view, quiz_view_all, quiz_preview. It makes sense here to aggregate (quiz_attempt, quiz_continue_attempt, quiz_close_attempt) into one category with the label attempt_quiz and (quiz_view, quiz_view all, quiz preview) to a new category with the label view_quiz. Optimizing the alphabet into a reasonable number of states also helps reduce complexity and facilitates interpretation. Of course, caution should be exercised not to aggregate meaningfully distinct states to avoid masking important patterns within the dataset.\n\n\n3.1.2 Specifying the time scheme\nThe second step is to define a time scheme, time epoch or window for the analysis. Sometimes the time window is fairly obvious, for instance, in case a researcher wants to study students’ sequence of courses in a program, the window can be the whole program e.g., [14]. Yet, oftentimes, a decision has to be taken about the time window which might affect the interpretation of the resulting sequences. For example, when a researcher is analyzing the sequence of interactions in a collaborative task, he/she may consider the whole collaborative task as a time window or may opt to choose segments or steps within the task as time epochs. In the same way, analyzing the sequence of tasks in a course, one would consider the whole course to be the time window for analysis or analyze the sequence of steps in each course task e-g., [24].\nIn online learning, the session has been commonly considered the time window e.g., [13, 16]. A session is an uninterrupted sequence of online activity which can be inferred from identifying the periods of inactivity as depicted in Figure 10.1. As can be seen, a user can have multiple sessions across the course. There is no standard guideline for what time window a researcher should consider, however, it is mostly defined by the research questions and the aims of analysis.\n\n\n\nFigure 1. Every line represents a click, a sequence of successive clicks is a session, and the session is defined by the inactivity period\n\n\n\n\n3.1.3 Defining the actor\nThe third important step is to define the actor or the unit of analysis of the sequences (see the actor in Table 10.3 or User in Table 10.4). The actor varies according to the type of analysis. When analyzing students’ sequences of actions, we may choose the student to be the actor and build a sequence of all student actions e.g., [24]. In online learning, sequence mining has always been created for “user sessions” i.e., each user session is represented as a sequence e.g., [13, 25] and therefore, a user typically has several sessions along the course. In other instances, you may be interested in the study of the sequences of the students’ states, for example engagement states in [14] where the student was the actor, or a group of collaborating students’ interactions as a whole such as [21] where the whole group is the actor. In the review of the literature, we have seen examples of such decisions and how varied they can be.\n\n\n3.1.4 Building the sequences\nThis step is specific to the software used. For example, in TraMineR the step includes specifying the dataset on which the building of the sequences is based and telling TraMineR the alphabet, the time scheme, and the actor id variable, as well as other parameters of the sequence object. This step will be discussed in detail in the analysis section.\n\n\n3.1.5 Visualizing and exploring the sequence data\nThe fourth step is to visualize the data and perform some descriptive analysis. Visualization allows us to summarize data easily and to see the full dataset at once. TraMineR includes several functions to plot the common visualization techniques, each one showing a different perspective.\n\n\n3.1.6 Calculating the dissimilarities between sequences\nThe fifth step is calculating dissimilarities or distances between pairs of sequences. Dissimilarity measures are a quantitative estimation of how different —or similar— the sequences are. Since there are diverse contexts, analysis objectives and sequence types, it is natural that there are several methods to compute the dissimilarities based on different considerations.\nOptimal matching (OM) may be the most commonly used dissimilarity measure used in social sciences and possibly also in education [8]. Optimal matching represents what it takes to convert or edit a sequence to become identical to another sequence. These edits may involve insertion, deletion (together often called indel operations) or substitution. For instance, in an example in Table 10.3, where we see a sequence of five students’ engagement states, we can edit Vera’s sequence and substitute the disengaged state with an average state; Vera’s sequence will become identical with Luis’ sequence. That is, editing Vera’s sequence takes one substitution to convert her sequence to that of Luis. We can also see that it will take four substitutions to convert Anna’s sequence to Maria’s sequence. In other words, Anna’s sequence is highly dissimilar to Maria. Different types of substitutions can be given different costs depending on how (dis)similar the two states are viewed (referred to as substitution costs). For example, the cost of substituting state engaged with state average might have a lower cost than substituting engaged with disengaged, since being disengaged is regarded most dissimilar to being engaged while average engagement is more similar to it. Since contexts differ, there are different ways of defining or computing the pairwise substitution costs matrix.\nOptimal matching derives from bioinformatics where transformations such as indels and substitutions are based on actual biological processes such as the evolution of DNA sequences. In many other fields such a transformation process would be unrealistic. In social sciences, [8] outlined five socially meaningful aspects and compared dissimilarity measures to determine how sensitive they are to the different aspects. These similarities are particularly relevant since learning, behavior, and several related processes e.g., progress in school or transition to the labor market are essentially social processes. We explain these aspects based on an example using fictional data in Table 10.3 following [14].\n\n\n\nTable 3. A sequence of engagement states using fictional data\n\n\n\n\n\n\n\n\n\n\nActor\nCourse 1\nCourse 2\nCourse 3\nCourse 4\nCourse 5\n\n\n\n\nMaria\nEngaged\nEngaged\nEngaged\nEngaged\nAverage\n\n\nVera\nDisengaged\nDisengaged\nAverage\nEngaged\nEngaged\n\n\nAnna\nAverage\nDisengaged\nDisengaged\nAverage\nAverage\n\n\nLuis\nDisengaged\nAverage\nAverage\nEngaged\nEngaged\n\n\nBob\nEngaged\nEngaged\nAverage\nEngaged\nEngaged\n\n\n\n\n\n\nExperienced states: how similar are the unique states forming the sequence. For instance, Maria and Bob in Table 10.3 have both experienced the same states (engaged and average).\nDistribution of the states: how similar is the distribution of states. We can consider that two sequences are similar when students spend most of their time in the same states. For instance, Bob and Maria have 80% engaged states and 20% average states.\nTiming: the time when each state occurs. For instance, two sequences can be similar when they have the same states occurring at the same time. For instance, Vera and Luis start similarly in a disengaged state, visit the average state in the middle, and finish in the engaged state.\nDuration: the durations of time spent continuously in a specific state (called spells) e.g., the durations of engaged states shared by the two sequences. For instance, Vera and Anna both had spells of two successive states in the disengaged state while Bob had two separate spells in the engaged state (both of length 2).\nSequencing: The order of different states in the sequence, for instance, Vera and Luis had similar sequences starting as disengaged, moving to average and then finishing as engaged.\n\nOf the aforementioned aspects, the first two can be directly determined from the last three. Different dissimilarity measures are sensitive to different aspects, and it is up to the researcher to decide which aspects are important in their specific context. Dissimilarity measures can be broadly classified in three categories [8]:\n\ndistance between distributions,\ncounting common attributes between sequences, and\nedit distances.\n\nCategory 1 includes measures focusing on the distance between distributions including, e.g., Euclidean distance and \\(\\chi^2\\)-distance that compare the total time spent in each state within each sequence. The former is based on absolute differences in the proportions while the latter is based on weighted squared differences.\nCategory 2 includes measures based on counting common attributes. For example, Hamming distances are based on counting the (possibly weighted) sum of position wise mismatches between the two sequences, the length of the longest common subsequence (LCS) is the number of shared states between two sequences that occur in the same order in both, while the subsequence vector representation -based metric (SVRspell) is counted as the weighted number of matching subsequences.\nCategory 3 includes edit distances that measure the costs of transforming one sequence to another by using edit operations (indels and substitutions). They include (classic) OM with different cost specifications as well as variants of OM such as OM between sequences of spells (OMspell) and OM between sequences of transitions (OMstran).\nStuder and Ritschard [8] give recommendations on the choice of dissimilarity measure based on simulations on data with different aspects. If the interest is on distributions of states within sequences, Euclidean and \\(\\chi^2\\)-distance are good choices. When timing is of importance, the Hamming distances are the most sensitive to differences in timing. With specific definitions also the Euclidean and \\(\\chi^2\\)-distance can be made sensitive to timing – the latter is recommended if differences in rare events are of particular importance. When durations are of importance, then OMspell is a good choice, and also LCS and classic OM are reasonable choices. When the main interest is in sequencing, good choices include OMstran, OMspell, and SVRspell with particular specifications. If the interest is in more than one aspect, the choice of the dissimilarity measure becomes more complex. By altering the specifications in measures such as OMstran, OMspell, and SVRspell the researcher could find a balance between the desired attributes. See [8] for more detailed information on the choice of dissimilarity measures and their specifications.\nDissimilarities are hard to interpret as such (unless the data are very small), so further analyses are needed to decrease the complexity. The most typical choice is to use cluster analysis for finding groups of individuals with similar patterns [22]. Other distance —or dissimilarity—based techniques include visualizations with multidimensional scaling [27], finding representative sequences [28], and ANOVA-type analysis of discrepancies [29].\n\n\n3.1.7 Finding similar groups or clusters of sequences\nThe sixth step is finding similar sequences, i.e., groups or patterns within the sequences where sequences within each group or cluster are as close to each other as possible and as different from other patterns in other clusters as possible. For instance, we can detect similar groups of sequences that show access patterns to online learning which are commonly referred to as tactics e.g., [7]. Such a step is typically performed using a clustering algorithm which may –or may not– require dissimilarity measures as an input [22, 26]. Common clustering algorithms that use a dissimilarity matrix are the hierarchical clustering algorithms. Hidden Markov models are among the most non-distance based cluster algorithms. See the remaining chapters about sequence analysis chapters for examples of these algorithms [30–32].\n\n\n3.1.8 Analyzing the groups and/or using them in subsequent analyses\nAnalysis of the identified patterns or subgroups of sequences is an important research question in many studies and oftentimes, it is the guiding research question. For instance, researchers may use log data to create sequences of learning actions, identify subgroups of sequences, and examine the association between the identified patterns and performance e.g., [6, 7, 13], associate the identified patterns with course and course delivery [15], examine how sequences are related to dropout using survival analysis [14], or compare sequence patterns to frequencies [21].\n\n\n\n3.2 Introduction to the technique\nBefore performing the actual analysis with R code, we need to understand how the data is processed for analysis. Four important steps that require more in-depth explanation will be clarified here, those are: defining the alphabet, the timing scheme, specifying the actor, and visualization. Oftentimes, the required information to perform the aforementioned steps are not readily obvious in the data and therefore some preparatory steps need to be taken to process the file.\nThe example shown in Table 10.4 uses fictional log trace data similar to those that come from LMSs. To build a sequence from the data in Table 10.4, we can use the Action column as an alphabet. If our aim here is to model the sequence of students’ online actions, this is a straightforward choice that requires no preparation. Since the log trace data has no obvious timing scheme, we can use the session as a time scheme. To compute the session, we need to group the actions that occur together without a significant delay between actions (i.e., lag) that can be considered as an inactivity (see Section 10.3.1.2). For instance, Layla’s actions in Table 10.4 started at 18:44 and ended at 18:51. As such, all Layla’s actions occurred within 7 minutes. As Table 10.4 also shows, the first group of Layla’s actions occur within 1 to 2 minutes of lag. The next group of actions by Layla occur after almost one day, an hour and six minutes (1506 minutes) which constitutes a period of inactivity long enough to divide Layla’s actions into two separate sessions. Layla’s actions on the first day can be labeled Layla-session1 and her actions on the second day are Layla-session2. The actor in this example is a composite of the student (e.g., Layla) and the session number. The same for Sophia and Carmen: their actions occurred within a few minutes and can be grouped into the sessions. Given that we have the alphabet (Action), the timing scheme (session), and the actor (user-session), the next step is to order the alphabet chronologically. In Table 10.4, the actions were sequentially ordered for every actor according to their chronological order. The method that we will use in our guide requires the data to be in so-called “wide format”. This is performed by pivoting the data, or creating a wide form where the column names are the order and the value of the Action column is sequentially and horizontally listed as shown in Table 10.5.\n\n\nTable 4. The first three columns are simulated sequence data and the three gray columns are computed.\n\n\n\n\n\n\n\n\n\n\nUser\nAction\nTime\nLag\nSession    \nOrder\n\n\n\n\nLayla\nCalendar\n9.1.2023 18:44\n-\nLayla session 1\n1\n\n\nLayla\nLecture\n9.1.2023 18:45\n1\nLayla session 1\n2\n\n\nLayla\nInstructions\n9.1.2023 18:47\n2\nLayla session 1\n3\n\n\nLayla\nAssignment\n9.1.2023 18:49\n2\nLayla session 1\n4\n\n\nLayla\nLecture\n9.1.2023 18:50\n1\nLayla session 1\n5\n\n\nLayla\nVideo\n9.1.2023 18:51\n1\nLayla session 1\n6\n\n\nSophia\nLecture\n9.1.2023 20:08\n-\nSophia session 1\n1\n\n\nSophia\nInstructions\n9.1.2023 20:12\n4\nSophia session 1\n2\n\n\nSophia\nAssignment\n9.1.2023 20:14\n2\nSophia session 1\n3\n\n\nSophia\nAssignment\n9.1.2023 20:18\n4\nSophia session 1\n4\n\n\nSophia\nAssignment\n9.1.2023 20:21\n3\nSophia session 1\n5\n\n\nCarmen\nLecture\n10.1.2023 10:08\n-\nCarmen session 1\n1\n\n\nCarmen\nVideo\n10.1.2023 10:11\n3\nCarmen session 1\n2\n\n\nLayla\nInstructions\n10.1.2023 19:57\n1506\nLayla session 2\n1\n\n\nLayla\nVideo\n10.1.2023 20:01\n4\nLayla session 2\n2\n\n\nLayla\nLecture\n10.1.2023 20:08\n7\nLayla session 2\n3\n\n\nLayla\nAssignment\n10.1.2023 20:14\n6\nLayla session 2\n4\n\n\n\n\n\n\nTable 5. A sequence of engagement states using fictional data\n\n\n\n\n\n\n\n\n\n\n\nActor\n1\n2\n3\n4\n5\n6\n\n\n\n\nLayla session1\nCalendar\nLecture\nInstructions\nAssignment\nLecture\nVideo\n\n\nSophia session1\nLecture\nInstructions\nAssignment\nAssignment\nAssignment\n\n\n\nCarmen session1\nLecture\nVideo\n\n\n\n\n\n\nLayla session2\nInstructions\nVideo\nLecture\nAssignment\n\n\n\n\n\n\nThe following steps are creating a sequence object using sequence mining software and using the created sequence in analysis. In our case, we use the TraMineR framework which has a large set of visualization and statistical functions. Sequences created with TraMineR also work with a large array of advanced tools, R packages, and extensions. However, it is important to understand sequence visualizations before delving into the coding part.\n\n\n3.3 Sequence Visualization\nTwo basic plots are important here and therefore will be explained in detail. The first is the index plot (Figure 10.2) which shows the sequences of stacked colored bars representing spells, with each token represented by a different color. For instance, if we take Layla’s actions (in session1) and represent them as an index plot, they will appear as shown in Figure 10.2 (see the arrow). Where the Calendar is represented as a purple bar, the Lecture as a yellow bar, and instructions as an orange bar etc. Figure 10.2 also shows the visualization of sequences in Table 10.5 and you can see each of the session sequences as stacked colored bars following their order in the table. Nevertheless, sequence plots commonly include a large number of sequences that are of the order of hundreds or thousands of sequences and may be harder to read than the one presented in the example (see examples in the next sections).\n\n\n\nFigure 2. A table with ordered sequences of actions and the corresponding index plot; the arrow points to Layla’s actions.\n\n\nThe distribution plot is another related type of sequence visualization. Distribution plots —as the name implies— represent the distribution of each alphabet at each time point. For example, if we look at Figure 10.3 (top) we see 15 sequences in the index plot. At time point 1, we can count eight Calendar actions, two Video actions, two Lecture actions and one Instruction action. If we compute the proportions: we get 8/15 (0.53) of Calendar actions; for Video, Assignment, and Lecture we get 2/15 (0.13) in each case, and finally Instructions actions account for 1/15 (0.067). Figure 10.3 (bottom) shows these proportions. At time point 1, we see the first block Assignment with 0.13 of the height of the bar, followed by the Calendar which occupies 0.53, then a small block (0.067) for the Instructions, and finally two equal blocks (0.13) representing the Video and Lecture actions.\nSince the distribution plot computes the proportions of activities at each time point, we see different proportions at each time point. Take for example, time point 6, we have only two actions (Video and Assignment) and therefore, the plot has 50% for each action. At the last point 7, we see 100% for Lecture. Distribution plots need to be interpreted with caution and in particular, the number of actions at each time point need to be taken into account. One cannot say that at the 7th time point, 100% of actions were Lecture, since it was the only action at this time point. Furthermore, distribution plots do not show the transitions between sequences and should not be interpreted in the same way as the index plot.\n\n\n\nFigure 3. Index plot (top) and distribution plot (bottom)"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#analysis-of-the-data-with-sequence-mining-in-r",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#analysis-of-the-data-with-sequence-mining-in-r",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "4 Analysis of the data with sequence mining in R",
    "text": "4 Analysis of the data with sequence mining in R\n\n4.1 Important packages\nThe most important package and the central framework that we will use in our analysis is the TraMineR package. TraMineR is a toolbox for creating, describing, visualizing and analyzing sequence data. TraMineR accepts several sequence formats, converts to a large number of sequence formats, and works with other categorical data. TraMineR computes a large number of dissimilarity measures and has several integrated statistical functions. TraMineR has been mainly used to analyze live event data such as employment states, sequence of marital states, or other life events. With the emergence of learning analytics and educational data mining, TraMineR has been extended into the educational field [33]. In the current analysis we will also need the packages TraMineRextras, WeightedCluster, and seqhandbook, which provide extra functions and statistical tools. The first code block loads these packages. In case you have not already installed them, you may need to install them.\n\nlibrary(TraMineR)\nlibrary(TraMineRextras)\nlibrary(WeightedCluster)\nlibrary(seqhandbook)\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(cluster)\nlibrary(MetBrewer)\nlibrary(reshape2)\n\n\n\n4.2 Reading the data\nThe example that will be used here is a Moodle log dataset that includes three important fields: the User ID (user), the time stamp (timecreated), and the actions (Event.context). Yet, as we mentioned before, there are some steps that need to be performed to prepare the data for analysis. First, the Event.context is very granular (80 different categories) and needs to be re-coded as mentioned in the basics of sequence mining section. We have already prepared the file with a simpler coding scheme where, for example, all actions intended as instructions were coded as instruction, all group forums were coded as group_work, and all assignment work was coded as Assignment. Thus, we have a field that we can use as the alphabet titled action. The following code reads the original coded dataset.\n\nSeqdatas <- \n  import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\")\n\n\n\n\n\n  \n\n\n\n\n\n4.3 Preparing the data for sequence analysis\nTo create a time scheme, we will use the methods described earlier in the basis of the sequence analysis section. The timestamp field will be used to compute the lag (the delay) between actions, find the periods of inactivity between the actions, and mark the actions that occur without a significant lag together as a session. Actions that follow with a significant delay will be marked as a new session. The following code performs these steps. First, the code arranges the data according to the timestamp for each user (see the previous example in the basics section), this is why we use arrange(timecreated, user). The second step (#2) is to group_by(user) to make sure all sessions are calculated for each user separately. The third step is to compute the lag between actions. Step #4 evaluates the lag length; if the lag exceeds 900 seconds (i.e., a period of inactivity of 900 seconds), the code marks the action as the start of a new session. Step #5 labels each session with a number corresponding to its order. The last step (#6) creates the actor variable, by concatenating the username with the string “Session_” and the session number; the resulting variable is called session_id.\n\nsessioned_data <- Seqdatas |>\n  arrange(timecreated, user) |> # Step 1\n  group_by(user) |> # Step 2\n  mutate(Time_gap = timecreated - (lag(timecreated))) |> # Step 3\n  mutate(new_session = is.na(Time_gap) | Time_gap > 900) |> # Step 4\n  mutate(session_nr = cumsum(new_session)) |> # Step 5\n  mutate(session_id = paste0 (user, \"_\", \"Session_\", session_nr)) # Step 6\n\nAn important question here is what is the optimum lag or delay that we should use to separate the sessions. Here, we used 900 seconds (15 minutes) based on our knowledge of the course design. In the course where the data comes from, we did not have activities that require students to spend long periods of idle time online (e.g., videos). So, it is reasonable here to use a relatively short delay (i.e., 900 seconds) to mark new sessions. Another alternative is to examine the distribution of lags or compute the percentiles.\n\nquantile(sessioned_data$Time_gap, \n         c(0.10, 0.50, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00), na.rm = TRUE)\n\nTime differences in secs\n    10%     50%     75%     80%     85%     90%     95%    100% \n      1       1      61      61     181     841   12121 1105381 \n\n\nThe previous code computes the proportion of lags at 10% to 100% and the results show that at 90th percentile, the length of lags is equal to 841 seconds, that is very close to the 900 seconds we set. The next step is to order the actions sequentially (i.e., create the sequence in each session) as explained in Section 10.3.1.2 and demonstrated in Table 10.4. We can perform such ordering using the function seq_along(session_id) which creates a new field called sequence that chronologically orders the action (the alphabet).\n\nsessioned_data <- sessioned_data |>\n  group_by(user, session_nr) |>\n  mutate(sequence = seq_along(session_nr)) |>\n  mutate(sequence_length = length(sequence))\n\nSome sessions are outliers (e.g., extremely long or very short) – there are usually very few– and therefore, we need to trim such extremely long sessions. We do so by calculating the percentiles of session lengths.\n\nquantile(sessioned_data$sequence_length, \n         c(0.05, 0.1, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99, 1.00),  na.rm = TRUE)\n\n  5%  10%  50%  75%  90%  95%  98%  99% 100% \n   3    4   16   29   42   49   59   61   62 \n\n\nWe see here that 95% of sequences lie within 49 states long and therefore, we can trim these long sessions as well as sessions that are only one event long.\n\nsessioned_data_trimmed <- sessioned_data %>% \n  filter(sequence_length > 1 & sequence <= 49)\n\nThe next step is to reshape or create a wide format of the data and convert each session into a sequence of horizontally ordered actions. For that purpose, we use the function dcast from the reshape2 package. For this function, we need to specify the ID columns (the actor) and any other properties for the users can be specified here also. We selected the variables user and session_id. Please note that only session_id is necessary (actor) but it is always a good idea to add variables that we may use as weights, as groups, or for later comparison. We also need to specify the sequence column and the alphabet (action) column. The resulting table is similar to Table 10.5.\nThe last step is creating the sequence object using the seqdef function from the TraMineR package. To define the sequence, we need the prepared file from the previous step (similar to Table 10.5) and the beginning and end of the columns to consider i.e., the start of the sequence. We have started from the fourth column since the first three columns are meta-data (user, session_id, and session_nr). To include all columns in the data we use the ncol function to count the number of columns in the data. Creating a sequence object enables the full potential of sequence analysis.\n\ndata_reshaped <- dcast(user + session_id + session_nr ~ sequence, \n                       data = sessioned_data_trimmed, \n                       value.var = \"Action\")\nSeqobject <- seqdef(data_reshaped, 4:ncol(data_reshaped))\n\n [>] found missing values ('NA') in sequence data\n [>] preparing 9383 sequences\n [>] coding void elements with '%' and missing values with '*'\n [>] 12 distinct states appear in the data: \n     1 = Applications\n     2 = Assignment\n     3 = Course_view\n     4 = Ethics\n     5 = Feedback\n     6 = General\n     7 = Group_work\n     8 = Instructions\n     9 = La_types\n     10 = Practicals\n     11 = Social\n     12 = Theory\n [>] 9383 sequences in the data set\n [>] min/max sequence length: 2/49\n \nAn optional —yet useful— step is to add a color palette to create a better looking plot. Choosing an appropriate palette with separable colors improves the readability of the plot by helping easily identify different alphabets.\n\nNumber_of_colors <- length(alphabet(Seqobject))\ncolors <- met.brewer(name = \"VanGogh2\", n = Number_of_colors)\ncpal(Seqobject) <- colors\n\n\n\n4.4 Statistical properties of the sequences\nA simple way to get the properties of the sequences is through the function summary(). The functions show the total number of sequences in the object, the number of unique sequences, and lists the alphabet. A better way to dig deeper into the sequence properties is to use the seqstatd() function which returns several statistics, most notably the relative frequencies, i.e., the proportions of each state at each time point or the numbers comprising the distribution plot. The function also returns the valid states, that is, the number of valid states at each time point as well as the transversal entropy, which is a measure of diversity of states at each time point [34]. The code in the next section computes the sequence statistics and then displays the results.\n\nsummary(Seqobject)\nseq_stats <- seqstatd(Seqobject)\nseq_stats$Frequencies \nseq_stats$Entropy \nseq_stats$ValidStates \n\n\nFrequenciesEntropyValid states\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n4.5 Visualizing sequences\nVisualization has a summarizing power that allows researchers to have an idea about a full dataset in one visualization. TraMineR allows several types of visualizations that offer different perspectives. The most common visualization type is the distribution plot (described earlier in Figure 10.3). To plot a distribution plot one can use the powerful seqplot function with the argument type=\"d\" or simply seqdplot().\n\nseqplot(Seqobject, type = \"d\")\n\n\n\n\nFigure 4. Sequence distribution plot\n\n\n\n\nThe default distribution plot has an y-axis that ranges from 0 to 1.0 corresponding to the proportion and lists the number of sequences which in our case is 9383 However, the default output of the seqdplot() function is rarely satisfactory and we need to use the function arguments to optimize the resulting plot. The help file contains a rather detailed list of arguments and types of visualizations that can be consulted for more options, which can be obtained like any other R function by typing ?seqplot. In this chapter we will discuss the most basic options. In Figure 10.4, we use cex.legend argument to optimize the legend text size, we use the ncol argument to make the legend spread over six columns, the argument legend.prop to make the legend a bit far away from the main plot so they do not overlap and we use the argument border=NA to remove the borders from the plot. With such small changes, we get a much cleaner and readable distribution plot. Please note, that in each case, you may need to optimize the plot according to your needs. It is important to note here that in the case of missing data or sequences with unequal lengths like ours —which is very common— the distribution plot may show results that are made of fewer sequences at later time points. As such, the interpretation of the distribution plot should take into account the number of sequences, missing data, and timing. An index plot may be rather more informative in cases where missing data is prevalent.\n\nseqplot(Seqobject, type = \"d\", cex.legend = 0.9, ncol = 6, cex.axis = 0.7, \n        legend.prop = 0.1, border = NA)\n\n\n\n\nFigure 5. Sequence distribution plot with customized arguments\n\n\n\n\nThe index plot can be plotted in the same way using seqplot() with the argument type=\"I\" or simply using seqIplot. The resulting plot (Figure 10.6) has each sequence of the 9383 plotted as a line of stacked colored bars. One of the advantages of index plots is that they show the transitions between states in each sequence spell. Of course, plotting more than nine thousand sequences results in very thin lines that may not be very informative. Nevertheless, index plots are very informative when the number of sequences is relatively small. Sorting the sequences could help improve the visualization. On the right side, we see the index plot using the argument “sortv =”from.start”, under which sequences are sorted by the elements of the alphabet at the successive positions starting from the beginning of the time window.\n\nseqplot(Seqobject, type = \"I\", cex.legend = 0.6, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.2, border = NA)\n\nseqplot(Seqobject, type = \"I\", cex.legend = 0.6, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.2, border = NA, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 6. Sequence index plots\n\n\n\n\nThe last visualization type we discuss here is the mean time plot, which plots the total time of every element of the alphabet across all time, i.e., a frequency distribution of all states regardless of their timing. As the plot in Figure 10.7 shows, group work seems to be the action that students performed the most, followed by course view.\n\nseqplot(Seqobject, type = \"mt\", cex.legend = 0.7, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.15, border = NA, ylim = c(0, 5))\n\n\n\n\nFigure 7. Mean time plot\n\n\n\n\n\n\n4.6 Dissimilarity analysis and clustering\nHaving prepared the sequences and explored their characteristics, we can now investigate if they have common patterns, recurrent sequences, or groups of similar sequences. This is a two-stage process; we will need to compute dissimilarities (along with associated substitution costs) and then perform cluster analysis on the resulting matrix. In the case of log trace data, clustering has always been performed to find learning tactics or sequences of students’ actions that are similar to each other or, put another way, patterns of similar behavior (e.g., [7, 16]). For the present analysis, we begin with the most common method for computing the dissimilarity matrix, that is Optimal Matching (OM). OM computes the dissimilarity between two sequences as the minimal cost of converting a sequence to the other. OM requires some steps that include specifying a substitution cost matrix, indel cost. Later, we use a clustering algorithm to partition the sequences according to the values returned by the OM algorithm [2, 35].\nA possible way to compute substitutions cost that has been commonly used – yet frequently criticized – in the literature is the TRATE method [36]. The TRATE method is data-driven and relies on transition rates; it assumes that pairs of states with frequent transitions between them should have “lower cost” of substitution (i.e., they are seen as being more similar). Thus, if we replace an action with another action that occurs often, it has a lower cost. This may be useful in some course designs, where some activities are very frequently visited and others are rare. The function seqsubm() is used to compute substitution costs with the TRATE method via:\n\nsubstitution_cost_TRATE <- seqsubm(Seqobject, method = \"TRATE\")\n\nIf we print the substitution cost matrix, we see that, for instance, the cost of replacing Applications with Applications is 0, whereas the cost of replacing Applications with Assignment (and vice versa) is higher (1.94). Since Course_view is the most common transition, replacing any action with Course_view tends to be the lowest in cost, which makes sense. Please note that the TRATE method is presented here for demonstration only. In fact, we do not recommend it to be used by default; readers should choose carefully what cost method best suits their data.\n\n\n\n\n  \n\n\n\nNevertheless, the most straightforward way of computing the cost is to use a constant cost; that is, to assume that the states are equally distant from one another. To do so, we can use the function seqsubm() and supply the argument method=\"CONSTANT\". In the following example, we assign a common cost of 2 (via the argument cval). We also refer below to other optional arguments which are not strictly necessary for the present application but nonetheless worth highlighting as options.\n\nsubstitution_cost_constant <- seqsubm(\n  Seqobject,            # Sequence object\n  method = \"CONSTANT\",  # Method to determine costs\n  cval = 2,             # Substitution cost\n  time.varying = FALSE, # Does not allow the cost to vary over time\n  with.missing = TRUE,  # Allows for missingness state\n  miss.cost = 1,        # Cost for substituting a missing state\n  weighted = TRUE)      # Allows weights to be used when applicable\n\nTo compute the OM dissimilarity matrix, the indel argument needs to be provided and we will use the default value 1 which is half of the highest substitution cost (2). We also need to provide the substitution cost matrix (sm). We opt for the matrix of constant substitution costs created above, given its straightforward interpretability.\n\ndissimilarities <- seqdist(Seqobject, method = \"OM\", indel = 1, \n                           sm = substitution_cost_constant)\n\nIn the resulting pairwise dissimilarity matrix, every sequence has a dissimilarity value with every other sequence in the dataset, and therefore, the dissimilarity matrix can be large and resource intensive in larger matrices. In our case, the dissimilarity matrix is 9383 * 9383 (i.e., 88,040,689) in size. With these dissimilarities between sequences as input, several distance-based clustering algorithms can be applied to partition the data into homogeneous groups. In our example, we use the hierarchical clustering algorithm from the package cluster by using the function hclust(), but note that the choice of clustering algorithm can also affect results greatly and should be chosen carefully by the reader. The seq_heatmap() function is used to plot a dendrogram of the index plot which shows a hierarchical tree of different levels of subgrouping and helps choose the number of clusters visually.\n\nclusters_sessionsh <- hclust(as.dist(dissimilarities), method = \"ward.D2\") \nseq_heatmap(Seqobject, clusters_sessionsh)\n\n\n\n\nFigure 8. Visualization of clusters using a dendrogram\n\n\n\n\nTo do the actual clustering, we use the function cutree() and with the argument k = 3 to cluster the sequence into three clusters according to the groups highlighted in Figure 10.8. The cutree function produces a vector of cluster numbers, we can create more descriptive labels as shown in the example and assign the results to an R object called Groups. Visualizations of the clustering results can be performed in a similar fashion to the earlier visualizations of the entire set of sequences: via seqplot(), with the desired type of plot, and the addition of the argument group. Readers have to choose the arguments and parameters according to contexts, research questions, and the nature of their data.\n\nCuts <- cutree(clusters_sessionsh, k = 3)\nGroups <- factor(Cuts, labels = paste(\"Cluster\", 1:3))\nseqplot(Seqobject, type = \"d\", group = Groups, cex.legend = 0.8, ncol = 2, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 9. Sequence distribution plot for the k=3 cluster solution\n\n\n\n\nHowever, the resulting clusters might not be the best solution and we need to try other dissimilarity measures and/or clustering algorithms, evaluate the results, and compare their fit indices. TraMineR provides several distance measures, the most common of which are:\n\nEdit distances: Optimal matching \"OM\" or optimal matching with sensitivity to certain factors, e.g., optimal matching with sensitivity to spell sequence (\"OMspell\") or with sensitivity to transitions (\"OMstran\").\nShared attributes: Distance based on the longest common subsequence (\"LCS\"), longest common prefix (\"LCP\"; which prioritizes sequence common initial states), or the subsequence vectorial representation distance (\"SVRspell\"; based on counting common subsequences).\nDistances between distributions of states: Euclidean (\"EUCLID\") distance or Chi-squared (“CHI2”).\n\nDetermining the distance may be done based on the research hypothesis, context, and the nature of the sequences. For instance, a researcher may decide to group sequences based on their common starting points (e.g., [21]) where the order and how a conversation starts matter. TraMineR allows the computation of several dissimilarities. The following code computes some of the most common dissimilarities and stores each in a variable that we can use later.\n\n# Edit distances and sequences\ndissimOMstran <- seqdist(Seqobject, method = \"OMstran\", otto = 0.1, \n                         sm = substitution_cost_constant, indel = 1)\ndissimOMspell <- seqdist(Seqobject, method = \"OMspell\", expcost = 0, \n                         sm = substitution_cost_constant, indel = 1)\ndissimSVRspell <- seqdist(Seqobject, method = \"SVRspell\", tpow = 0)\ndissimOM <- seqdist(Seqobject, method = \"OM\", otto = 0.1, \n                    sm = substitution_cost_constant, indel = 1)\n\n\n# Distances between state distributions\ndissimCHI2 <- seqdist(Seqobject, method = \"CHI2\", step = 1)\ndissimEUCLID <- seqdist(Seqobject, method = \"EUCLID\", step = 49)\n\n\n# Distances based on counts of common attribute e.g., duration (spell lengths)\ndissimOMspell <- seqdist(Seqobject, method = \"OMspell\", expcost = 1, \n                         sm = substitution_cost_constant, indel = 1)\ndissimLCS <- seqdist(Seqobject, method = \"LCS\")\ndissimLCP <- seqdist(Seqobject, method = \"LCP\")\ndissimRLCP <- seqdist(Seqobject, method = \"RLCP\")\n\nWe can then try each dissimilarity with varying numbers of clusters and compute the clustering evaluation measures. The function as.clustrange from the WeightedCluster package computes several cluster quality indices including, among others, the Average Silhouette Width (ASW) which is commonly used in cluster evaluation to measure the coherence of the clusters. A value above 0.25 means that the data has some structure or patterns, whereas a value below 0.25 signifies the lack of structure in the data. The function also computes the R^2 Value which represents the ratio of the variance explained by the clustering solution. The results can be plotted and inspected. We can see that four clusters seem to be a good solution. Table 10.6 and Figure 10.10 show that the ASW and CHsq measures are maximized for the four-cluster solution, for which other parameters such as R^2 are also relatively good. Thus, we can use the four cluster solution. We note the use of the norm=“zscoremed” argument which improves the comparability of the various metrics in Figure 10.10 by standardizing the values to make it easier to identify the maxima. Table 10.6, however, presents the values on their original scales. Finally, the ranges and other characteristics of each cluster quality metric are summarized in Table 10.7. For brevity, we proceed with only the Euclidean distance matrix.\n\ndissimiarities_tested <- dissimEUCLID\nClustered <- hclust(as.dist(dissimiarities_tested), method = \"ward.D2\")\nClustered_range <- as.clustrange(Clustered, diss = dissimiarities_tested, \n                                 ncluster = 10)\nplot(Clustered_range, stat = \"all\", norm = \"zscoremed\", lwd = 2)\n\n\n\n\nFigure 10. Cluster performance metrics. X-axis represents the number of clusters, Y-axis represents the fit index standardized value.\n\n\n\n\n\nClustered_range[[\"stats\"]]\n\n\n\n\n\n\n\nTable 6.  Cluster performance metrics \n  \n    \n    \n      PBC\n      HG\n      HGSD\n      ASW\n      ASWw\n      CH\n      R2\n      CHsq\n      R2sq\n      HC\n    \n  \n  \n    0.281\n0.336\n0.335\n0.268\n0.268\n2,230.420\n0.192\n3,565.535\n0.275\n0.319\n    0.417\n0.489\n0.488\n0.307\n0.307\n1,992.266\n0.298\n3,553.441\n0.431\n0.246\n    0.507\n0.614\n0.614\n0.333\n0.333\n1,952.437\n0.384\n3,985.571\n0.560\n0.190\n    0.491\n0.635\n0.634\n0.272\n0.272\n1,717.644\n0.423\n3,524.149\n0.601\n0.184\n    0.511\n0.677\n0.677\n0.287\n0.288\n1,580.850\n0.457\n3,301.251\n0.638\n0.165\n    0.534\n0.736\n0.735\n0.308\n0.309\n1,449.482\n0.481\n3,178.407\n0.670\n0.138\n    0.557\n0.812\n0.812\n0.324\n0.325\n1,362.368\n0.504\n3,110.913\n0.699\n0.104\n    0.559\n0.831\n0.830\n0.327\n0.327\n1,341.529\n0.534\n3,124.571\n0.727\n0.096\n    0.571\n0.865\n0.865\n0.329\n0.330\n1,274.303\n0.550\n3,085.893\n0.748\n0.080\n  \n  \n  \n\n\n\n\n\n\n\nTable 7. Measures of the quality of a partition. Note: Table is based on [22] with permission from the author [22]\n\n\n\n\n\n\n\n\n\nName\nAbrv.\nRange\nMin/Max\nInterpretation\n\n\n\n\nPoint Biserial Correlation\nPBC\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances.\n\n\nHubert’s Gamma\nHG\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances (order of magnitude).\n\n\nHubert’s Somers’ D\nHGSD\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances (order of magnitude) taking into account ties in distances.\n\n\nHubert’s C\nHC\n[0;1]\nMin\nGap between the partition obtained and the best partition theoretically possible with this number of groups and these distances.\n\n\nAverage Silhouette Width\nASW\n[-1;1]\nMax\nCoherence of assignments. High coherence indicates high between-group distances and strong within-group homogeneity.\n\n\nAverage Silhouette Width (weighted)\nASWw\n[-1;1]\nMax\nAs previous, for floating point weights.\n\n\nCalinski-Harabasz index\nCH\n[0; \\(+\\infty\\)[\nMax\nPseudo F computed from the distances.\n\n\nCalinski-Harabasz index\nCHsq\n[0; \\(+\\infty\\)[\nMax\nAs previous, but using squared distances.\n\n\nPseudo R2\nR2\n[0;1]\nMax\nShare of the discrepancy explained by the clustering solution (only to compare partitions with identical number of groups).\n\n\nPseudo R2\nR2sq\n[0;1]\nMax\nAs previous, but using squared distances.\n\n\n\n\nTo get the cluster assignment, we can use the results from the Clustered_range object and plot the clusters using the previously shown distribution, index, and mean time plot types.\n\ngrouping <- Clustered_range$clustering$cluster4\nseqplot(Seqobject, type = \"d\", group = grouping, cex.legend = 0.9, ncol = 6, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 11. Sequence distribution plot for the four clusters\n\n\n\n\n\nseqplot(Seqobject, type = \"I\", group = grouping, cex.legend = 0.9, ncol = 6, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 12. Sequence index plot for the four clusters\n\n\n\n\n\nseqplot(Seqobject, type = \"mt\", group = grouping, cex.legend = 1, ncol = 6, \n        cex.axis = .5, legend.prop = 0.2, ylim = c(0, 10))\n\n\n\n\nFigure 13. Mean time plot for the four clusters\n\n\n\n\nGiven the clustering structure, we also use a new plot type: the implication plot from the TraMineRextras package. Such a plot explicitly requires a group argument; in each of theseplots, at each time point, “being in this group implies to be in this state at this time point”. The strength of the rule is represented by a plotted line and a 95% confidence interval. Put another way, the more likely states have higher implicative values, which are more relevant when higher than the 95% confidence level.\n\nimplicaton_plot <- seqimplic(Seqobject, group = grouping)\nplot(implicaton_plot, conf.level = 0.95, cex.legend = 0.7)\n\n\n\n\nFigure 14. Implication plot for the four clusters\n\n\n\n\nGiven the implication plot as well as the other plots, the first cluster seems to be a mixed cluster with no prominent activity. Cluster 2 is dominated by practical activities, Cluster 3 is dominated by group work activities, Cluster 4 is dominated by assignments. Researchers usually give these clusters a label e.g., for Cluster 1, one could call it a diverse cluster. See some examples here in these papers [7, 15, 16]."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#more-resources",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#more-resources",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "5 More resources",
    "text": "5 More resources\nSequence analysis is a rather large field with a wealth of methods, procedures, and techniques. Since we have used the TraMineR software in this chapter, a first place to seek more information about sequence analysis would be to consult the TraMineR manuals and guides [22, 26, 37]. More tools for visualization can be found in the package ggseqplot [38]. The ggseqplot package reproduces similar plots to TraMineR with the ggplot2 framework as well as other interesting visualizations [39]. This allows further personalisation using the ggplot2 grammar, as we have learned in Chapter 6 of this book on data visualization [40]. Another important sequence analysis package is seqHMM [41], which contains several functions to fit hidden Markov models. In the next chapter, we see more advanced aspects of sequence analysis for learning analytics [30–32].\nTo learn more about sequence analysis in general, you can consult the book by Cornwell (2015), which is the first general textbook on sequence analysis in the context of social sciences. Another valuable resource is the recent textbook by Raab and Struffolino [38], which introduces the basics of sequence analysis and some recent advances as well as data and R code."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#acknowledgements",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#acknowledgements",
    "title": "10  Sequence analysis: Basic principles, technique, and tutorial",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nThis paper has been co-funded by the Academy of Finland (decision numbers 350560 and 331816), the Academy of Finland Flagship Programme (decision number 320162), the Strategic Research Council (SRC), FLUX consortium (decision numbers: 345130 and 345130), and the Swiss National Science Foundation (project “Strengthening Sequence Analysis”, grant No.: 10001A_204740)."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#introduction",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#introduction",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "1 Introduction",
    "text": "1 Introduction\nModeling a longitudinal process brings a lot of variability over time. The modeling procedure becomes even harder when we use multivariate continuous variables to model a single construct. For example, in education research we might model students’ online behavioral engagement through their number of clicks, time spent online, and frequency of interactions [1]. Most human behavioral constructs are an amalgam of interrelated features with complex fluctuations over time. Modeling such processes requires a method that takes into account the multidimensional nature of the examined construct as well as the temporal evolution. Nevertheless, despite the rich boundless information in the quantitative data, discrete patterns can be captured, modeled, and traced using appropriate methods. Such discrete patterns represent an archetype or a “state” that is typical of behavior or function [2]. For instance, a combination of frequent consumption of online course resources, long online time, interaction with colleagues and intense interactions in cognitively challenging collaborative tasks can be coined as an “engaged state” [3] The same student that shows an “engaged state” at one point, can transition to having few interactions and time spent online at the next time point, i.e., a “disengaged state”.\nCapturing a multidimensional construct into qualitative discrete states has several advantages. First, it avoids information overload where the information at hand is overwhelmingly hard to process accurately because of the multiplicity and lack of clarity of how to interpret small changes and variations. Second, it allows an easy way of communicating the information; it is understandable that communicating a state such as “engaged” is easier than telling the values of several activity variables. Third, it is easier to trace or track. As we are interested in significant shifts overtime, fine-grained changes in the activities are less meaningful. We are rather interested in significant shifts between behavioral states, e.g., from engaged to disengaged. Besides, such a shift is also actionable. As [4] puts it, “reliability can sometimes be improved by tuning grain size of data so it is neither too coarse, masking variance within bins, nor too fine-grained, inviting distinctions that cannot be made reliably.” More importantly, capturing the states is more tethered to reality of human nature and function. In fact, many psychological, physiological or disease constructs have been described as states with defining criteria, e.g., motivation, depression or migraine.\nExisting methods for longitudinal analysis are often limited to the study of a single variable’s evolution over time [5]. Some examples of such methods are longitudinal k-means [6], group-based trajectory modeling (GBTM) [7], or growth models [8]. However, when multivariate data is the target of analysis, these methods cannot be used. Multivariate methods are usually limited to one step or another of the analysis e.g., clustering of multivariate data into categorical variables (e.g., states), or chart the succession of categories into sequences. The method presented in this chapter provides an ensemble of methods and tools to effectively model, visualize and statistically analyse the longitudinal evolution of multivariate data. As such, modeling the temporal evolution of latent states, as we propose in this chapter, may not be entirely new and has been performed — at least in part— using several models, algorithms and software platforms [9–11]. For instance, the package lmfa can capture latent states in multivariate data, model their trajectories as well as transition probabilities [12]. Nevertheless, most of the existing methods are concerned with modeling disease states, time-to event (survival models), time-to-failure models [11] or lack a sequential analysis.\nThe VaSSTra method described in this chapter allows to summarize multiple variables into states that can be analyzed using sequence analysis across time. Then, using life-event methods, distinct trajectories of sequences that undergo a similar evolution can be analyzed in detail. VaSSTra consists of three steps: (1) capturing the states or patterns (from variables); (2) modeling the temporal process (from states); and (3) capturing the patterns of longitudinal development (similar sequences are grouped in trajectories). As such, the method described in this chapter is a combination of several methods. First, a person-centered method (latent class or latent profile analysis) is used to capture the unobserved “states” within the data. The states are then used to construct a “sequence of states”, where a sequence represents a person’s ordered states for each time point. The construction of “sequence of states” unlocks the full potential of sequence analysis visually and mathematically. Later, the longitudinal modeling of sequences is performed using a clustering method to capture the possible trajectories of progression of states. Thus, the name of the method is “from variables to states”, “from states to sequences” and “from sequences to trajectories” VaSSTra [5].\nThroughout the chapter, we discuss how to derive states from different variables related to students, how to construct sequences from students’ longitudinal progression of states, and how to identify and study distinct trajectories of sequences that undergo a similar evolution. We also cover some advanced properties of sequences that can help us analyze and compare trajectories. In the next section, we explain the VaSSTra method in detail. Next, we review the existing literature that has used the method. After that, we present a step-by-step tutorial on how to implement the method using a dataset of students’ engagement indicators across a whole program."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-from-variables-to-states-from-states-to-sequences-from-sequences-to-trajectories",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-from-variables-to-states-from-states-to-sequences-from-sequences-to-trajectories",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "2 VaSSTra: From variables to states, from states to sequences, from sequences to trajectories",
    "text": "2 VaSSTra: From variables to states, from states to sequences, from sequences to trajectories\nIn Chapter 10, we went through the basics of sequence analysis in learning analytics [13]. Specifically, we learned how to construct a sequence from a series of ordered student activities in a learning session, which is a very common technique in learning analytics (e.g., [14]). In the sequences we studied, each time point represents a single instantaneous event or action by the students. In this advanced chapter, we take a different approach, where sequences are not built from a series of events but rather from states. Such states represent a certain construct (or cluster of variables) related to students (e.g., engagement, motivation) in a certain period (e.g., a week, a semester, a course). The said states are derived from a series of data variables related to the construct under study over the stipulated time period. Analyzing the sequence of such states over several sequential periods allows us to summarize large amounts of longitudinal information and to study complex phenomena across longer timespans [5]. This approach is known as the VaSSTra method. VaSSTra utilizes a combination of person-based methods (to capture the latent states) along with life events methods to model the longitudinal process. In doing so, VaSSTra effectively leverages the benefits of both families of methods in mapping the patterns of longitudinal temporal dynamics. The method has three main steps that can be summarized as (1) identifying latent States from Variables, (2) modeling states as Sequences, and (3) identifying Trajectories within sequences. The three steps are depicted in Figure 11.1 and described in detail below:\n\n\n\nFigure 1. Summary of the three steps of the VaSSTra method\n\n\n\nStep 1. From variables to states: In the first step of the analysis, we identify the “states” within the data using a method that can capture latent or unobserved patterns from multidimensional data (variables). The said states represent a behavioral pattern, function or a construct that can be inferred from the data. For instance, engagement is a multidimensional construct and is usually captured through several indicators. e.g., students’ frequency and time spent online, course activities, cognitive activities and social interactions. Using an appropriate method, such as person-based clustering in our case, we can derive students’ engagement states for a given activity or course. For instance, the method would classify students who invest significant time, effort and mental work are “engaged.” Similarly, students who are investing low effort and time in studying would be classified as “diseganged.” Such powerful summarization would allow us to use the discretized states in further steps. An important aspect of such states is that they are calculated for a specific timespan. Therefore, in our example we could infer students’ engagement states per activity, per week, per lesson, per course, etc. Sometimes, such time divisions are by design (e.g., lessons or courses), but in other occasions researchers have to establish a time scheme according to the data and research questions (e.g., weeks or days). Computing states for multiple time periods is a necessary step to create time-ordered state sequences and prepare the data for sequence analysis.\nStep 2. From states to sequences: Once we have a state for each student at each time point, we can construct an ordered sequence of such states for each student. For example, if we used the scenario mentioned above about measuring engagement states, a sequence of a single student’s engagement states across a six-lesson course would be like the one below. When we convert the ordered states to sequences, we unlock the potential of sequence analysis and life events methods. We are able to plot the distribution of states at each time point, study the individual pathways, the entropy, the mean time spent at each state, etc. We can also estimate how frequently students switch states, and what is the likelihood they finish their sequence in a “desirable” state (i.e., “engaged”).\n\n\nStep 3. From sequences to trajectories: Our last step is to identify similar trajectories —sequences of states with a similar temporal evolution— using temporal clustering methods (e.g., hidden Markov models or hierarchical clustering). Covariates (i.e., variables that could explain cluster membership) can be added at this stage to help identify why a trajectory has evolved in a certain way. Moreover, sequence analysis can be used to study the different trajectories, and not only the complete cohort. We can compare trajectories according to their sequence properties, or to other variables (e.g., performance)."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#review-of-the-literature",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#review-of-the-literature",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nThe VaSSTra method has been used to study different constructs related to students’ learning (Table 11.1), such as engagement [3, 15, 16], roles in computer-supported collaborative learning (CSCL) [17], and learning strategies [18, 19]. Several algorithms have been operationalized to identify latent states from students’ online data. Some examples are: Agglomerative Hierarchical Clustering (AHC) [19], Latent Class Analysis (LCA) [3, 15, 16], Latent Profile Analysis (LPA) [17], and mixture hidden Markov models (MHMM) [18].\nMoreover, sequences of states have mostly been used to represent each course in a program [3, 15–18], but also smaller time spans, such as each learning session in a single course [18, 19]. Different algorithms have also been used to cluster sequences of states into trajectories including HMM [3, 19], mixture hidden Markov models (MHMM) [15, 17], and AHC [16, 18]. Moreover, besides the basic aspects of sequence analysis discussed in the previous chapter, previous work have explored advanced features of sequence analysis such as survival analysis [3, 16], entropy [3, 17, 18], sequence implication [3, 18], transitions [3, 17, 18], covariates [17], discriminating subsequences [16] or integrative capacity [17]. Other studies have made used of multi-channel sequence analysis [15, 19], which is covered in Chapter 13 [20].\n\n\n\nTable 1. Previous literature in which VaSSTra has been used\n\n\n\n\n\n\n\n\n\n\n\n\n\nRef.\nVariables\nStates\nAlphabet\nActor\nTime unit\nMethod Step 1\nMethod Step 3\nAdvance sequence methods\n\n\n\n\n[3]\nFrequency, time spent and regularity of online activities\nEngage- ment\nActive, Average, Disengaged, Withdraw\nStudy program\nCourse\nLCA\nHMM\nEntropy, sequence implication, survival, transitions\n\n\n[15]\nFrequency, time spent and regularity of online activities. Grades.\nEngage- ment, achievement\nActive, Average, Disengages.\nAchiever, Intermediate, Low.\nStudy program\nCourse\nLCA\nMHMM\nMulti-channel sequence analysis\n\n\n[16]\nFrequency, time spent and regularity of online activities\nEngage- ment\nActively engaged, Averagely engaged, Disengaged, Dropout\nStudy program\nCourse\nLCA\nAHC\nSurvival analysis, discriminating subsequences, mean time\n\n\n[17]\nCentrality measures\nCSCL roles\nInfluencer, Mediator, Isolate\nStudy program\nCourse\nLPA\nMHMM\nCovariate analysis, spell duration, transitions, entropy, integrative capacity\n\n\n[18]\nOnline activities\nLearning strategies\nDiverse, Forum read, Forum interact, Lecture read.\nIntense diverse, Light diverse, Light interactive, Moderate interactive.\nCourse. Study program.\nLearning session. Course.\nMHMM\nAHC\nEntropy, implication, transitions\n\n\n[19]\nOnline activities\nLearning strategies\nVideo-oriented instruction, Slide-oriented instruction, Help-seeking.\nAssignment struggling, Assignment approaching, Assignment succeeding.\nCourse\nLearning session\nAHC\nHMM\nMulti-channel sequence analysis"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-with-r",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-with-r",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "4 VassTra with R",
    "text": "4 VassTra with R\nIn this section we provide a step-by-step tutorial on how to implement VaSSTra with R. To illustrate the method, we will conduct a case study in which we examine students’ engagement throughout all the courses of the first two years of their university studies, using variables derived from their usage of the learning management system.\n\n4.1 The packages\nIn order to conduct our analysis we will need several packages besides the basic rio (for reading and saving data in different extensions), tidyverse (for data wrangling), cluster (for clustering features), and ggplot2 (for plotting). Below is a brief summary of the rest of the packages needed:\n\nBBmisc: A package with miscellaneous helper functions [21]. We will use its normalize function to normalize our data across courses to remove the differences in students’ engagement that are due to different course implementations (e.g., larger number of learning resources).\ntidyLPA: A package for conducting Latent Profile Analysis (LPA) with R [22]. We will use it to cluster students’ variables into distinct clusters or states.\nTraMineR: As we have seen in Chapter 10 about sequence analysis [13], this package helps us construct, analyze and visualize sequences from time-ordered states or events [23].\nseqhandbook: This package complements TraMineR by providing extra analyses and visualizations [24].\nGmisc: A package with miscellaneous functions for descriptive statistics and plots [25]. We will use it to plot transitions between states.\nWeightedCluster: A package for clustering sequences using hierarchical cluster analysis [26]. We will use it to cluster sequences into similar trajectories.\n\nThe code below imports all the packages that we need. You might need to install them beforehand using the install.packages command:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(BBmisc)\nlibrary(tidyLPA)\nlibrary(TraMineR)\nlibrary(seqhandbook)\nlibrary(Gmisc)\nlibrary(WeightedCluster)\n\n\n\n4.2 The dataset\nFor our analysis, we will use a dataset of students’ engagement indicators throughout eight courses, corresponding to the first two years of a blended higher education program. The dataset is described in detail in the data chapter. The indicators or variables are calculated from students’ log data in the learning management system, and include the frequency (i.e., number of times) with which certain actions have been performed (e.g., view the course lectures, read forum posts), the time spent in the learning management system, the number of sessions, the number of active days, and the regularity (i.e., consistency and investment in learning). These variables represent students’ bahavioral engagement indicators. The variables are described in detail in Chapter 2 about the datasets of the book [27] and Chapter 7 about predictive learning analytics [28] and in previous works [3]. Below we use rio’s import function to read the data.\n\nLongitudinalData <- \n  import(\"https://github.com/lamethods/data/raw/main/9_longitudinalEngagement/LongitudinalEngagement.csv\")\nLongitudinalData\n\n\n\n  \n\n\n\n\n\n4.3 From variables to states\nThe first step in our analysis is to detect latent states from the multiple engagement-related variables in our dataset (e.g., frequency of course views, frequency of forum posts, etc.). For this purpose, we will use LPA, a person-based clustering method, to identify each student’s engagement state at each course. We first need to standardize the variables, to account for the possible differences in course implementations (e.g., each course has a different number of learning materials and slightly different duration). This way, the mean value of each indicator will be always 0 regardless of the course. Any value above the mean will be always positive, and any value below will be negative. As such the engagement is measures on the same scale. To standardize the data we first group it by CourseID using tidyverse’s group_by and then we apply the normalize function from the BBmisc package to all the columns that contain the engagement indicators using mutate_at and specifying the range of columns. If we inspect the data now, we will see that all variables are centered around 0.\n\nLongitudinalData |> group_by(CourseID) |>  \n  mutate_at(vars(Freq_Course_View:Active_Days), \n            function(x) normalize(x, method = \"standardize\")) |> \n  ungroup() -> df\ndf\n\n\n\n  \n\n\n\nNow, we need to subset our dataset and choose only the variables that we need for clustering. That is, we exclude the metadata about the user and course, and keep only the variables that we believe are relevant to represent the engagement construct.\n\nto_cluster <- dplyr::select(df, Freq_Course_View, Freq_Forum_Consume, \n                                Freq_Forum_Contribute, Freq_Lecture_View, \n                                Regularity_Course_View, Session_Count, \n                                Total_Duration, Active_Days)\n\nBefore we go on, we must choose a seed so we can obtain the same results every time we run the clustering algorithm. We can now finally use tidyLPA to cluster our data. We try from 1 to 10 clusters and different models that enforce different constraints on the data. For example, model 3 takes equal variances and equal covariances, whereas model 6 takes varying variances and varying covariances. You can find out more about this in the tidyLPA documentation [22]. Be aware that running this step may take a while. For more details about LPA, consult the model-based clustering chapter.\n\nset.seed(22294)\nMclustt <- to_cluster |> ungroup() |> \n  single_imputation() |> \n  estimate_profiles(1:10, models = c(1, 2, 3, 6)) \n\n\n\n\nOnce all the possible cluster models and numbers have been calculated, we can calculate several statistics that will help us choose which is the right model and number of clusters for our data. For this purpose, we use the compare_solutions function from tidyLPA and we use the results of calling this function to plot the BIC and the entropy of each model for the range of cluster numbers that we have tried (1-10). In the model-based clustering chapter you can find out more details about how to choose the best cluster solution.\n\ncluster_statistics <- Mclustt |>  \n  compare_solutions(statistics = c(\"AIC\", \"BIC\",\"Entropy\"))\nfits <- cluster_statistics$fits |> mutate(Model = factor(Model))\n\nfits |>\n  ggplot(aes(x = Classes,y = Entropy, group = Model, color = Model)) + \n  geom_line() +\n  scale_x_continuous(breaks = 0:10) +\n   geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nfits |>\n  ggplot(aes(x = Classes,y = BIC, group = Model, color = Model)) + \n  geom_line() +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 2. Choosing the number of clusters by plotting statistics\n\n\n\n\nAlthough, based on the BIC values, Model 6 with 3 classes would be the best fit, the entropy for this model is quite low. Instead, Models 1 and 2 have a higher overall entropy and quite a large fall in BIC when increasing from 2 to 3 classes. Taken together, we choose Model 1 with 3 classes which shows better separation of clusters (high entropy) and a large drop in BIC value (elbow). We add the cluster assignment back to the data so we can compare the different variables between clusters and use the cluster assignment for the next steps. Now, for each student’s course enrollment, we have assigned a state (i.e., cluster) that represents the student’s engagement during that particular course.\n\ndf$State <- Mclustt$model_1_class_3$model$classification\n\nWe can plot the mean variable values for each of the three clusters to understand what each of them represents:\n\ndf |> pivot_longer(Freq_Course_View:Active_Days) |> \n  mutate(State = factor(State)) |>\n  filter(name %in% names(to_cluster)) |>\n  mutate(name = gsub(\"_\", \" \", name)) |>\n  group_by(name, State) |>\n  summarize_if(is.double, mean) -> long_mean\n\nlong_mean |>\n  ggplot(aes(fill = name, y = value, x = State)) +\n  geom_col(position = \"dodge\") + \n  scale_fill_brewer(\"\", type = \"qual\", palette=8) +\n  theme_minimal() +\n  ylab (\"Mean value\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 3. Mean value of each variable for each cluster\n\n\n\n\nWe clearly see that the first cluster represents students with low mean levels of all engagement indicators; the second cluster represents students with average values, and the third cluster with high values. We can convert the State column of our dataset to a factor to give the clusters an appropriate descriptive label:\n\nengagement_levels = c(\"Disengaged\", \"Average\", \"Active\")\ndf_named <- df |> \n  mutate(State = factor(State, levels = 1:3, labels = engagement_levels)) \n\n\n\n4.4 From states to sequences\nIn the previous step, we turned a large amount of variables representing student engagement in a given course into a single state: Disengaged, Average, or Active. Each student has eight engagement states: one per each course (time point in our data) in the first two years of the program. Since the dataset includes the order of each course for each student, we can construct a sequence of the engagement states throughout all courses for each student. To do that we first need to transform our data into a wide format, in which each row represents a single student, and each column represents the student’s engagement state at a given course:\n\nclus_seq_df <- df_named |> arrange(UserID, Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"State\")\n\nNow we can use TraMineR to construct the sequence and assign colors to represent each of the engagement states:\n\ncolors <- c(\"#28a41f\", \"#FFDF3C\", \"#e01a4f\")\nclus_seq <- seqdef(clus_seq_df , 2:9, \n                   alphabet = sort(engagement_levels), cpal = colors)\n\nWe can use the sequence distribution plot from TraMineR to visualize the distribution of each state at each time point (Figure 11.4). We see that the distribution of states is almost constant throughout the eight courses. The ‘Average’ state takes the largest share, followed by the ‘Engaged’ state, and the ‘Disengaged’ state is consistently the least common. For more hints on how to interpret the sequence distribution plot, refer to Chapter 10 [13].\n\nseqdplot(clus_seq, border = NA, use.layout = TRUE, \n         with.legend = T, ncol = 3, legend.prop = 0.2)\n\n\n\n\nFigure 4. Sequence distribution plot of the course states\n\n\n\n\nWe can also visualize each of the individual students’ sequences of engagement states using a sequence index plot (Figure 11.5). In this type of visualization, each horizontal bar represents a single student, and each of the eight colored blocks along the bar represents the students’ engagement states. We can order the students’ sequences according to their similarity for a better understanding. To do this, we calculate the substitution cost matrix (seqsubm) and the distance between the sequences according to this cost (seqdist). Then we use an Agglomerative Nesting Hierarchical Clustering algorithm (agnes) to group sequences together according to their similarity (see Chapter 10 [13]). We may now use the seq_heatmap function of seqhandbook to plot the sequences ordered by their similarity. From this plot, we already sense the existence of students that are mostly active, students that are mostly disengaged, and students that are in-between, i.e., mostly average.\n\nsm <- seqsubm(clus_seq, method = \"CONSTANT\")\nmvad.lcss <- seqdist(clus_seq, method = \"LCS\", sm = sm, with.missing = T)\nclusterward2 <- agnes(mvad.lcss, diss = TRUE, method = \"ward\")\nseq_heatmap(clus_seq, clusterward2)\n\n\n\n\nFigure 5. Sequence index plot of the course states ordered by sequence distance\n\n\n\n\n\n\n4.5 From sequences to trajectories\nIn the previous step we constructed a sequence of each student’s engagement states throughout eight courses. By plotting these sequences, we observed that there might be distinct trajectories of students that undergo a similar evolution of engagement. In this last step, we use hierarchical clustering to cluster the sequences of engagement states into distinct trajectories of similar engagement patterns. To perform hierarchical clustering we first need to calculate the distance between all the sequences. As we have seen in the Sequence Analysis chapter, there are several algorithms to calculate the distance. We choose LCS (Longest Common Subsequence), implemented in the TraMineR package which calculates the distance based on the longest common subsequence.\n\ndissimLCS <- seqdist(clus_seq, method = \"LCS\")\n\nNow we can perform the hierarchical clustering. For this purpose, we use the hclust function of the stats package\n\nClustered <- hclust(as.dist(dissimLCS), method = \"ward.D2\")\n\nWe create partitions for clusters ranging from 2 to 10 and we plot the cluster statistics to be able to select the most suitable cluster number.\n\nClustered_range <- as.clustrange(Clustered, diss = dissimLCS, ncluster = 10)\nplot(Clustered_range, stat = \"all\", norm = \"zscoremed\", lwd = 2)\n\n\n\n\nFigure 6. Cluster statistics for the hierarchical clustering\n\n\n\n\nThere seems to be a maximum for most statistics at three clusters, so we save the cluster assignment for three clusters in a variable named grouping.\n\ngrouping <- Clustered_range$clustering$cluster3\n\nNow we can use the variable grouping to plot the sequences for each trajectory using the sequence index plot (Figure 11.7):\n\nseqIplot(clus_seq, group = grouping, sortv = \"from.start\")\n\n\n\n\nFigure 7. Sequence index plot of the course states per trajectory\n\n\n\n\nIn Figure 11.7, we see that the first trajectory corresponds to mostly average students, the second one to mostly active students, and the last one to mostly disengaged students. We can rename the clusters accordingly.\n\ntrajectories_names = c(\"Mostly average\", \"Mostly active\", \"Mostly disengaged\")\ntrajectories = trajectories_names[grouping]\n\nWe can plot the sequence distribution plot to see the overall distribution of the sequences for each trajectory (Figure 11.8).\n\nseqdplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 8. Sequence distribution plot of the course states per trajectory\n\n\n\n\n\n\n4.6 Studying trajectories\nThere are many aspects that we can study about our trajectories. For example, we can use the mean time plot to compare the time spent in each engagement state for each trajectory. This plot summarizes the time distribution plot across all time points (Figure 11.9). As expected, we see that the mostly active students spend most of their time in an ‘Active’ state, the mostly average students in an Average state, and the mostly disengaged students in a ‘Disengaged’ state, although they spend quite some time in an ‘Average’ state as well.\n\nseqmtplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 9. Mean time plot of the course states per trajectory\n\n\n\n\nAnother very useful plot is the sequence frequency plot (Figure 11.10), that shows the most common sequences in each trajectory and the percentage of all the sequences that they represent. We see that, for each trajectory, the sequence that has all equal engagement states is the most common. The mostly active has, of course, sequences dominated by ‘Active’ states, with sparse average states, and one ‘Disengaged’ state. The mostly disengaged shows a similar pattern dominated by disengaged states with some average and one active state. The mostly average, although it is dominated by ‘Average’ states, shows diversity of shifts to active or disengaged.\n\nseqfplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 10. The 10 most frequent sequences in each trajectory\n\n\n\n\nTo measure the stability of engagement states for each trajectory at each time point, we can use the between-study entropy. Entropy is lowest when all students have the same engagement state at the same time point and highest when the heterogeneity is maximum. We can see that the “Mostly active” and “Mostly disengaged” trajectories have a slightly lower entropy compared to the “Mostly average” one, which is a sign that the students in this trajectory are the least stable (Figure 11.11).\n\nseqHtplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 11. Transversal entropy plot of each trajectory\n\n\n\n\nAnother interesting aspect to look into is the difference in the most common subsequences among trajectories (Figure 11.12). We first search the most frequent subsequences overall and then compare them among the three trajectories. Interestingly enough, the most frequent subsequence is remaining ‘Active’, and remaining ‘Disengaged’ is number five. Remaining average is not among the top 10 most common subsequences, but rather the subsequences containing the ‘Average’ state always include transitions to other states.\n\nmvad.seqe <- seqecreate(clus_seq)\nfsubseq <- seqefsub(mvad.seqe, pmin.support = 0.05, max.k = 2)\ndiscr <- seqecmpgroup(fsubseq, group = trajectories, method = \"chisq\")\nplot(discr[1:10])\n\n\n\n\nFigure 12. Most discriminating subsequences per trajectory\n\n\n\n\nThere are other sequence properties that we may need compare among the trajectories. The function seqindic calculates sequence properties for each individual sequence (Table 11.2). Some of these indices need additional information about the sequences, namely, which are the positive and the negative states. In our case, we might consider the Active state to be positive and the Disengaged state to be negative. Below we discuss some of the most relevant measures:\n\nTrans: Number of transitions. It represents the number of times there has been a change of state. If a sequence maintains the same state throughout its whole length, the value of Trans would be zero; if there were two shifts of state, the value would be 2.\nEntr: Longitudinal entropy or within-student entropy is a measure of the diversity of the sequence states. In contrast with the transversal or between-students entropy that we saw earlier (Figure 11.11), which is calculated per time point, longitudinal entropy is calculated per sequence (i.e., which represents a student’s engagement through the courses in a program in this case). Longitudinal entropy is calculated using Shannon’s entropy formula. Sequences that remain in the same state most of the time have a low entropy whereas sequences that shift states continuously with great diversity have a high entropy.\nCplx: The complexity index is a composite measure of a sequence’s complexity based on the number of transitions and the longitudinal entropy. It measures the variety of states within a sequence, as well as the frequency and regularity of transitions between them. In other words, a sequence with a high complexity index is characterized by many different and unpredictable states or events, and frequent transitions between them.\nPrec: Precarity is a measure of the (negative) stability or predictability of a sequence. It measures the proportion of time that a sequence spends in negative or precarious states, as well as the frequency and duration of transitions between positive and negative states. A sequence with a high precarity index is characterized by a high proportion of time spent in negative or precarious states, and frequent transitions between positive and negative states.\nVolat: Objective volatility represents the average between the proportion of states visited and the proportion of transitions (state changes). It is measure of the variability of the states and transitions in a sequence. A sequence with high volatility would be characterized by frequent and abrupt changes in the states or events, while a sequence with low volatility would have more stable and predictable patterns.\nIntegr: Integrative capacity (potential) is the ability to reach a positive state and then stay in a positive state. Sequences with a high integrative capacity not only include positive states but also manage to stay in such positive states.\n\n\nIndices <- seqindic(clus_seq,\n            indic = c(\"trans\",\"entr\",\"cplx\",\"prec\",\"volat\",\"integr\"), \n            ipos.args = list(pos.states = c(\"Active\")), \n            prec.args = list(c(\"Disengaged\")))\n\n\n\n\n\n\n\nTable 2.  Sequence indicators \n  \n    \n    \n      \n      Trans\n      Entr\n      Volat\n      Cplx\n      Integr\n      Prec\n    \n  \n  \n    1\n2.000\n0.343\n0.393\n0.313\n0.000\n0.600\n    2\n2.000\n0.343\n0.393\n0.313\n0.000\n0.600\n    3\n4.000\n0.512\n0.536\n0.541\n0.000\n0.964\n    4\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n    5\n4.000\n0.631\n0.536\n0.600\n0.000\n1.159\n    6..141\n\n\n\n\n\n\n    142\n2.000\n0.343\n0.393\n0.313\n0.917\n0.006\n  \n  \n  \n\n\n\n\n\nWe can compare the distribution of these indices between the different trajectories to study their different properties (Figure 11.13). Below is an example for precarity and integrative capacity. We clearly see how the Mostly disengaged trajectory has the highest value of precarity, whereas the Mostly active students have the highest integrative capacity. Beyond a mere visual representation, we could also conduct statistical tests to compare whether these properties differ significantly from each other among trajectories.\n\nIndices$Trajectory = trajectories\n\nIndices |> ggplot(aes(x = Trajectory, y = Prec, fill = Trajectory)) + \n  geom_boxplot() + scale_fill_manual(values = colors) + \n  theme_minimal() + theme(legend.position = \"none\")\n\nIndices |> ggplot(aes(x = Trajectory, y = Integr, fill = Trajectory)) + \n  geom_boxplot() + scale_fill_manual(values = colors) + \n  theme_minimal() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 13. Comparison of sequence indicators between trajectories\n\n\n\n\nAs we have mentioned, an important aspect of the study of students’ longitudinal evolution is looking at the transitions between states. We can calculate the transitions using seqtrate from TraMineR and plot them using transitionPlot from Gmisc.\n\ntransition_matrix = seqtrate(clus_seq, count = T)  \n\nFrom the transition matrix and Figure 11.14 we can see how most transitions are between one state and the same (no change). The most unstable state is ‘Average’ with frequent transitions both to ‘Active’ and ‘Disengaged’. Both ‘Active’ and ‘Disengaged’ had occasional transitions to ‘Average’ but rarely from one another.\n\ntransition_matrix = seqtrate(clus_seq, count = T)  \ntransitionPlot(transition_matrix, \n                              fill_start_box = colors,\n                              txt_start_clr = \"black\",\n                              cex = 1,\n                              box_txt = rev(engagement_levels))\n\n\n\n\nFigure 14. Transition plot between states"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#discussion",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#discussion",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "5 Discussion",
    "text": "5 Discussion\nIn this chapter, the VaSSTra method is presented as a person-centered approach for the longitudinal analysis of complex behavioral constructs over time. In the step-by-step tutorial, we have analyzed students’ engagement states throughout all the courses in the first two years of program. First, we have clustered all the indicators of student engagement into three engagement states using model-based clustering: active, average and disengaged. This step allowed us to summarize eight continuous numerical variables representing students’ online engagement indicators of each course into a single categorical variable (state). Then, we constructed a sequence of engagement states for each student, allowing us to map the temporal evolution of engagement and make use of sequence analysis methods to visualize and investigate such evolution. Lastly, we have clustered students’ sequences of engagement states into three different trajectories: a mostly active trajectory which is dominated by engaged students who are stable throughout time, a mostly average trajectory with averagely engaged students who often transition to engaged or disengaged states, and a mostly disengaged trajectory with inactive students that fail to catch up and remain disengaged most of the program. As such, VaSSTra offers several advantages over the existing longitudinal methods for clustering (such as growth models or longitudinal k-means) which are limited to a single continuous variable [29–31], instead of taking advantage of multiple variables in the data. Through the summarizing power of visualization, VaSSTra is able to represent complex behaviors captured through several variables using a limited number of states. Moreover, through sequence analysis, we can study how the sequences of such states evolve over time and differ from one another, and whether there are distinct trajectories of evolution.\nSeveral literature reviews of longitudinal studies (e.g., [32]) have highlighted the shortcomings of existing research for using variable-centered methods or ignoring the heterogeneity of students’ behavior. Ignoring the longitudinal heterogeneity means mixing trends of different types, e.g., an increasing trend in a subgroup and a decreasing trend in another subgroup exist. Another limitation of the existing longitudinal clustering methods is that cluster membership can not vary with time so one student is assigned to a single longitudinal cluster, which makes it challenging to study variability and transitions.\nAs we have seen in the literature review section, the VaSSTra method can be adapted to various scenarios beyond engagement, such as collaborative roles, attitudes, achievement, or self-efficacy, and can be used with different time points such as tasks, days, weeks, or school years. The reader should refer to Chapter 8 [33] and Chapter 9 [34] about clustering to learn other clustering techniques that may be more appropriate for transforming different types of variables into states (that is, conducting the first step of VaSSTra). Moreover, in Chapter 10 [13], the basics of sequence analysis are described, including how to cluster sequences into trajectories using different distance measures that might be more appropriate in different situations. The next chapter (Chapter 12) [35] presents Markovian modeling, which constitutes another way of clustering sequences into trajectories according to their state transitions. Lastly, Chapter 13 [20] presents multi-channel sequence analysis, which could be used to extend VaSSTra to study several parallel sequences (of several constructs) at the same time."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html",
    "href": "chapters/ch12-markov/ch12-markov.html",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#introduction",
    "href": "chapters/ch12-markov/ch12-markov.html#introduction",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn the previous two chapters, we have learned about sequence analysis [1, 2] and its relevance to educational research. This chapter delves into a closely-related method: Markovian models. Specifically, we focus on a particular type of Markovian model, where the data are assumed to be categorical and observed at discrete time intervals, as per the previous chapters about sequence analysis, although in general Markovian models are not restricted to categorical data. One of the main differences between sequence analysis and Markovian modelling is that the former relies on deterministic data mining, whereas the latter uses probabilistic models [3]. Moreover, sequence analysis takes a more holistic approach by analysing sequences as a whole, whereas Markovian modelling focuses on the transitions between states, their probability, and the reasons (covariates) which explain why these transitions happen.\nWe provide an introduction and practical guide to the topic of Markovian models for the analysis of sequence data. While we try to avoid advanced mathematical notations, to allow the reader to continue to other, more advanced sources when necessary, we do introduce the basic mathematical concepts of Markovian models. When doing so, we use the same notation as in the R package seqHMM [4], which we also use in the examples. In particular, we illustrate first-order Markov models, hidden Markov models, mixture Markov models, and mixture hidden Markov models with applications to synthetic data on students’ collaboration roles throughout a complete study program.\nThe chapter proceeds to describe the theoretical underpinnings on each method in turn, then showcases each method with code, before presenting some conclusions and further readings. In addition to the aforementioned applications to collaboration roles and achievement sequences, we also provide a demonstration of the utility of Markovian models in another context, namely process mining. In the process mining application, we leverage Markov models and mixture Markov models to explore learning management system logs. Finally, we conclude with a brief discussion of Markovian models in general and provide some recommendations for further reading of advanced topics in this area as a whole."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#methodological-background",
    "href": "chapters/ch12-markov/ch12-markov.html#methodological-background",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "2 Methodological Background",
    "text": "2 Methodological Background\n\n2.1 Markov model\nThe simple first-order Markov chain or Markov model (MM) can be used to model transitions between successive states. In the first-order MM, given the current observation, the next observation in the sequence is independent of the past —this is called the Markov property (the order of MM determines on how many previous observations the next observation depends on). For example, when predicting a student’s school success in the fourth year under a first-order model, we only need to consider their success in the third year, while their success in the first and second year give no additional information for the prediction (see Figure 12.1 for an illustration). As such, the model is said to be memoryless.\n\n\n\n\n\nFigure 1. Illustration of the Markov Model. The nodes \\(Y_1\\) to \\(Y_4\\) refer to states at time points 1 to 4. The arrows indicate dependencies between states.\n\n\n\n\nAs an example, consider the data described in Table 12.1 which includes four sequences of length ten. The alphabet — that is, the list of all possible states appearing in the data — consists of two types of observed state; low achievement success (\\(L\\)) and high achievement (\\(H\\)). Here, the individuals are assumed to be independent from one another:\n\n\nTable 1. Four example sequences of school achievement with individuals A–D across the rows and years 1–10 across the columns.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nA\nL\nL\nL\nH\nL\nH\nL\nH\nH\nH\n\n\nB\nL\nH\nH\nL\nH\nL\nH\nL\nL\nH\n\n\nC\nH\nH\nL\nH\nL\nL\nH\nL\nH\nH\n\n\nD\nH\nH\nL\nL\nL\nH\nL\nL\nL\nH\n\n\n\n\nSay \\(t\\) describes the position in the sequence, or in this example, the year (in other words, here \\(t\\) runs from 1 to 10). If we assume that the probability of observing \\(L\\) or \\(H\\) at any given point \\(t\\) depends on the current observation only, we can estimate the transition probabilities \\(a_{LL}\\) (from state \\(L\\) to state \\(L\\)), \\(a_{LH}\\) (\\(L\\) to \\(H\\)), \\(a_{HL}\\) (\\(H\\) to \\(L\\)), and \\(a_{HH}\\) (\\(H\\) to \\(H\\)) by calculating the number of observed transitions from each state to all states and scaling these with the total number of transitions from that state. Mathematically, we can write the transition probability \\(a_{rs}\\) from state \\(r\\) to state \\(s\\) as\n\\[a_{rs}=P(z_t = s\\,|\\,z_{t-1} = r), \\ s,r \\in \\{L,H\\},\\]\nwhich simply states that the observed state \\(z_t\\) in year \\(t\\) being \\(L\\) or \\(H\\) depends on which of the two states were observed in the previous year \\(t-1\\). For example, to compute \\(a_{LH}=P(z_t=H\\,|\\,z_{t-1}=L)\\), the probability of transitioning from the origin state \\(L\\) to the destination state \\(H\\), we divide the eight observed transitions to state \\(H\\) from state \\(L\\) by 20, which is the total number of transitions from \\(L\\) to any state.\nThe basic MM assumes that the transition probabilities remain constant in time (this property is called time-homogeneity). This means, for example, that the probabilities of transitioning from the low-achievement state to the high-achievement state is the same in the ninth year as it was in the second year. We can collect the transition probabilities in a transition matrix (which we call \\(A\\)) which shows all of the possible transition probabilities between each pair of origin and destination states, as illustrated in Table 12.2. For example, when a student has low achievement in year \\(t\\), they have a 40 percent probability to have low achievement in year \\(t+1\\) and a higher 60 percent probability to transition to high achievement instead, regardless of the year \\(t\\). Notice that the probabilities in each row must add up to 1 (or 100%).\n\n\nTable 2. Transition matrix showing the probabilities of transitioning from one state to another (low or high achievement). The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n8/20 = 0.4\n12/20 = 0.6\n\n\nHigh \\(\\to\\)\n10/16 = 0.625\n6/16 = 0.375\n\n\n\n\nLastly, we need to define probabilities for the starting states of the sequences, i.e., the initial probabilities \\[\\pi_s=P(z_1 = s), \\ s \\in \\{L,H\\}.\\]\nIn the example, half of the students have low achievement and the other half have high achievement in the first year, so \\(\\pi_L=\\pi_H = 0.5\\). This basic MM is very simple and is often not realistic in the context of educational sciences. We can, however, extend the basic MM in several ways.\nFirst of all, we can include covariates to explain the transition and/or initial probabilities. For example, if we think that transitioning from low to high achievement becomes more challenging as the students get older we may add time as an explanatory variable to the model, allowing the probability of transitioning from low to high achievement to decrease in time. We could also increase the order of the Markov chain, accounting for longer histories. This may be more realistic, but at the same time increasing the order makes the model considerably more complex, the more so the longer the history considered.\nSecondly, one of the most useful extensions is the inclusion of hidden (or latent) states that cannot be observed directly but can be estimated from the sequence of observed states. An MM with time-constant hidden states is typically called the mixture Markov model (MMM). It can be used to find latent subpopulations, or in other words, to cluster sequence data. A model with time-varying hidden states is called the hidden Markov model (HMM), which allows the individuals to transition between the hidden states. Allowing for both time-constant and time-varying hidden states leads to a mixture hidden Markov model (MHMM). Unless otherwise specified, from now on when talking about hidden states we refer always to time-varying hidden states, while time-constant hidden states are referred to as clusters.\n\n\n2.2 Mixture Markov model\nConsider a common case in sequence analysis where individual sequences are assumed to be clustered into subpopulations such as those with typically high and low achievement. In the introductory sequence analysis chapter, the clustering of sequences was performed based on a matrix of pairwise dissimilarities between sequences. Alternatively, we can use the MMM to group the sequences based on their initial and transition probabilities, for example, into those who tend to stay in and transition to high achievement states and those that tend to stay in and transition to low achievement states, as illustrated in Table 12.3.\n\n\nTable 3. Two transition matrices showing the probabilities of transitioning from one state of achievement to another in two clusters of Low achievement and High achievement. The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\n(a) Low achievement\n\n\nCluster: Low achievement\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.8\n0.2\n\n\nHigh \\(\\to\\)\n0.4\n0.6\n\n\n\n\n\n\n(b) High achievement\n\n\nCluster: High achievement\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.6\n0.4\n\n\nHigh \\(\\to\\)\n0.1\n0.9\n\n\n\n\n\n\nIn MMMs, we have a separate transition matrix \\(A^k\\) for each cluster \\(k\\) (for \\(k=1,\\ldots,K\\) clusters/subpopulations), and the initial state distribution defines the probabilities to start (and stay) in the hidden states corresponding to a particular cluster. This probabilistic clustering provides group membership probabilities for each sequence; these define how likely it is that each individual is a member of each cluster. We can easily add (time-constant) covariates to the model to explain the probabilities of belonging to each cluster. By incorporating covariates in this way we could, for example, find that being in a high-achievement cluster is predicted by gender or family background. However, we note that this is distinct from the aforementioned potential inclusion of covariates to explain the transition and/or initial probabilities.\nAn advantage of this kind of probabilistic modelling approach is that we can use traditional model selection methods such as likelihood information criteria or cross-validation for choosing the best model. For example, if the number of subpopulations is not known in advance —as is typically the case— we can compare models with different clustering solutions (e.g., those obtained with different numbers of clusters, different subsets of covariates, or different sets of initial probabilities, for example) and choose the best-fitting model with, for example, the Bayesian information criterion (BIC) [5].\n\n\n2.3 Hidden Markov model\nThe HMM can be useful in a number of cases when the state of interest cannot be directly measured or when there is measurement error in the observations. In HMMs, the Markov chain operates at the level of hidden states, which subsequently generate or emit observed states with different probabilities. For example, think about a progression of a student’s ability as a hidden state and school success as the observed state. We cannot measure true ability directly, but we can estimate the student’s progress by their test scores that are emissions of their ability. There is, however, some uncertainty in how well the test scores represent students’ true ability. For example, observing low test scores at some point in time does not necessarily mean the student has low ability; they might have scored lower than expected in the test due to other reasons such as being sick at that particular time. Such uncertainty can be reflected in the emission probabilities; for example, in the high-ability state students get high test scores eight times out of ten and low test scores with a 20 percent probability, while in the low-ability state the students get low test scores nine times out of ten and high test scores with a 10 percent probability. These probabilities are collected in an emission matrix as illustrated in Table 12.4.\n\n\nTable 4. Emission matrix showing the probabilities of each hidden state (low or high ability) emitting each observed state (low or high test scores).\n\n\n\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.9\n0.1\n\n\nHigh ability\n0.2\n0.8\n\n\n\n\nAgain, the full HMM is defined by a set of parameters: the initial state probabilities \\(\\pi_s\\), the hidden state transition probabilities \\(a_{rs}\\), and the emission probabilities of observed states \\(b_s(m)\\). What is different to the MM is that in the HMM, the initial state probabilities \\(\\pi_s\\) define the probabilities of starting from each hidden state. Similarly, the transition probabilities \\(a_{rs}\\) define the probabilities of transitioning from one hidden state to another hidden state. The emission probabilities \\(b_s(m)\\) (collected in an emission matrix \\(B\\)) define the probability of observing a particular state \\(m\\) (e.g., low or high test scores) given the current hidden state \\(s\\) (e.g., low or high ability).\nWhen being in a certain hidden state, observed states occur randomly, following the emission probabilities. Mathematically speaking, instead of assuming the Markov property directly on our observations, we assume that the observations are conditionally independent given the underlying hidden state. We can visualise the HMM as a directed acyclic graph (DAG) illustrated in Figure 12.2. Here \\(Z\\) are the unobserved states (such as ability) which affect the distribution of the observed states \\(Y\\) (test scores). At each time point \\(t\\), the state \\(z_t\\) can obtain one of \\(S\\) possible values (there are two hidden states in the example of low and high ability, so \\(S=2\\)), which in turn defines how \\(Y_t\\) is distributed.\n\n\n\nFigure 2. Illustration of the HMM. The nodes \\(Z_1\\) to \\(Z_4\\) refer to hidden states at time points 1 to 4, while the nodes \\(Y_1\\) to \\(Y_4\\) refer to observed states. The arrows indicate dependencies between hidden and/or observed states.\n\n\n\n\n2.4 Mixture hidden Markov models\nCombining the ideas of both time-constant clusters and time-varying hidden states leads to the concept of mixture hidden Markov model (MHMM). Here we assume that the population of interest consists of a finite number of subpopulations, each with their own HMM with varying transition and emission probabilities. For example, we could expect to find underlying groups which behave differently when estimating the progression of ability through the sequence of test scores, such as those that consistently stay on a low-ability or high-ability track (stayers) and those that move between low and high ability (movers). In this case, we need two transition matrices: the stayers’ transition matrix allows for no transitions while the movers’ transition matrix allows for transitioning between low and high ability, as illustrated in Table 12.5.\n\n\nTable 5. Two transition matrices showing the probabilities of transitioning from one state of ability to another in two clusters, the Stayers and the Movers. The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\n(a) Stayers\n\n\nCluster: Stayers\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n1\n0\n\n\nHigh \\(\\to\\)\n0\n1\n\n\n\n\n\n\n(b) Movers\n\n\nCluster: Movers\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.6\n0.4\n\n\nHigh \\(\\to\\)\n0.3\n0.7\n\n\n\n\n\n\nSimilarly, we need two emission matrices that describe how the observed states are related to hidden states, as illustrated in Table 12.6. In this example, there is a closer match between low/high ability and low/high test scores in the Stayers cluster in comparison to the Movers cluster.\n\n\nTable 6. Two emission matrices showing the probabilities of each hidden state (low or high ability) emitting each observed state (low or high test scores).\n\n\n\n\n(a) Stayers\n\n\nCluster: Stayers\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.9\n0.1\n\n\nHigh ability\n0.1\n0.9\n\n\n\n\n\n\n(b) Movers\n\n\nCluster: Movers\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.7\n0.3\n\n\nHigh ability\n0.2\n0.8\n\n\n\n\n\n\nMathematically, when estimating a MHMM we first fix the number of clusters \\(K\\), and create a joint HMM consisting of \\(K\\) submodels (HMMs). The number of hidden states does not have to be fixed but can vary by submodel, so that the HMMs have more hidden states for some clusters and fewer for others (in our example, because the transition matrix is of the Stayers cluster is diagonal, we could also split the cluster into two single state clusters, one corresponding to low and other to high ability). This can increase the burden of model selection, so often a common number of hidden states is assumed for each cluster for simplicity. In any case, the initial state probabilities of this joint model define how sequences are assigned to different clusters. We estimate this joint model using the whole data and calculate cluster membership probabilities for each individual. The idea of using mixtures of HMMs has appeared in literature under various names with slight variations, e.g., [6], [7], and [4]. Notably, MHMMs inherit from MMMs the ability to incorporate covariates to predict cluster memberships.\n\n\n2.5 Multi-channel sequences\nThere are two options to analyse multi-channel (or multi-domain or multi-dimensional) sequence data with Markovian models. The first option is to combine observed states in different channels into one set of single-channel sequences with an expanded alphabet. This option is simple, and works for MMs, HMMs, MMMs, and MHMMs, but can easily lead to complex models as the number of states and channels increases considerably. The second option, which can only be used when working with HMMs and MHMMs, is to treat the observed states in each channel independently given the current hidden state. This can be easily performed by defining multiple emission probability matrices, one for each channel. The assumption of conditional independence simplifies the model, but is sometimes unrealistic, in which case it is better to resort to the first option and convert the data into single-channel sequences. Both options are discussed further in Chapter 13 [8], a dedicated chapter on multi-channel sequences, where applications of distance-based and Markovian clustering approaches are presented. In this chapter, we henceforth focus on single-channel sequences.\n\n\n2.6 Estimating model parameters\nThe model parameters, i.e. the elements of the initial probability vectors \\(\\pi\\), transition probability matrices \\(A\\), and emission probability matrices \\(B\\), can be estimated from data using various methods. Typical choices are the Baum-Welch algorithm (an instance of the expectation-maximisation, i.e., the EM algorithm) and direct (numerical) maximum likelihood estimation. It is possible to restrict models, for example, by setting some parameters to fixed values (typically zeros), for example, to make certain starting states, transitions, or emissions impossible.\nAfter the parameter estimation, in addition to studying the estimated model parameters upon convergence, we can, for example, compute cluster-membership probabilities for each individual and find the most probable paths of hidden state sequences using the Viterbi algorithm ([9]). These can be further analysed and visualised for interpretation."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#review-of-the-literature",
    "href": "chapters/ch12-markov/ch12-markov.html#review-of-the-literature",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nMarkovian methods have been used across several domains in education and have gained renewed interest with the surge in learning analytics and educational data mining. Furthermore, the introduction of specialised R packages (e.g., seqHMM [10]) and software applications (e.g., Mplus [11, 12]) have made it easier to implement Markov models. One of the most common applications of Markovian methods is the clustering of sequence data [13–15]. Markov models offer a credible alternative to existing distance-based methods (e.g. optimal matching) and can be used with different sequence types (e.g. multi-channel sequences). Furthermore, Markovian methods offer some advantages in clustering sequential data such as the inclusion of covariates that can explain why a sequence emerged (e.g., [16]). More importantly, Markovian models are relatively scalable and can be used to cluster large sequences [17]. As Saqr et al. [17] noted, large sequences are hard to cluster using standard methods such as hierarchical clustering, which is memory inefficient, and hard to parallelise or scale [18, 19]. Furthermore, distance-based clustering methods are limited by the theoretical maximum dimension of a matrix in R which is 2,147,483,647 (i.e., a maximum of 46,430 sequences). In such a case, Markovian methods may be the solution.\nExamples of Markovian methods in clustering sequences are plentiful. For example, HMMs have been used to cluster students’ sequences of learning management system (LMS) trace data to detect their patterns of activities or what the authors referred to as learning tactics and strategies [15]. Another close example was that of López-Pernas and Saqr [20], who used HMMs to cluster multi-channel data of students’ learning strategies of two different tools (an LMS and an automated assessment tool). Other examples include using HMM in clustering sequences of students’ engagement states [21], sequences of students’ collaborative roles [16], or sequences of self-regulation [13, 14].\nMarkovian methods are also popular in studying transitions and have therefore been used across several applications and with different types of data. One of the most common usages is what is known as stochastic processes mining which typically uses first-order Markov models to map students’ transitions between learning activities. For example, Matcha et al. [22] used first-order Markov models to study students’ processes of transitions between different learning tactics. Other uses include studying the transitions between tactics of academic writing [23], between self-regulated learning events [24], or within collaborative learning settings [25]. Yet, most of such work has been performed by the pMiner R package [26], which was recently removed from The Comprehensive R Archive Network (CRAN) due to slow updates and incompatibility with existing guidelines. This chapter offers a modern alternative that uses modern and flexible methods for fitting, plotting, and clustering stochastic process mining models as well as the possibility to add covariates to understand “why” different transitions pattern emerged.\nIndeed, transition analysis in general has been a popular usage for Markovian models and has been used across several studies. For instance, for the analysis of temporal patterns of students’ activities in online learning (e.g., [27]), or transitions between latent states [28], or transitions between assignment submission patterns [29]."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#examples",
    "href": "chapters/ch12-markov/ch12-markov.html#examples",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "4 Examples",
    "text": "4 Examples\nAs a first step, we will import all the packages required for our analyses. We have used most of them throughout the book. Below is a brief summary:\n\nqgraph: A package for visualising networks, which can be used to plot transition probabilities [30]. This is used only for the process mining application in Section \\(\\ref{process}\\).\nrio: A package for reading and saving data files with different extensions [31].\nseqHMM: A package designed for fitting hidden (latent) Markov models and mixture hidden Markov models for social sequence data and other categorical time series [32].\ntidyverse: A package that encompasses several basic packages for data manipulation and wrangling [33].\nTraMineR: As seen in the introductory sequence analysis chapter, this package helps us construct, analyze, and visualise sequences from time-ordered states or events [34].\n\n\nlibrary(qgraph)\nlibrary(rio)\nlibrary(seqHMM)\nlibrary(tidyverse)\nlibrary(TraMineR)\n\nHenceforth, we divide our examples into two parts: the first largely focuses on traditional uses of the seqHMM package to fit Markovian models of varying complexity to sequence data; the latter presents a demonstration of Markovian models from the perspective of process mining. We outline the steps involved in using seqHMM in general in Section \\(\\ref{steps}\\), demonstrate the application of MMs, HMMs, MMMs, and MHMMs in Section \\(\\ref{markov}\\), and explore process mining using Markovian models in Section \\(\\ref{process}\\), leveraging much of the steps and code from the previous two sections. We note that different datasets are used in Section \\(\\ref{markov}\\) and Section \\(\\ref{process}\\); we begin by importing the data required for Section \\(\\ref{markov}\\) and defer the importing of the data used in the process mining application to the later section.\nWith this in mind, we start by using the ìmport() function from the rio package to import our sequence data. Based on the description of the MHMM in [35], we used the seqHMM package to simulate a synthetic dataset (simulated_data) consisting of students’ collaboration roles (obtained from [36]) on different courses across a whole program. As the original data, the simulation was based on the two-channel model (collaboration and achievement), but we only use the collaboration sequences in the following examples, and leave the multi-channel sequence analysis to Chapter 13 [8]. While not available in the original study, we also simulated students’ high school grade point average (GPA, for simplicity categorised into three levels) for each student, which will be used to predict cluster memberships. Using this data, we show how the seqHMM package can be used to analyse such sequences. We start with the simple MM, and then transition to HMMs and their mixtures. To be able to use the seqHMM functions, we need to convert the imported data to a sequence using the function seqdef() from the TraMineR package (see Chapter 10 [1] for more information about creating stslist objects). We subsequently assign a colour palette to each state in the alphabet for later visualisations using the function cpal(). Finally, we can also extract the covariate information separately (cov_data).\n\nURL <- \"https://github.com/sonsoleslp/labook-data/raw/main/\"\nsimulated_data <- import(paste0(URL, \"12_longitudinalRoles/simulated_roles.csv\"))\n\nroles_seq <- seqdef(\n  simulated_data, \n  var = 3:22, \n  alphabet = c(\"Isolate\", \"Mediator\", \"Leader\"),\n  cnames = 1:20\n)\n\n [>] 3 distinct states appear in the data: \n\n\n     1 = Isolate\n\n\n     2 = Leader\n\n\n     3 = Mediator\n\n\n [>] state coding:\n\n\n       [alphabet]  [label]  [long label] \n\n\n     1  Isolate     Isolate  Isolate\n\n\n     2  Mediator    Mediator Mediator\n\n\n     3  Leader      Leader   Leader\n\n\n [>] 200 sequences in the data set\n\n\n [>] min/max sequence length: 20/20\n\ncpal(roles_seq) <- c(\"#FBCE4B\", \"#F67067\", \"#5C2262\")\n\ncov_data <- simulated_data |>\n  select(ID, GPA) |>\n  mutate(GPA = factor(GPA, levels = c(\"Low\", \"Middle\", \"High\")))\n\n\n4.1 Steps of estimation\nWe will first briefly introduce the steps of the analysis with the seqHMM package and then show examples of estimating MMs, HMMs, MMMs, and MHMMs.\n\n4.1.1 Defining the model structure\nFirst, we need to create the model object which defines the structure of the model. This can be done by using one of the model building functions of seqHMM. The build functions include build_mm() for constructing the simple MM, build_hmm() for the HMM, build_mmm() for the MMM, and build_mhmm() for the MHMM. The user needs to give the build function the sequence data and the number of hidden states and/or clusters (when relevant). The user can also set restrictions on the models, for example, to forbid some transitions by setting the corresponding transition probabilities to zero. To facilitate the estimation of the parameters of more complex models, the user may also set informative starting values for model parameters.\n\n\n4.1.2 Estimating the model parameters\nAfter defining the model structure, model parameters need to be estimated. The fit_model() function estimates model parameters using maximum likelihood estimation. The function has several arguments for configuring the estimation algorithms. For simple models the default arguments tend to work well enough, but for more complex models the user should adjust the algorithms. This is because the more parameters the algorithm needs to estimate, the higher the risk of not finding the model with the optimal parameter values (the one which maximises the likelihood).\nIn order to reduce the risk of being trapped in a local optimum of the likelihood surface (instead of a global optimum), we advise to estimate the model numerous times using different starting values for the parameters. The seqHMM package strives to automate this. One option is to run the EM algorithm multiple times with random starting values for any or all of initial, transition, and emission probabilities. These are specified in the control_em argument. Although not done by default, this method seems to perform very well as the EM algorithm is relatively fast. Another option is to use a global direct numerical estimation method such as the multilevel single-linkage method. See [4] for more detailed information on model estimation.\n\n\n4.1.3 Examining the results\nThe output of the fit_model contains the estimated model (stored in fit_hmm$model) as well as information about the estimation of the model, such as the log-likelihood of the final model (fit_hmm$logLik). The print method provides information about the estimated model in a written format, while the plot() function visualises the model parameters as a graph. For HMMs and MHMMs, we can calculate the most probable sequence of hidden states for each individual with the hidden_paths() function. Sequences of observed and hidden state sequences can be plotted with the ssplot() function for MMs and HMMs and with the mssplot() function for the MMMs and the MHMMs. For MMMs and MHMMs, the summary() method automatically computes some features of the models such as standard errors for covariates and prior and posterior cluster membership probabilities for the subjects.\n\n\n\n4.2 Markov models\nWe now follow the steps outlined above for each model in turn, starting from the most basic Markov model, proceeding through a hidden Markov model and a mixture Markov model, and finally concluding with a mixture hidden Markov model.\n\n4.2.1 Markov model\nWe focus on the sequences of collaboration roles, collected in the roles_seq object. The build_mm() function only takes one argument, observations, which should be an stslist object created with the seqdef() function from the TraMineR package as mentioned before. We can build a MM as follows:\n\nmarkov_model <- build_mm(roles_seq)\n\nFor the MM, the build_mm() function estimates the initial probabilities and the transition matrix. Note that the build_mm() function is the only build function that automatically estimates the parameters of the model. This is possible because for the MM the estimation is a simple calculation while for the other types of models the estimation process is more complex. The user can access the estimated parameters by calling markov_model$initial_probs and markov_model$transition_probs or view them by using the print method of the model:\n\nprint(markov_model)\n\nInitial probabilities :\n Isolate Mediator   Leader \n   0.375    0.355    0.270 \n\nTransition probabilities :\n          to\nfrom       Isolate Mediator Leader\n  Isolate   0.4231    0.478 0.0987\n  Mediator  0.1900    0.563 0.2467\n  Leader    0.0469    0.428 0.5252\n\n\nWe can see that the initial state probabilities are relatively uniform, with a slightly lower probability for starting in the Leader state. In terms of the transition probabilities, the most distinct feature is that that it is rare to transition directly from the Leader state to Isolate and vice versa (estimated probabilities are about 5% and 10%, respectively). It is also more common to drop from Leader to Mediator (43%) than to increase collaboration from Mediator to Leader (25%). Similarly, the probability of moving from Mediator to Isolate is only 19 percent, but there is a 48 percent chance of transitioning from Isolate to Mediator.\nWe can also draw a graph of the estimated model using the plot method which by default shows the states as pie graphs (for the MM, the pie graphs only consist of one state), transition probabilities as directed arrows, and initial probabilities below each state (see Figure 12.3).\n\nplot(markov_model, \n  legend.prop = 0.2, ncol.legend = 3, \n  edge.label.color = \"black\", vertex.label.color = \"black\")\n\n\n\n\nFigure 3. Estimated Markov model as a pie charts with the transition probabilities shown as labelled edges.\n\n\n\n\n\n\n4.2.2 Hidden Markov models\nThe structure of an HMM is set with the build_hmm() function. In contrast to build_mm(), other build_*() functions such as build_hmm() do not directly estimate the model parameters. For build_hmm(), in addition to observations (an stslist), we need to provide the n_states argument which tells the model how many hidden states to construct. Using again the collaboration roles sequences, if we want to estimate an HMM with two hidden states, we can write:\n\nset.seed(1)\nhidden_markov_model <- build_hmm(observations = roles_seq, n_states = 2)\n\nThe set.seed call ensures that we will always end up with the same exact initial model with hidden states in the same exact order even though we use random values for the initial parameters of the model (which is practical for reproducibility). We are now ready to estimate the model with the fit_model() function. The HMM we want to estimate is simple, so we rely on the default values and again use the print method to provide information about the estimated model:\n\nfit_hmm <- fit_model(hidden_markov_model)\nfit_hmm$model\n\nInitial probabilities :\nState 1 State 2 \n  0.657   0.343 \n\nTransition probabilities :\n         to\nfrom      State 1 State 2\n  State 1  0.9089  0.0911\n  State 2  0.0391  0.9609\n\nEmission probabilities :\n           symbol_names\nstate_names Isolate Mediator Leader\n    State 1  0.4418    0.525 0.0336\n    State 2  0.0242    0.478 0.4980\n\n\nThe estimated initial state probabilities show that it is more probable to start from hidden state 1 than from hidden state 2 (66% vs. 34%). The high transition probabilities on the diagonal of the transition matrix indicate that the students typically tend to stay in the hidden state they currently are in. Transition probabilities between the hidden states are relatively low and also asymmetric: it is more likely that students move from state 1 to state 2 than from state 2 to state 1. Looking at the emission matrices, we see that the role of the students in state 2 is mostly Leader or Mediator (emission probabilities are 50% and 48%). On the other hand, state 1 captures more of those occasions where students are isolated or exhibit at most a moderate level of participation (mediators). We can also visualise this with the plot() method of seqHMM (see Figure 12.4):\n\nplot(fit_hmm$model, \n  ncol.legend = 4, legend.prop = 0.2, \n  edge.label.color = \"black\", vertex.label.color = \"black\"\n)\n\n\n\n\nFigure 4. HMM with two hidden states (pie charts), with transitions between hidden states shown as labelled edges.\n\n\n\n\nThe plot values mainly shows the same information. By default, to simplify the graph, the plotting method combines all states with less than 5% emission probabilities into one category. This threshold can be changed with the combine.slices argument (setting combine.slices = 0 plots all states).\nFor simple models, using n_states is sufficient. It automatically draws random starting values that are then used for the estimation of model parameters. However, as parameter estimation of HMMs and mixture models can be sensitive to starting values of parameters, it may be beneficial to provide starting values manually using the initial_probs, transition_probs, and emission_probs arguments. This is also necessary in case we want to define structural zeros for some of these components, e.g., if we want to restrict the initial probabilities so that each sequence starts from the same hidden state, or if we want to set an upper diagonal transition matrix, which means that the model does not allow transitioning back to previous states (this is called a left-to-right model) [4]. It is also possible to mix random and user-defined starting values by using simulate_*() functions (e.g. simulate_transition_probs()) for some of the model components and user-defined values for others.\nIn the following example we demonstrate estimating a three-state HMM with user-defined starting values for the initial state probabilities and the transition matrix but simulate starting values for the emission matrices. For simulating starting values with simulate_emission_probs, we need to define the number of hidden states, and the number of observed symbols, i.e., the length of the alphabet of the sequences.\n\n## Set seed for randomisation\nset.seed(1)\n\n## Initial state probability vector, must sum to one\ninit_probs <- c(0.3, 0.4, 0.3)\n\n## a 3x3 transition matrix, each row should sum to one\ntrans_probs <- rbind(c(0.8, 0.15, 0.05), c(0.2, 0.6, 0.2), c(0.05, 0.15, 0.8))\n\n## Simulate emission probabilities\nemission_probs <- simulate_emission_probs(\n  n_states = 3, n_symbols = length(alphabet(roles_seq))\n)\n\n## Build the HMM\nhidden_markov_model_2 <- build_hmm(\n  roles_seq, initial_probs = init_probs, transition_probs = trans_probs,\n  emission_probs = emission_probs\n)\n\nOur initial probabilities suggest that it is slightly more likely to start from the second hidden state than the first and the third. Furthermore, the starting values for the transition matrices suggest that staying in hidden states 1 and 3 is more likely than staying in hidden state 2. All non-zero probabilities are, however, mere suggestions and will be estimated with the fit_model() function. We now estimate this model 50 times with the EM algorithm using randomised starting values:\n\nset.seed(1)\nfit_hmm_2 <- fit_model(hidden_markov_model_2, \n  control_em = list(restart = list(times = 50))\n)\n\nWe can get the information on the EM estimation as follows:\n\nfit_hmm_2$em_results\n\n$logLik\n[1] -3546.155\n\n$iterations\n[1] 488\n\n$change\n[1] 9.947132e-11\n\n$best_opt_restart\n [1] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n [8] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n[15] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n[22] -3546.155 -3546.155 -3546.155 -3546.155\n\n\nThe loglik element gives the log-likelihood of the final model. This value has no meaning on its own, but it can be used to compare HMMs with the same data and model structure (e.g., when estimating the same model from different starting values). The iterations and change arguments give information on the last EM estimation round: how many iterations were used until the (local) optimum was found and what was the change in the log-likelihood at the final step.\nThe most interesting element is the last one: best_opt_restart shows the likelihood for 25 (by default) of the best estimation rounds. We advise to always check these to make sure that the best model was found several times from different starting values: this way we can be fairly certain that we have found the actual maximum likelihood estimates of the model parameters (global optimum). In this case all of the 25 log-likelihood values are identical, meaning that it is likely that we have found the best possible model among all HMMs with three hidden states.\n\nplot(fit_hmm_2$model, \n  legend.prop = 0.15, ncol.legend = 3,\n  edge.label.color = \"black\", vertex.label.color = \"black\",\n  combine.slices = 0, trim = 0.0001\n)\n\n\n\n\nFigure 5. HMM with three hidden states (pie charts), with transitions between hidden states shown as labelled edges.\n\n\n\n\nInterpreting the results in Figure 12.5 we see that the first hidden state represents about equal amounts of isolate and mediator roles, the second hidden state represents mainly Leaders and some Mediator roles, and the third hidden state represents mainly Mediator roles and partly Leader roles. Interestingly, none of the students start as Mediator/Leader, while of the other two the Isolate/Mediator state is more typical (two thirds). There are no transitions from the first to the second state nor vice versa, and transition probabilities to the second state are considerably higher than away from it. In other words, it seems that the model has two different origin states and one destination state.\nWe can visualise the observed and/or hidden state sequences with the ssplot() function. The ssplot() function can take an stslist object or a model object of class mm or hmm (see Figure 12.6). Here we want to plot full sequence index plots (type = \"I\") of both observed and hidden states (plots = \"both\") and sort the sequences using multidimensional scaling of hidden states (sortv = \"mds.hidden\"). See the seqHMM manual and visualisation vignette for more information on the different plotting options.\n\nssplot(fit_hmm_2$model, \n  # Plot sequence index plot (full sequences)\n  type = \"I\", \n  # Plot observed and hidden state sequences\n  plots = \"both\", \n  # Sort sequences by the scores of multidimensional scaling\n  sortv = \"mds.hidden\",\n  # X axis tick labels\n  xtlab = 1:20\n)\n\n\n\n\nFigure 6. Observed and hidden state sequences from the HMM with three hidden states.\n\n\n\n\nBy looking at the sequences, we can see that even though none of the students start in hidden state 3, the majority of them transition there. In the end, most students end up alternating between mediating and leadership roles.\nIs the three-state model better than the two-state model? As already mentioned, we can use model selection criteria to test that. To make sure that the three-state model is the best, we also estimate a HMM with four hidden states and then use the Bayesian information criterion for comparing between the three models. Because the four-state model is more complex, we increase the number of re-estimation rounds for the EM algorithm to 100.\n\n## Set seed for randomisation\nset.seed(1)\n\n## Build and estimate a HMM with four states\nhidden_markov_model_3 <- build_hmm(roles_seq, n_states = 4)\n\nfit_hmm_3 <- fit_model(hidden_markov_model_3, \n  control_em = list(restart = list(times = 100))\n)\n\nfit_hmm_3$em_results$best_opt_restart\n\n [1] -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304\n [8] -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.305\n[15] -3534.305 -3534.306 -3534.308 -3534.310 -3534.332 -3534.335 -3534.335\n[22] -3534.335 -3534.336 -3534.337 -3534.337\n\n\nThe best model was found only 13 times out of 101 estimation rounds from randomised starting values. A cautious researcher might be wise to opt for a higher number of estimation rounds for increased certainty, but here we will proceed to calculating the BIC values.\n\nBIC(fit_hmm$model)\n\n[1] 7430.028\n\nBIC(fit_hmm_2$model)\n\n[1] 7208.427\n\nBIC(fit_hmm_3$model)\n\n[1] 7259.37\n\n\nGenerally speaking, the lower the BIC, the better the model. We can see that the three-state model (fit_hmm_2) has the lowest BIC value, so three clusters is the best choice (at least among HMMs with 2–4 hidden states).\n\n\n4.2.3 Mixture Markov models\nThe MMM can be defined with the build_mmm() function. Similarly to HMMs, we need to either give the number of clusters with the n_clusters argument, which generates random starting values for the parameter estimates, or give starting values manually as initial_probs and transition_probs. Here we use random starting values:\n\n## Set seed for randomisation\nset.seed(123)\n## Define model structure (3 clusters)\nmmm <- build_mmm(roles_seq, n_clusters = 3)\n\nAgain, the model is estimated with the fit_model() function:\n\nfit_mmm <- fit_model(mmm)\n\nThe results for each cluster can be plotted one at a time (interactively, the default), or in one joint figure. Here we opt for the latter (see Figure 12.7). At the same time we also illustrate some other plotting options:\n\nplot(fit_mmm$model, \n  # Plot all clusters at the same time\n  interactive = FALSE, \n  # Set the number of rows and columns for cluster plots (one row, three columns)\n  nrow = 1, ncol = 3,\n  # Omit legends\n  with.legend = FALSE, \n  # Choose another layout for the vertices (see plot.igraph)\n  layout = layout_in_circle,\n  # Omit pie graphs from vertices\n  pie = FALSE,\n  # Set state colours\n  vertex.label.color = c(\"black\", \"black\", \"white\"),\n  # Set state label colours\n  vertex.color = cpal(roles_seq),\n  # Increase the size of the circle\n  vertex.size = 80,\n  # Plot state labels instead of initial probabilities\n  vertex.label = \"names\", \n  # Choose font colour for state labels\n  vertex.label.color = \"black\", \n  # Set state label in the centre of the circle\n  vertex.label.dist = 0,\n  # Omit labels for transition probabilities\n  edge.label = NA\n)\n\n\n\n\nFigure 7. MMM with three clusters.\n\n\n\n\nThe following code plots the sequence distribution plot of each cluster (Figure 12.8). In Cluster 1, we see low probabilities to downward mobility and high probabilities for upward mobility, so this cluster describes leadership trajectories. In Cluster 2, we can see that the thickest arrows lead to mediator and isolates roles, so this cluster describes trajectories with less central roles in collaboration. In Cluster 3, we see the highest transition probabilities for entering the mediator role but also some transitions from mediator to leader, so this cluster describes trajectories with more moderate levels of participation in comparison to cluster 1. This behavior is easier to see when visualising the sequences in their most probable clusters. The plot is interactive, so we need to hit ‘Enter’ on the console to generate each plot. Alternatively, we can specify which cluster we want to plot using the which.plots argument.\n\ncl1 <- mssplot(fit_mmm$model, \n  # Plot Y axis\n  yaxis = TRUE, \n  # Legend position\n  with.legend   = \"bottom\",\n  # Legend columns\n  ncol.legend = 3,\n  # Label for Y axis\n  ylab = \"Proportion\"\n)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 8. State distribution plots by most probable clusters estimated with the mixture Markov model.\n\n\n\nWe can add covariates to the model to explain cluster membership probabilities. For this, we need to provide a data frame (argument data) and the corresponding formula (argument formula). In the example data we use the data frame called cov_data that we created at the beginning of the tutorial with columns ID and GPA, where the order of the ID variable matches to that of the sequence data roles_seq (note that the ID variable is not used in the model building, so the user needs to make sure that both matrices are sorted by ID). We can now use the information about students’ GPA level as a predictor of the cluster memberships.\nNumerical estimation of complex models from random starting values may lead to convergence issues and other problems in the estimation (you may, for example, get warnings about the EM algorithm failing). To avoid such issues, giving informative starting values is often helpful. This model is more complex than the model without covariates and estimation from random starting values leads to convergence issues (not shown here). To facilitate model estimation, we use the results from the previous MMM as informative starting values. Here we also remove the common intercept by adding 0 to the formula, which simplifies the interpretation of the covariate effects later (instead of comparing to a reference category, we get separate coefficients for each of the three GPA categories).\n\nset.seed(98765)\nmmm_2 <- build_mmm(\n  roles_seq, \n  # Starting values for initial probabilities\n  initial_probs = fit_mmm$model$initial_probs,\n  # Starting values for transition probabilities\n  transition_probs = fit_mmm$model$transition_probs,\n  # Data frame for covariates\n  data = cov_data, \n  # Formula for covariates (one-sided)\n  formula = ~ 0 + GPA\n)\n\nAgain, the model is estimated with the fit_model() function. Here we use the EM algorithm with 50 restarts from random starting values:\n\nset.seed(12345)\nfit_mmm_2 <- fit_model(\n  mmm_2, \n  # EM with randomised restarts\n  control_em = list(\n    restart = list(\n      # 50 restarts\n      times = 50, \n      # Store loglik values from all 50 + 1 estimation rounds\n      n_optimum = 51\n    )\n  )\n)\n\nWarning in fit_model(mmm_2, control_em = list(restart = list(times = 50, : EM\nalgorithm failed: Estimation of gamma coefficients failed due to singular\nHessian.\n\n\nThe model was estimated 50 + 1 times (first from the starting values we provided and then from 50 randomised values). We get one warning about the EM algorithm failing. However, 50 estimation rounds were successful. We can check that the best model was found several times from different starting values (37 times, to be precise):\n\nfit_mmm_2$em_results$best_opt_restart\n\n [1] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n [8] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[15] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[22] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[29] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[36] -3614.627 -3614.627 -3619.695 -3624.547 -3624.547 -3624.547 -3624.547\n[43] -3624.547 -3624.547 -3624.547 -3624.547 -3624.547 -3624.547 -3631.328\n[50] -3637.344      -Inf\n\n\nWe can now be fairly certain that the optimal model has been found, and can proceed to interpreting the results. The clusters are very similar to what we found before. We can give the clusters more informative labels and then show state distribution plots in each cluster in Figure 12.9:\n\ncluster_names(fit_mmm_2$model) <- c(\n  \"Mainly leader\", \"Isolate/mediator\", \"Mediator/leader\"\n)\nmssplot(fit_mmm_2$model, with.legend = \"bottom\", ncol.legend = 3)\n\n\n\n\n\n\n\n(a) Mainly leader.\n\n\n\n\n\n\n\n(b) Isolate/mediator.\n\n\n\n\n\n\n\n\n\n(c) Mediator/leader.\n\n\n\n\nFigure 9. State distribution plots by most probable clusters estimated with the mixture Markov model with covariates.\n\n\n\nThe model summary shows information about parameter estimates of covariates and prior and posterior cluster membership probabilities (these refer to cluster membership probabilities before or after conditioning on the observed sequences, respectively):\n\nsummary_mmm_2 <- summary(fit_mmm_2$model)\nsummary_mmm_2\n\nCovariate effects :\nMainly leader is the reference.\n\nIsolate/mediator :\n           Estimate  Std. error\nGPALow       1.9221       0.478\nGPAMiddle    0.3901       0.314\nGPAHigh     -0.0451       0.277\n\nMediator/leader :\n           Estimate  Std. error\nGPALow        1.670       0.487\nGPAMiddle     0.411       0.312\nGPAHigh      -0.667       0.332\n\nLog-likelihood: -3614.627   BIC: 7461.487 \n\nMeans of prior cluster probabilities :\n   Mainly leader Isolate/mediator  Mediator/leader \n           0.244            0.425            0.331 \n\nMost probable clusters :\n            Mainly leader  Isolate/mediator  Mediator/leader\ncount                  49                87               64\nproportion          0.245             0.435             0.32\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n                 Mainly leader Isolate/mediator Mediator/leader\nMainly leader          0.91758          0.00136          0.0811\nIsolate/mediator       0.00081          0.89841          0.1008\nMediator/leader        0.05902          0.10676          0.8342\n\n\nWe will first interpret the information on prior and posterior cluster membership probabilities and then proceed to interpreting covariate effects. Firstly, the means of prior cluster probabilities give information on how likely each cluster is in the whole population of students (33% in Mediator, 24% in Leader, and 43% in Isolate). Secondly, Most probable clusters shows group sizes and proportions if each student would be classified into the cluster for which they have the highest cluster membership probability.\nThirdly, the Classification table shows mean cluster probabilities (in columns) by the most probable cluster (in rows). We can see that the clusters are fairly crisp (the certainty of cluster memberships are fairly high) because the membership probabilities are large in the diagonal of the table. The uncertainty of the classification is the highest for the Mediator/leader cluster (among those that had the highest membership probability in that cluster, average cluster memberships were 84% for the Mediator/leader cluster, 6% for the Mainly leader cluster, and 10% for the Isolate/mediator cluster) and the highest in the Mainly leader cluster (92% for the Mainly leader cluster, 8% for the Mediator/leader cluster, and 0.1% for the Isolate/mediator cluster).\nThe part titled Covariate effects shows the parameter estimates for the covariates. Interpretation of the values is similar to that of multinomial logistic regression, meaning that we can interpret the direction and uncertainty of the effect –relative to the reference cluster Mainly leader– but we cannot directly interpret the magnitude of the effects (the magnitudes are on log-odds scale). We can see that individuals with low GPA more often end up in the Isolate/mediator cluster and the Mediator/leader cluster in comparison to the Mainly leader cluster (i.e., the standard errors are small in comparison to the parameter estimates), while individuals with high GPA levels end up in the Mediator/leader cluster less often but are not more or less likely to end up in the Isolate/mediator cluster. For categorical covariates such as our GPA variable, we can also easily compute the prior cluster membership probabilities from the estimates with the following call:\n\nexp(fit_mmm_2$model$coefficients)/rowSums(exp(fit_mmm_2$model$coefficients))\n\n          Mainly leader Isolate/mediator Mediator/leader\nGPALow       0.07605453        0.5198587       0.4040868\nGPAMiddle    0.25090105        0.3705958       0.3785031\nGPAHigh      0.40497185        0.3870997       0.2079285\n\n\nThe matrix shows the levels of the covariates in the rows and the clusters in the columns. Among the high-GPA students, 41 percent are classified as Mainly leaders, 39 percent as Isolate/mediators, and 21 percent as Mediator/leaders. Among middle-GPA students classification is relatively uniform (25% as Mainly leaders, 37% as Isolate/mediators and 38 Mediator/leaders) whereas most of the low-GPA students are classified as Isolate/mediators or Mediator/leaders (52% and 40%, respectively).\nThe summary object also calculates prior and posterior cluster memberships for each student. We omit them here, for brevity, but demonstrate that they can be obtained as follows:\n\nprior_prob <- summary_mmm_2$prior_cluster_probabilities\nposterior_prob <- summary_mmm_2$posterior_cluster_probabilities\n\n\n\n4.2.4 Mixture hidden Markov models\nFinally, we will proceed to the most complex of the models, the MHMM.\nFor defining a MHMM, we use the build_mhmm() function. Again, we can use the argument n_states which is now a vector showing the number of hidden states in each cluster (the length of the vector defines the number of clusters). We will begin by estimating a MHMM with three clusters, each with two hidden states:\n\n\n\n\nset.seed(123)\nmhmm <- build_mhmm(\n  roles_seq, \n  n_states = c(2, 2, 2),\n  data = cov_data, \n  formula = ~ 0 + GPA\n)\nfit_mhmm <- fit_model(mhmm)\n\nError in fit_model(mhmm): EM algorithm failed: Estimation of gamma coefficients\nfailed due to singular Hessian.\n\n\nIn this case, we get an error message about the EM algorithm failing. This means that the algorithm was not able to find parameter estimates from the random starting values the build_mhmm() function generated and we need to adjust our code.\nStarting values for the parameters of the MHMM can be given with the arguments initial_probs, transition_probs, and emission_probs. For the MHMM, these are lists of vectors and matrices, one for each cluster. We use the same number of hidden states (two) for each cluster. We define the initial values for the transition and emission probabilities as well as regression coefficients ourselves. We also restrict the initial state probabilities so that in each cluster every student is forced to start from the same (first) hidden state.\n\nset.seed(1)\n\n## Set initial probabilities\ninit <- list(c(1, 0), c(1, 0), c(1, 0))\n\n## Define own transition probabilities\ntrans <- matrix(c(\n  0.9, 0.1,\n  0.1, 0.9\n), nrow = 2, byrow = TRUE)\n\ntranslist <- list(trans, trans, trans)\n\n## Simulate emission probabilities\nemiss <- simulate_emission_probs(\n  n_states = c(2, 2, 2), \n  n_symbols = 3, \n  n_clusters = 3\n)\n\nemiss <- replicate(3, matrix(1/3, 2, 3), simplify = FALSE)\n\n## Define initial values for coefficients\n## Here we start from a case where low GPA correlates with Cluster 1, \n## whereas middle and high GPA has no effect\nbeta <- cbind(0, c(-2, 0, 0), c(-2, 0, 0))\n\n## Define model structure\nmhmm_2 <- build_mhmm(\n  roles_seq, \n  initial_probs = init, transition_probs = translist, \n  emission_probs = emiss, data = cov_data, \n  formula = ~ 0 + GPA, beta = beta\n)\n\nNow that we have built the MHMM, we can estimate its parameters:\n\nset.seed(1)\nsuppressWarnings(fit_mhmm_2 <- fit_model(\n  mhmm_2,\n  control_em = list(restart = list(times = 100, n_optimum = 101)))\n)\n\nWe can now check how many times the log-likelihood values occurred in the 101 estimations:\n\ntable(round(fit_mhmm_2$em_results$best_opt_restart, 2))\n\n\n    -Inf -3672.25 -3595.82 -3588.58 -3584.14 -3526.42 -3525.06 -3519.53 \n      56        2        1        3        1        4        1        2 \n -3519.5 -3519.24 \n      15       16 \n\n\nThe best model was found 16 times out of 101 times, although the second beset model with log-likelihood of -3519.5 is likely almost indistinguishable from the optimal model (-3519.24) as their log-likelihoods are so close to each other.\nWe will start to interpret the model by looking at the sequence plots in each cluster (see Figure 12.10). The function call is interactive. As before, if you only want to plot one cluster you can use the which.plots argument:\n\nmssplot(fit_mhmm_2$model, \n        plots = \"both\", type = \"I\", sortv = \"mds.hidden\", \n        with.legend = \"bottom.combined\", legend.prop = .15)\n\n\n\n\n\n\n\n(a) Cluster 1\n\n\n\n\n\n\n\n(b) Cluster 2\n\n\n\n\n\n\n\n(c) Cluster 3\n\n\n\n\nFigure 10. MHMM estimated sequence distribution plot with hidden states.\n\n\n\nWe can also visualise the model parameters in each cluster (see Figure 12.11):\n\nplot(fit_mhmm_2$model, \n  vertex.size = 60,\n  label.color = \"black\",\n  vertex.label.color = \"black\",\n  edge.color = \"lightgray\",\n  edge.label.color = \"black\",\n  legend.prop = 0.4,\n  ncol.legend = 1, \n  ncol = 3,\n  interactive = FALSE,\n  combine.slices = 0\n)\n\n\n\n\nFigure 11. Transitions between states for each trajectory.\n\n\n\n\nBased on the two plots, we can determine that Cluster 1 describes students who start as leaders but then transition to alternating between mediator and leader. Cluster 2 describes students who start by alternating between isolate and mediator roles and then mainly transition to alternating between mediator and leader roles. Cluster 3 describes students who start as alternating between isolate and mediator roles, after which they transition between isolate/mediator and mediator/leader.\n\ncluster_names(fit_mhmm_2$model) <- c(\n  \"Downward transition\", \"Upward transition\", \"Alternating\"\n)\n\nWith summary(fit_mhmm_2$model) we get the parameter estimates and standard errors for the covariates and information about clustering:\n\nsummary(fit_mhmm_2$model)\n\nCovariate effects :\nDownward transition is the reference.\n\nUpward transition :\n           Estimate  Std. error\nGPALow       -0.455       0.464\nGPAMiddle     0.440       0.310\nGPAHigh      -2.743       0.727\n\nAlternating :\n           Estimate  Std. error\nGPALow       1.3560       0.324\nGPAMiddle    0.3461       0.316\nGPAHigh      0.0468       0.250\n\nLog-likelihood: -3519.243   BIC: 7237.543 \n\nMeans of prior cluster probabilities :\nDownward transition   Upward transition         Alternating \n              0.302               0.181               0.517 \n\nMost probable clusters :\n            Downward transition  Upward transition  Alternating\ncount                        61                 30          109\nproportion                0.305               0.15        0.545\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n                    Downward transition Upward transition Alternating\nDownward transition             0.95727            0.0267      0.0161\nUpward transition               0.03007            0.8037      0.1662\nAlternating                     0.00975            0.0962      0.8940\n\n\nWe can see, that the prior probabilities of belonging to each cluster are very different: half of the students can be described as alternating, while of the rest, a downward transition is more typical (31%). Based on the classification table, the Downward transition cluster is rather crisp, while the other two are partly overlapping (see the MMM example for more information on interpreting the classification table).\nThe Covariate effects tables show that, in comparison to Alternating cluster, students with low GPA are less likely to end up in the Upward or Downward transition clusters and students with high GPA are less likely to end up in Upward transition cluster. Again, we can calculate the probabilities of belonging to each cluster by GPA levels:\n\nexp(fit_mhmm_2$model$coefficients)/rowSums(exp(fit_mhmm_2$model$coefficients))\n\n          Downward transition Upward transition Alternating\nGPALow              0.1813217        0.11502283   0.7036555\nGPAMiddle           0.2521406        0.39144399   0.3564154\nGPAHigh             0.4734128        0.03048189   0.4961054\n\n\nThe table shows that students with low GPA typically belong to the Alternating cluster (70 % probability) while students with high GPA mainly end up in the Downward transition cluster (47%) or the Alternating cluster (50%). Most students with middle GPA end up in the Upward transition cluster (39%), but the probabilities are almost as high for the Alternating cluster (36%) and also fairly high for the Downward transition cluster (25%).\nIn light of this, it is worth noting that the covariates do not merely explain the uncovered clusters; as part of the model, they drive the formation of the clusters. In other words, an otherwise identical model without the dependence on the GPA covariate may uncover different groupings with different probabilities.\nIf we are not sure how many clusters or hidden states we expect, or if we wish to investigate different combinations of covariates, we can estimate several models and compare the results with information criteria or cross-validation. Estimating a large number of complex models is, however, very time-consuming. Using prior information for restricting the pool of potential models is useful, and sequence analysis can also be used as a helpful first step [10, 37].\n\n\n\n4.3 Stochastic process mining with Markovian models\nProcess miming is a relatively recent method for the analysis of event-log data (time-stamped logs) which aims to understand the flow and dynamics of the process under study. In education, process mining has been used extensively to analyse learners’ online logs collected from Learning Management Systems (LMS), to understand how they utilize learning resources and transitions between learning activities to mention a few [38, 39]. In this book, we have devoted a full chapter for process mining where we explained how process mining can be performed in R [40]. Yet, in this chapter we will present a novel method that we propose to perform stochastic process mining using MMs. While process mining can be performed using different software, techniques and algorithms, MMs offer a powerful framework for process mining with several advantages over the commonly used methods. First, it is more theoretically aligned with the idea of a transition from an action to an action and that actions are temporally dependent on each other. Second, MMs allow for data to be clustered into similar transition patterns, a possibility not offered by other process mining methods (see the process mining chapter of this book [40]). Third, contrary to other process mining methods, MMs do not require researchers to arbitrarily exclude —or trim— a large part of the data to “simplify” the model. For instance, most of the process mining analyses require an arbitrary cutoff to trim the data so that the process model is readable. This trimming signficaintly affect the resulting model and makes it hard to replicate. Most importantly, MMs have several fit statistics that we can use to compare and judge the model fit as we have seen before.\nSeveral R packages can perform stochastic process mining; in this tutorial we will rely on the same package we discussed earlier and combine it with a powerful visualization that allows us to effectively visualize complex processes. In the next example, we will analyse data extracted from the learning management system logs and offer a detailed guide to process mining. We will also use MMMs to cluster the data into latent patterns of transitions. Given that the traditional plotting function in seqHMM works well with a relatively short alphabet, we will use a new R package called qgraph for plotting. The package qgraph offers powerful visualizations which makes plotting easier, and more interpretable especially for larger models. Furthermore, qgraph allows researchers to use a fixed layout for all the plotted networks so the nodes can be compared to each other more easily.\nLet us now go through the analysis. The next chunk of code imports the prepared sequence data from the sequence analysis chapter. The data belong to a learning analytics course and the events are coded trace logs of students’ actions such as Course view, Instructions, Practicals, Social, etc. Then, we build a sequence object using the function seqdef() from TraMineR.\n\nseq_data <- import(paste0(URL, \"1_moodleLAcourse/LMS_data_wide.xlsx\"))\nseq_data_all <- seqdef(seq_data, var = 7:54 )\n\nBefore proceeding further, it is advisable to visualise the sequences. Figure 12.12 shows the sequence index plot, sorted according to the first states. The data are much larger than the collaboration roles and achievement sequences analysed previously; there are 9478 observations with an alphabet of 12 states. Unlike in the previous example, the sequence lengths vary considerably. Due to this, shorter sequences contain missing values to fill the empty cells in the data frame. However, there are no internal gaps. When creating the sequence object with the seqdef function, TraMineR allows for distinguishing between real missing values (NA, where the true state is unknown) and technical missing values (void) used to pad the sequences to equal lengths. The seqHMM package is able to account for both types of missing values and treats them slightly differently, for example when calculating the most probable paths of hidden states.\n\nseqplot(seq_data_all, \n  type = \"I\", ncol = 4, sortv = \"from.start\",\n  legend.prop = 0.2, cex.legend = 0.7, border = NA,\n  ylab = \"Sequence (sorted)\", xlab = \"Time\"\n)\n\n\n\n\nFigure 12. Sequence index plot for the learning management system logs.\n\n\n\n\nA simple transition analysis can be performed by estimating and plotting the transition probabilities. This can be performed using the TraMineR package. Yet, this simple approach has drawbacks and it is advisable to estimate the MM and use their full power. The next code estimates the transition probabilities of the full dataset and visualize them using the function seqtrate() from TraMineR package.\n\noveralltransitions <- seqtrate(seq_data_all)\n\n\n\n\n\n\n\nTable 7.  Transition probabilities \n  \n    \n    \n      From\\To\n      Applications\n      Assignment\n      Course_view\n      Ethics\n      Feedback\n      General\n      Group_work\n      Instructions\n      La_types\n      Practicals\n      Social\n      Theory\n    \n  \n  \n    Applications\n0.46\n0.07\n0.13\n0.01\n0.01\n0.19\n0.05\n0.01\n0.01\n0.05\n0.00\n0.00\n    Assignment\n0.00\n0.70\n0.19\n0.00\n0.01\n0.02\n0.03\n0.02\n0.02\n0.02\n0.00\n0.00\n    Course_view\n0.01\n0.07\n0.35\n0.01\n0.03\n0.03\n0.28\n0.10\n0.02\n0.08\n0.02\n0.01\n    Ethics\n0.01\n0.00\n0.12\n0.61\n0.01\n0.04\n0.10\n0.01\n0.03\n0.04\n0.01\n0.02\n    Feedback\n0.00\n0.02\n0.23\n0.00\n0.56\n0.00\n0.11\n0.04\n0.01\n0.02\n0.00\n0.00\n    General\n0.04\n0.05\n0.18\n0.01\n0.00\n0.49\n0.06\n0.06\n0.05\n0.03\n0.01\n0.02\n    Group_work\n0.00\n0.01\n0.19\n0.00\n0.01\n0.01\n0.73\n0.02\n0.00\n0.01\n0.01\n0.00\n    Instructions\n0.00\n0.02\n0.33\n0.00\n0.03\n0.04\n0.12\n0.37\n0.02\n0.03\n0.04\n0.00\n    La_types\n0.01\n0.06\n0.24\n0.01\n0.00\n0.10\n0.07\n0.05\n0.38\n0.03\n0.01\n0.03\n    Practicals\n0.00\n0.02\n0.17\n0.00\n0.01\n0.01\n0.03\n0.02\n0.01\n0.73\n0.00\n0.01\n    Social\n0.00\n0.01\n0.25\n0.00\n0.00\n0.01\n0.12\n0.11\n0.01\n0.02\n0.48\n0.00\n    Theory\n0.00\n0.02\n0.15\n0.03\n0.00\n0.02\n0.06\n0.01\n0.05\n0.05\n0.00\n0.60\n  \n  \n  \n\n\n\n\n\nAs we mentioned earlier, we will use a novel plotting technique that is more suitable for large process models. Below, we plot the transition probabilities with the qgraph() function from the qgraph package (Figure 12.13). We use some arguments to improve the process model visualization. First, we use the argument cut = 0.15 to show the edges with probabilities below 0.15 in lower thickness and colour intensity. This cut makes the graph easier to read and less crowded, and gives emphasis to the edges which matter. The argument minimum = 0.05 hides small edges below the probability threshold of 0.05. We use edge.labels = TRUE to show the transition probabilities as edge labels. The argument color gets the colour palette from the sequence with the function cpal() and the argument curveAll = TRUE ensures the graph shows curved edges. The \"colorblind\" theme makes sure that the colours can be seen by everyone regardless of colour vision abilities. Lastly, the mar argument sets the margin of the figure to make all graphical aspects fit within the figure area.\n\nLabelx <- alphabet(seq_data_all) # get the labels to use them as nodes names.\ntransitionsplot <- qgraph(\n  overalltransitions, cut = 0.15, minimum = 0.05, \n  labels = Labelx, edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), curveAll = TRUE, \n  theme = \"colorblind\", mar = c(4, 3, 4, 3)\n)\n\n\n\n\nFigure 13. Process map for the overall process.\n\n\n\n\nThe seqtrate() function only computes the transition probabilities but does not compute the initial probabilities. While it is not difficult to calculate the proportions of starting in each state, we can also estimate a simple Markov model which does the same with a short command. We do so using the build_mm() function as per Section \\(\\ref{markov}\\), recalling that the build_mm() function is distinct from build_hmm(), build_mmm(), and build_mhmm() in that it is the only build function that automatically estimates the parameters of the model.\nThe plotting now includes an extra option called pie = overallmodel$initial_probs which tells qgraph to use the initial probabilities from the fitted MM as the sizes of the pie charts in the borders of the nodes in Figure 12.14. For instance, the pie around Course view is around half of the circle corresponding to 0.48 initial probability to start from Course view. Please also note that the graph is otherwise equal to the one generated via seqtrate() apart from these initial probabilities.\n\noverallmodel <- build_mm(seq_data_all)\n\noverallplot <- qgraph(\n  overalltransitions, \n  cut = 0.15, \n  minimum = 0.05, \n  labels = Labelx, \n  mar = c(4, 3, 4, 3), \n  edge.labels = TRUE, \n  edge.label.cex = 0.65, \n  color = cpal(seq_data_all), \n  curveAll = TRUE, \n  theme = \"colorblind\", \n  pie = overallmodel$initial_probs\n)\n\n\n\n\nFigure 14. Process map for the overall process with initial probabilities.\n\n\n\n\nHaving plotted the transitions of the full dataset, we can now look for transition patterns, that is typical transition patterns (i.e., clusters) that are repeated within the data. The procedure is the same as before. In the next example, we use the function build_mmm() to build the model with four clusters as a demonstration. Ideally, researchers need to estimate several models and choose the best model based on model selection criteria (such as BIC) values as well as interpretability.\nThe steps involved in fitting the model are as before; we make use of the function fit_model() to estimate the model. The results of the running the code will be an MM for each cluster (with distinct initial and transition probabilities). Given the number of sequences in the dataset, their length, and the number of states, the computational burden is larger than for previous applications in this chapter. For illustrative purposes, instead of repeated EM runs with random starting values, we use single EM run followed by global optimisation, using the argument global_step = TRUE. One benefit of this global (and local) step in fit_model over the EM algorithm is the flexibility to define a maximum runtime (in seconds) for the optimization process (argument maxtime in control_global). This can be valuable for larger problems with predefined runtime (e.g., in a shared computer cluster). Note, however, that relying on the runtime can lead to non-reproducible results even with fixed seed if the optimisation terminates due to the time limit. Finally, we run additional local optimisation step using the results of the global optimisation, for more accurate results. The last argument threads = 16 instructs to use parallel computing to enable faster fitting (please, customise according to the number of cores in your computer). As for the starting values, we use the transition probabilities computed from the full data for all clusters, and random values for the initial probabilities.\nWhile in theory many of global optimisation algorithms should eventually find the global optimum, in practice there are no guarantees that it is found in limited time. Thus, as earlier, in practice it is advisable to try different global/local optimisation algorithms and/or EM algorithm with different initial values to make it more likely that the global optimum is found (see [4] for further discussion).\n\nset.seed(1)\ntrans_probs <- simulate_transition_probs(12, 4, diag_c = 5)\ninit_probs <- as.numeric(prop.table(table(seq_data_all[,1])[1:12]))\ninit_probs <- replicate(4, init_probs, simplify = FALSE)\n\nbuiltseqLMS <- build_mmm(\n  seq_data_all,\n  transition_probs = trans_probs,\n  initial_probs = init_probs\n)\n\nfitLMS <- fit_model(\n  builtseqLMS, \n  global_step = TRUE,\n  control_global = list(\n    maxtime = 3600, \n    maxeval = 1e5,\n    algorithm = \"NLOPT_GD_STOGO_RAND\"),\n  local_step = TRUE,\n  threads = 16\n)\n\nfitLMS$global_results$message\nfitLMS$logLik\n\n\n\n\n\n\n[1] \"NLOPT_SUCCESS: Generic success return value.\"\n\n\n[1] -114491.2\n\n\nBefore plotting the clusters, let us do some cleanups. First, we get the transition probabilities of each cluster and assign them to a variable. In that way, it is easier to manipulate and work with. In the same way, we can extract the initial probabilities for each cluster.\n\n##extract transition probabilities of each cluster\nClustertp1 <- fitLMS$model$transition_probs$`Cluster 1`\nClustertp2 <- fitLMS$model$transition_probs$`Cluster 2`\nClustertp3 <- fitLMS$model$transition_probs$`Cluster 3`\nClustertp4 <- fitLMS$model$transition_probs$`Cluster 4`\n\n##extract initial probabilities of each cluster\nClusterinitp1 <- fitLMS$model$initial_probs$`Cluster 1`\nClusterinitp2 <- fitLMS$model$initial_probs$`Cluster 2`\nClusterinitp3 <- fitLMS$model$initial_probs$`Cluster 3`\nClusterinitp4 <- fitLMS$model$initial_probs$`Cluster 4`\n\nPlotting the process maps can be performed in the same way we did before. However, if we need to compare clusters, it is best if we use a unified layout. An average layout can be computed with the function averageLayout() which takes the transition probabilities of the four clusters as input and creates — as the name implies— an averaged layout. Another option is to use the same layout of the overallplot in the previous example. This can be obtained from the plot object overallplot$layout. This can be helpful if you would like to plot the four plots corresponding to each cluster with the same layout as the overall plot (see Figure 12.15).\n\nLabelx <- colnames(Clustertp1) # we need to get the labels\n\nAveragelayout <- averageLayout(\n  list(Clustertp1, Clustertp2, Clustertp3, Clustertp4)\n)\n## You can also try with this layout from the previous plot\nOveralllayout <- overallplot$layout \n\nqgraph(\n  Clustertp1, cut = 0.15, minimum = 0.05 , labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all), \n  layout = Averagelayout, pie = Clusterinitp1, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Diverse\"\n)\n\nqgraph(\n  Clustertp2, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp2, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Assignment-oriented\"\n)\n\nqgraph(\n  Clustertp3, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp3, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Practical-oriented\"\n)\n\nqgraph(\n  Clustertp4, cut = 0.15, minimum = 0.05 , labels = Labelx, \n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp4, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Group-centered\"\n)\n\n\n\n\nFigure 15. Process maps for each cluster.\n\n\n\n\nOftentimes, the researcher is interested in comparing two pre-defined fixed groups, e.g., high achievers and low achievers, rather than between the computed clusters. In the next example we will compare high to low achievers based on their achievement levels. First, we have to create a separate sequence object for each group. We do this by filtering but you can do it in other ways. For instance, you can create two sequences from scratch for each group. The next is to build the MMs separately for each group.\n\nseq_high <- seq_data_all[seq_data$Achievementlevel4 <= 2,]\nseq_low <-  seq_data_all[seq_data$Achievementlevel4 > 2,]\n\nhigh_mm <- build_mm(seq_high)\nlow_mm <- build_mm(seq_low)\n\nBefore plotting the groups, let us do some cleaning, like we did before. First, we get the transition and initial probabilities of each group. We also compute an average layout. Please note that you can use the layout from the previous examples if you are comparing the models against each other and you need a unified framework. The plotting is the same as before (see Figure 12.16).\n\npar(mfrow=c(1, 2))\n\n##extract transition probabilities of each cluster\nHighprobs <- high_mm$transition_probs\nLowprobs <- low_mm$transition_probs\n\n##extract initial probabilities of each cluster\nHighinit <- high_mm$initial_probs\nLowinit <- high_mm$initial_probs\n\nAveragelayout <- averageLayout(list(Highprobs, Lowprobs))\n\nHighplot <- qgraph(\n  Highprobs, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), layout = Averagelayout, \n  pie = Highinit, theme = \"colorblind\", title = \"High achievers\"\n)\n\nLowplot <-  qgraph(\n  Lowprobs, cut=0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), layout = Averagelayout, \n  pie = Lowinit, theme = \"colorblind\", title = \"Low achievers\"\n)\n\n\n\n\nFigure 16. Process maps for high achievers and low achievers using average layout.\n\n\n\n\nWe can also plot the difference plot (see Figure 12.17); that is, what the low achievers do less than high achievers. In this case, red edges are negative (events they do less) and blue edges are positive (events that they do more than high achievers). As you can see, the differences are not that huge. In fact, much of the literature comparing high and low achievers uses higher thresholds e.g., top 25% to bottom 25% or even top 10% to bottom 10%.\n\ndiffplot <- qgraph(\n  Lowprobs - Highprobs, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, layout = Averagelayout, \n  color = cpal(seq_data_all), theme = \"colorblind\"\n)\n\n\n\n\nFigure 17. Difference between process maps of high achievers and low achievers using average layout."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#conclusions-further-readings",
    "href": "chapters/ch12-markov/ch12-markov.html#conclusions-further-readings",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "5 Conclusions & further readings",
    "text": "5 Conclusions & further readings\nMarkovian models provide a flexible model-based approach for analysing complex sequence data. MMs and HMMs have proven useful in many application areas such as biology and speech recognition, and can be a valuable tool in analysing data in educational settings as well. Their mixture variants allow for the representation of complex systems by combining multiple MMs or HMMs, each capturing different aspects of the underlying processes, allowing probabilistic clustering, information compression (e.g. visualisation of multicategory data from multiple domains), and detection of latent features of sequence data (e.g, extraction of different learning strategies). The ability to incorporate covariates in the case of MMMs and MHMMs makes those models even more powerful, and generally MMs and MMMs represent useful tools in the field of process mining also.\nThe seqHMM package used in the examples supports time-constant covariates for predicting cluster memberships for each individual. In theory, covariates could be used to define transition or emission probabilities as well, leading to subject-specific and possibly time-varying transition and emission probabilities (in the case of time-varying covariates). However, at the time of writing this chapter, these are not supported in seqHMM (this may change in the future). In R, there are at least two other, potentially useful packages: for MMs, the dynamite [41] package supports covariates on the transition probabilities with potentially time-varying effects, whereas LMest [42] supports MMs and HMMs with covariates, and restricted variants of the MHMM where only the initial and transition matrices vary between clusters. Going beyond the R software, some commercial software also offers tools to analyse Markovian models, including latentGold [43] and Mplus [11].\nThe conditional independence assumption of observations given the latent states in HMMs can sometimes be unrealistic. In these settings, the so-called double chain MMs can be used [44]. There the current observation is allowed to depend on both the current state and the previous observation. Some restricted variants of such models are implemented in the march package in R [45]. Finally, variable-order MMs extend basic MMs by allowing the order of the MM to vary in time. A TraMineR-compatible implementation of variable-order models can be found in the PST package [46].\nWe encourage readers to read more about how to interpret the results in the original study where the data for this chapter was drawn from [36]. We also encourage readers to learn more about Markovian models in the context of multi-channel sequence analysis in Chapter 13 [8]."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#acknowledgements",
    "href": "chapters/ch12-markov/ch12-markov.html#acknowledgements",
    "title": "12  A modern approach to transition analysis and process mining with Markov models: A tutorial with R",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nJH and SH were supported by Research Council of Finland (PREDLIFE: Towards well-informed decisions: Predicting long-term effects of policy reforms on life trajectories, grants 331817 and 331816). MS was supported by Research Council of Finland (TOPEILA: Towards precision education: Idiographic learning analytics, grant 350560)."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html",
    "href": "chapters/ch13-multichannel/ch13-multi.html",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#introduction",
    "href": "chapters/ch13-multichannel/ch13-multi.html#introduction",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning is a dynamic phenomenon which follows the unidirectional forward law of time; that is, learning occurs sequentially in a time-ordered manner [1, 2]. Throughout this book, we have devoted several chapters to sequence analysis of learner-related data, due to its increasingly central role in learning analytics and education research as a whole. First, in Chapter 10 [3], we presented an introduction to sequence analysis and a discussion on the relevance of this method. Subsequently, in Chapter 11 [4], we studied how to build sequences from multiple variables, analyse more complex aspects of sequences, and cluster sequences into trajectories which undergo a similar evolution, using distance-based methods. Then, in Chapter 12 [5], we learned how to cluster sequential data using Markov models, and touched briefly on multi-channel sequences.\nIn this new chapter, we cover multi-channel sequence analysis, which deals with the analysis of two or more synchronised sequences, in greater detail. An example in educational research would be the analysis of the simultaneous unfolding of motivation, achievement, and engament sequences. Multi-channel sequence analysis is a rather novel method in social sciences [6–9] and its applications within educational research in general remain scarce. The increasing availability of student multimodal temporal data makes multi-channel sequence analysis a relevant and timely method to tackle the challenges that these new data bring.\nThroughout this chapter, we describe multi-channel sequence analysis in detail, with an emphasis on how to detect patterns within the sequences, i.e., clusters —or trajectories— of multi-channel sequences that share similar temporal evolutions (or similar trajectories). We illustrate the method with a case study in which we examine students’ sequences of online engagement and academic achievement, where we analyse the longitudinal association between both constructs simultaneously. Here, we outline two perspectives on clustering multi-channel sequences and present both in the tutorial. The first approach uses distance-based clustering algorithms, very much in the spirit of the single-channel cluster analysis described in Chapter 10. We describe some limitations of this framework and then principally focus on a more efficient approach to identifying distinct trajectories using mixture hidden Markov models. We also show how covariates can be incorporated to make the Markovian framework even more powerful. This main analysis is inspired by the recently published paper by [10]. In the next section, we provide a description of the multi-channel sequence analysis framework. We follow with a review of the existing literature in learning analytics that has relied on multi-channel sequence analysis. Next, we include a step-by-step tutorial on how to implement multi-channel sequence analysis with R, with particular attention paid to clustering via distance-based algorithms and mixture hidden Markov models. Finally, we conclude with a brief discussion and provide recommendations for further reading."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#multi-channel-sequence-analysis",
    "href": "chapters/ch13-multichannel/ch13-multi.html#multi-channel-sequence-analysis",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "2 Multi-channel sequence analysis",
    "text": "2 Multi-channel sequence analysis\nMulti-channel sequence analysis refers to the process of analysing sequential data that consists of multiple parallel channels or streams of (categorical) information. Each channel provides a different perspective or type of information. The goal of multi-channel sequence analysis is to jointly explore the dependencies and temporal interactions between the different channels and extract meaningful insights that may not be evident when considering each channel in isolation. The sources of information can be of very varied nature. For example, using video data of students working on a collaborative task, we could code students’ spoken utterances in one channel, and their facial expressions in another channel throughout the whole session (see Figure 13.1). In this way, we could analyse how students’ expressions relate to what they are saying. Multi-channel sequence analysis often follows the following steps:\n\n\n\nFigure 1. An example of two channels of sequence data where the first channel (top) represents students’ utterances in collaborative work and the second channel (bottom) represents their facial expressions.\n\n\n\n2.1 Step 1: Building the channel sequences\nJust like in regular sequence analysis (see Chapter 10 [3]), the first step in multi-channel sequence analysis is to construct the sequences which constitute the different channels. A sequence is an ordered list of categorical elements (i.e., events, states or categories). These elements are discrete (as opposed to numerical values such as grades) and are ordered chronologically. In a sequence, time is not a continuum but a succession of discrete timepoints. These timepoints can be generated by sampling (e.g., each 30 seconds constitutes a new timepoint) or can emerge from an existing time scheme (e.g., each new lesson in a course is a new timepoint). In multi-channel sequence analysis, it is very important that all channels (i.e., parallel sequences) follow the same time scheme so they can be synchronised. As well as time points being aligned in this fashion, it is also typically assumed that the sequence length for each observation matches across channels (although some of the methods used can also deal with partially missing information). The elements of each sequence —typically referred to as the alphabet, which is unique for each channel— can be predefined events or categories from the beginning (e.g., utterances or facial expressions) or, in cases where we are dealing with numerical variables (e.g., heart rate or grades), we can convert them into a state or category by dividing the numerical variable into levels (e.g., tertiles, quartiles) or using clustering techniques. This way, we can focus on sharp changes in numerical variables and conceal small, probably insignificant changes. As Winne puts it [11], “reliability can sometimes be improved by tuning grain size of data so it is neither too coarse, masking variance within bins, nor too fine-grained, inviting distinctions that cannot be made reliably”. Once we have our channels represented as sequences of ordered elements that follow the same time scheme, we can use the steps learned in the introductory sequence analysis chapter to construct the sequences.\n\n\n2.2 Step 2: Visualising the multi-channel sequence\nWhen we have built the sequences corresponding to all of the channels, we can visualize the data. Visualizing multi-channel sequence data is not a straightforward task, and we need to decide whether we want to present each channel separately or extend the alphabet to show a plot of combined states—or sometimes even both. The extended alphabet approach —in which the states are the combination of the states of all the channels— helps us understand the structure of the data and the typical combinations and evolution of the combined states from all of the channels. It usually works well when (1) the extended alphabet is moderate in size, (2) when there are no state combinations that are very rare, and (3) when the states are either fully observed or missing in all channels at the same time. The extended alphabet is likely to be of moderate size when there are at most 2–3 channels with a small alphabet in each, or if some combinations are not present in the data. Rare state combinations are typically impractical both when visualizing and analyzing the data—especially if there are many rare combinations or if the data are very sensitive in nature. Finally, if there are partially missing observations, i.e., observations that at some specific time point are missing from some of the channels but not all, we would need to deal with each combination of missing and observed states separately, which further extends the alphabet. If one or more of the three conditions are not met, it is often preferable to resort to presenting each channel separately or using visualization techniques that summarize information (e.g., sequence distribution plots).\nContinuing with the previous example, the possible states in the extended alphabet would be all the combinations between the alphabet of the utterances channel (Question-Argument-Agreement) and the alphabet of the facial expressions channel (Happy-Neutral-Anxious). In Figure 13.2, we can see that the first student starts by ‘Question/Happy’, then goes to ‘Argument/Anxious’ and so on.\n\n\n\nFigure 2. An example of a multi-channel sequence object obtained by using the extended alphabet approach, i.e., combining the states of the utterances and facial expressions channels of the data shown in Figure 13.1.\n\n\n\n\n2.3 Step 3: Finding patterns (clusters or trajectories)\nPatterns may exist in all types of sequences and, in fact, in all types of data. Discovering such patterns, variations, or groups of patterns is a central theme in analytics. In sequence analysis, similar patterns reflect common temporal pathways where the data share a similar temporal course. This could be a group of students, a typical succession of behavior, or a sequence of interactions. As such, the last typical step in the analysis is to find such patterns. Since these patterns are otherwise latent or unobservable, we need a clustering algorithm that unveils such hidden patterns. There are different possibilities to cluster multi-channel sequence data, of which two are described below: extensions to traditional distance-based clustering approaches widely used in single-channel analyses and Markovian models. The two approaches are then further distinguished in Section \\(\\ref{covariates}\\), with regard to the ability of the Markovian approach to directly accommodate covariates.\n\n2.3.1 Traditional sequence analysis extensions\nBroadly speaking, the first paradigm involves extending techniques commonly adopted in single-channel analyses to the multi-channel domain, by suitably summarising the information arising from all constituent channels. Within this first framework, two different approaches have been proposed.\nThe first consists of flattening the multi-channel data by combining the states (such as in the example in Figure 13.2) and treating the sequence as a single channel with an extended alphabet. Typically, one would then compute dissimilarities using this extended alphabet, most commonly using Optimal Matching (OM). However, this approach is not limited to distance-based clustering; any of the methods, distance-based or otherwise, that we have seen in the previous chapters related to sequence analysis can be used to cluster sequences represented in this way (e.g., aggregated hierarchical clustering using OM, or hidden Markov models). While this seems like an easier choice, the models tend to become too complex —in the sense of requiring a larger number of clusters and/or hidden states in order to adequately characterise the data— when the number of channels or numbers of per-channel states increase. Moreover, for even a moderate number of channels and/or moderate numbers of states per-channel, the size of the combined alphabet can become unwieldy. Indeed, using this approach in conjunction with OM has been criticised because of the difficulty of specifying appropriate substitution costs for such large combined alphabets [12]. (see Chapter 10 [3]).\nThe second, more common approach is explicitly distance-based and relies on an extension of the OM metric itself, by using a set of overall multi-channel costs derived from channel-specific costs, to calculate a pairwise dissimilarity matrix. The method relies on computing substitution costs for each constituent channel, combining the channels into a new multi-channel sequence object, and deriving an overall substitution cost matrix for the multi-channel object by adding (or averaging) the appropriate substitution costs across channels for a particular multi-state change, typically with equal weight given to each channel [13]. For instance, if the cost associated with substituting the states ‘Question’ and ‘Agreement’ in the utterances channel in Figure 13.1 is \\(0.6\\) and the cost of substituting ‘Happy’ and ‘Neutral’ in the facial expressions channel is \\(1.2\\), the cost associated with substitutions of ‘Question/Neutral’ for ‘Question/Happy’ and ‘Question/Neutral’ for ‘Agreement/Happy’ in Figure 13.2 would be \\(1.2\\) and \\(1.8\\), respectively. From such multi-channel costs, a pairwise multi-channel dissimilarity matrix can be obtained and all subsequent steps of the clustering analysis can proceed as per Chapter 10 thereafter.\nThat being said, adopting the extended OM approach rather than the extended alphabet approach does not resolve all limitations of the distance-based clustering paradigm. As Saqr et al. [14] noted, large sequences are hard to cluster using standard methods such as hierarchical clustering, which is memory inefficient and hard to parallelise or scale [15, 16]. Furthermore, distance-based clustering methods are limited by the theoretical maximum dimension of a matrix in R which is 2,147,483,647,  corresponding to a maximum of 46,430 sequences. Using weights can fix the memory issue if the number of unique sequences remains below the threshold [17]. However, the more states (and combinations of states), the more unique the sequences tend to be, so the memory issue is even more typical with multi-channel sequence data. In such a case, Markovian methods may be the solution. Moreover, the particular choice of distance-based clustering algorithm must be chosen with care, and as ever the user must pre-specify the number of clusters, thereby often necessitating the evaluation of several computing solutions with different numbers of clusters, different hierarchical clustering linkage criteria, or different distance-based clustering algorithms entirely.\n\n\n2.3.2 Mixture hidden Markov models\nEven so, the extended OM approach is still liable to overestimate the number of clusters. A second, more sophisticated option is to use mixture hidden Markov models (MHMM) [18], which we have already encountered in some detail in Chapter 12. Such models notably support both single- and multi-channel sequences and often lead to more parsimonious representations of the latent group structure than the distance-based paradigm. MHMMs are a thus a far more efficient and powerful method for temporally aligning the multiple channels of data into homogeneous temporal patterns. As we have learned in Chapter 12, when we estimate a mixture hidden Markov model, we need to specify the number of clusters (\\(K\\)) and we create \\(K\\) submodels that are each a hidden Markov model. Each submodel can have a different number of hidden states, which allows for great flexibility in modeling the variability of the data, without unduly inflating the number of clusters as per the distance-based approaches, thereby enabling more concise and interpretable characterisation of the distinct patterns in the data. Accommodating multi-channel sequences in the MHMM framework requires treating the observed states in each channel independently given the current hidden state. This can be easily performed by defining multiple emission probability matrices (one per channel). The assumption of conditional independence simplifies the model, but is sometimes unrealistic —if there are strong dependencies or interactions between the different channels—, in which case it is better to either analyse single-channel sequences (using extended alphabet) or resort to the distance-based paradigm described above.\nWe also learned in Chapter 12 that an advantage of MHMMs, as a probabilistic modelling approach, is that we can use traditional model selection methods such as likelihood-based information criteria or cross-validation for choosing the best model. For example, if the number of subpopulations is not known in advance —as is typically the case— we can compare models with different clustering solutions (be they those obtained with different numbers of clusters or different sets of initial probabilities, for example) and choose the best-fitting model with, for example, the Bayesian information criterion (BIC) [19]. Conversely, the distance-based paradigm relies on heuristic cluster quality measures for conducting model selection. In addition, an advantage of the distance-based approach is that it does not rely on making any assumptions about the data-generating mechanism. \nThe extended OM method for calculating multi-channel dissimilarities is implemented in the R package TraMineR and is applied in our case study, while the extended alphabet approach (i.e., combining all channels into a single one where the alphabet contains all possible state combinations) is not pursued any further here. We also cluster via the MHMM framework in our case study, using the R package seqHMM [20], in order to empirically compare and contrast both perspectives. It is worth noting that, of the Markovian methods we learned about in Chapter 12, only hidden Markov models and mixture hidden Markov models explicitly allow for multi-channel sequences by treating the observed states in each channel independently given the current hidden state. The more basic Markov and mixture Markov models can only accommodate multi-channel sequences by treating them as a single channel using the extended alphabet approach described above, but we do not consider this option any further here. To summarise, we do not explore the extended alphabet approach in either the distance-based or Markovian settings; we only demonstrate the multi-channel extension of the OM metric from the distance-based point of view, and only the MHMM approach from the Markovian point of view.\n\n\n\n2.4 Step 4: Relating clusters to covariates\nAs we have seen, there are, broadly speaking, two conflicting perspectives on clustering in the sequence analysis community; algorithmic, distance-based clustering on the one hand, and model-based clustering on the other. Distance-based clustering of multi-channel sequences has already been described above, but it is useful to acknowledge that mixture Markov models and mixture hidden Markov models are precisely examples of model-based clustering methods. Moreover, seqHMM and the methods it implements are unique in offering a model-based approach to clustering multi-channel sequences. A key advantage of the model-based clustering framework is that it can easily be extended to directly account for information available in the form of covariates, as part of the underlying probabilistic model. Though the recently introduced mixture of exponential-distance models framework [21] attempts to reconcile the distance-based and model-based cultures in sequence analysis, while allowing for the direct incorporation of covariates, it is not based on Markovian principles, and most importantly can currently only accommodate single-channel sequences. Otherwise, distance-based approaches typically cannot accommodate covariates, except as part of a post-hoc step whereby, typically, covariates are used as explanatory variables in a post-hoc multinomial regression with the cluster memberships used as the response (or as independent variables in linear or non-linear regression models). This is questionable as substituting a categorical variable indicating cluster membership disregards the heterogeneity within clusters and is clearly only sensible when the clusters are sufficiently homogeneous [22, 23]. It is often more desirable to incorporate covariates directly, i.e. to cluster sequences and relate the clusters to the covariates simultaneously. This represents an important distinction between the two approaches outlined above; this step does not apply to distance-based clustering approaches and is a crucial advantage of the Markovian approach which makes MHMMs even more powerful tools for clustering multi-channel sequences.\nIn theory, MHMMs can include covariates to explain the initial, transition, and/or emission probabilities. A natural use-case would be to allow subject-specific and possibly time-varying transition and emission probabilities (in the case of time-varying covariates). For example, if we think that transitioning from low to high achievement gets harder as the students get older we may add time as an explanatory variable to the model, allowing the probability of transitioning from low to high achievement to diminish in time. However, at the time of writing this chapter, these extensions are not supported in seqHMM (this may change in the future). Instead, the seqHMM package used in the examples supports time-constant covariates only. Specifically, the manner in which covariates are accommodated is similar in spirit to the aforementioned mixture of exponential-distance models framework and latent class regression [24], in that covariates are used for predicting cluster memberships for each observed sequence. Adding covariates to the model thus helps to both explain and influence the probabilities that each individual has of belonging to each cluster. By incorporating covariates in this way, we could, for example, find that being in a high-achievement cluster is predicted by gender, previous grades, or family background\nIn the terminology of mixtures of experts modelling, this approach corresponds to a so-called “gating network mixture of experts model”, whereby the distribution of the sequences depends on the latent cluster membership variable, which in turn depends on the covariates, while the sequences are independent of the covariates, conditional on the latent cluster membership variable [25]. This, rather than having covariates affect the component distributions, is particularly appealing, as the interpretation of state-specific parameters is the same as it would be under a model without covariates. However, it is worth noting that the covariates do not merely explain the uncovered clusters; as part of the model, they drive the formation of the clusters. In other words, an otherwise identical model without dependence on covariates may uncover different groupings with different probabilities. However, this can be easily overcome by treating the selection of the most relevant subset of covariates as an issue of model selection via the BIC or other criteria and taking the number of clusters/states, set of initial probabilities, and set of covariates which jointly optimise the chosen criterion. The incorporation of covariates in MHMMs and related practical issues are also illustrated in the case study on the longitudinal association of engagement and achievement which follows."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#review-of-the-literature",
    "href": "chapters/ch13-multichannel/ch13-multi.html#review-of-the-literature",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nThe use of multi-channel sequence analysis in learning analytics has so far been scarce. Most of the few studies that implemented this method used it to combine multiple modalities of data such as coded video data [26, 27], electrodermal activity [26, 27], clickstream/logged data [10, 28, 29], and assessment/achievement data [10, 29, 30]. The data were converted into different sequence channels using a variety of methods. For example, [26] manually coded video interactions as positive/negative/mixed to represent the emotional valence of individual interaction, and used a pre-defined threshold to discern between high and low arousal from electrodermal activity data. In the work by [29], two sequence channels were built from students’ daily tactics using the learning management system and an automated assessment tool for programming assignments respectively. The learning tactics were identified through hierarchical clustering of students’ trace log data in each environment. Similarly [10], created an engagement channel for each course in a study program based on students’ engagement states derived from the learning management system data, and an achievement state based on their grade tertiles. Another study [28] had five separate channels: the first channel represented the interactive dimension built from the social interactions through peer communications and online behaviours; the second channel represented the cognitive dimension constructed from students’ knowledge contributions at the superficial, medium, and deep levels; the third channel represented the regulative dimension which represented students’ regulation of their collaborative processes, including task understanding, goal setting and planning, as well as monitoring and reflection; the fourth channel represented the behavioural dimension which analysed students’ online behaviours, including resource management, concept mapping and observation, and the fifth channel represented the socio-emotional dimension, which included active listening and respect, encouraging participation and inclusion, as well as fostering cohesion.\nA common step in the existing works is the use of clustering techniques to detect unobserved patterns within the multi-channel sequences. Articles have relied on hidden Markov models to identify hidden states in the data. For instance, [29] found three hidden states consisting of learning strategies which combine the use of the learning management system and an automated assessment tool: an instruction-oriented strategy where students consume learning resources to learn how to code; an independent coding strategy where students relied on their knowledge or used the learning resources available to complete the assignments, and a dependent coding strategy where students mostly relied on the help forums to complete their programming assignments. Most studies go one step further and identify distinct trajectories based on such hidden states. For example, [27] found four clusters of socio-emotional interaction episodes (positive, negative, occasional regulation, frequent regulation), which differed in terms of fluctuation of affective states and activated regulation of learning. The authors of [28] found three types of group collaborative patterns: behaviour-oriented, communication-behaviour-synergistic, and communication-oriented. To the knowledge of the authors, no article has relied on the distance-based approach (commonly used in single-channel sequence analysis) to cluster multi-channel sequences. However, this approach has been used in social sciences [9] as well as in other disciplines [e.g., 31]."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#case-study-the-longitudinal-association-of-engagement-and-achievement",
    "href": "chapters/ch13-multichannel/ch13-multi.html#case-study-the-longitudinal-association-of-engagement-and-achievement",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "4 Case study: the longitudinal association of engagement and achievement",
    "text": "4 Case study: the longitudinal association of engagement and achievement\nTo learn how to implement multi-channel sequence analysis we are going to practice with a real case study: we will investigate the longitudinal association between engagement and achievement across a study program using simulated data based on the study by [10]. We begin by creating a sequence for each of the channels (engagement and achievement) and then explore visualisations thereof. We then focus on clustering these data using the methods described above specifically tailored for multi-channel sequences, in order to present an empirical view of both the distance-based and Markovian perspectives. In doing so, we will first demonstrate how to construct a multi-channel pairwise OM dissimilarity matrix in order to apply the distance-based clustering approach of Chapter 10 to these data. Secondly, we will largely focus on using a mixture hidden Markov model to detect the longitudinal clusters of students that share a similar trajectory of engagement and achievement. Finally, we will demonstrate how to incorporate covariates under the model-based Markovian framework.\n\n\n\n4.1 The packages\nTo accomplish our task, we will rely on several R packages We have use most of them throughout the book. Below is a brief summary of the packages required:\n\nrio: A package for reading and saving data files with different extensions [32].\nseqHMM: A package designed for fitting hidden (latent) Markov models and mixture hidden Markov models for social sequence data and other categorical time series [20].\ntidyverse: A package that encompasses several basic packages for data manipulation and wrangling [33].\nTraMineR: As seen in the introductory sequence analysis chapter, this package helps us construct, analyse, and visualise sequences from time-ordered states or events [34].\nWeightedCluster: A package to cluster sequences and computing quality measures [17].\n\nIf you have not done so already, install the packages using the install.packages() command (e.g., install.packages(\"seqHMM\")). We can then import them as follows:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(TraMineR)\nlibrary(seqHMM) \nlibrary(WeightedCluster) \n\n\n\n4.2 The data\nThe data that we are going to use in this chapter is a synthetic dataset which contains the engagement states (Active, Average, or Disengaged) and achievement states (Achiever, Intermediate, or Low) for eight successive courses (each course is a timepoint). Each row contains an identifier for the student (UserID), an identifier for the course (CourseId), and the order (Sequence) in which the student took that course (1–8). For each course, a student has both an engagement state (Engagement) and an achievement state (Achievement). For example, a student can be 'Active' and 'Achiever' in the first course (Sequence = 1) they take, and 'Disengaged' and 'Low' achiever in the next. In addition, the dataset contains the final grade (0-100) and three time-constant covariates (they are the same for each student throughout all four courses): the previous grade (Prev_grade, 1-10), Attitude (0-20), and Gender (Male/Female). The dataset is described in more detail in the Data chapter of this book. We can import the data using the ìmport() command from rio. A preview is shown in Table 13.1.\n\nURL <- \"https://github.com/sonsoleslp/labook-data/raw/main/\"\nfileName <- \"9_longitudinalEngagement/SequenceEngagementAchievement.xlsx\"\ndf <- import(paste0(URL, fileName))\ndf\n\n\n\n\n\n\n\nTable 1.  Preview of the data \n  \n    \n    \n      \n      UserID\n      CourseID\n      Sequence\n      Engagement\n      Final_Grade\n      Achievement\n      Prev_grade\n      Attitude\n      Gender\n    \n  \n  \n    1\n00050F0E\n4C3F37F0\n1\nAverage\n72.27\nAchiever\n6.826613\n12.92833\nMale\n    2\n00050F0E\nE54A52A3\n2\nDisengaged\n72.56\nAchiever\n6.826613\n12.92833\nMale\n    3\n00050F0E\nAB7EC624\n3\nAverage\n78.78\nAchiever\n6.826613\n12.92833\nMale\n    4\n00050F0E\nB0E95213\n4\nAverage\n74.19\nAchiever\n6.826613\n12.92833\nMale\n    5\n00050F0E\n0B301F55\n5\nAverage\n87.35\nAchiever\n6.826613\n12.92833\nMale\n    6..1135\n\n\n\n\n\n\n\n\n\n    1136\nFE2E4C85\n79ED6873\n8\nActive\n87.69\nAchiever\n4.189420\n20.00000\nMale\n  \n  \n  \n\n\n\n\n\n\n\n4.3 Creating the sequences\nWe are going to first construct and visualise each channel separately (engagement and achievement), and then study them in combination.\n\n4.3.1 Engagement channel\nTo build the engagement channel, we need to construct a sequence of students’ engagement states throughout the eight courses. To do so, we need to first arrange our data by student (UserID) and course order (Sequence). Then, we pivot the data to a wide format, where the first column is the UserID and the rest of the columns are the engagement states at each of the courses (ordered from 1 to 8). For more details about this process, refer to the introductory sequence analysis chapter.\n\neng_seq_df <- df |> arrange(UserID, Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"Engagement\")\n\nNow we can use TraMineR to construct the sequence object and assign colours to it to represent each of the engagement states:\n\nengagement_levels <- c(\"Active\", \"Average\", \"Disengaged\")\ncolours <- c(\"#28a41f\", \"#FFDF3C\", \"#e01a4f\")\neng_seq <- seqdef(eng_seq_df , 2:9, alphabet = engagement_levels, cpal = colours)\n\nWe can use the sequence distribution plot from TraMineR to visualise the distribution of each state at each time point, and the sequence index plot to visualise each student’s sequence of engagement states in Figure 13.3:\n\nseqdplot(eng_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(eng_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 3. Sequence distribution plot and index plot of the course engagement states.\n\n\n\n\nPlease, refer to Chapter 10 [3] for guidance on how to interpret sequence distribution and index plots.\n\n\n4.3.2 Achievement channel\nWe follow a similar process to construct the achievement sequences. First we arrange the data by student and course order and then we convert the data into the wider format using —this time— the achievement states as the values.\n\nach_seq_df <- df |> arrange(UserID,Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"Achievement\")\n\nWe use again TraMineR to construct the sequence object for achievement and we again provide a suitable, distinct colour scheme:\n\nachievement_levels <- c(\"Low\", \"Intermediate\", \"Achiever\")\n\ncoloursA <- c(\"#a5e3ff\",\"#309bff\",\"#3659bf\")\nach_seq <- seqdef(ach_seq_df , 2:9, cpal = coloursA, alphabet = achievement_levels)\n\nWe may use the same visualisations to plot our achievement sequences in Figure 13.4:\n\nseqdplot(ach_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(ach_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 4. Sequence distribution plot and index plot of the course achievement states.\n\n\n\n\n\n\n4.3.3 Visualising the multi-channel sequence\nNow that we have both channels, we can use the mc_to_sc_data() function from seqHMM to convert the two channels of engagement and achievement sequences to a single channel with an extended alphabet. \n\nmulti_seq <- mc_to_sc_data(list(eng_seq, ach_seq))\n\nIf we check the alphabet of the sequence, we can see that it contains all combinations of engagement and achievement states.\n\nalphabet(multi_seq)\n\n[1] \"Active/Achiever\"         \"Active/Intermediate\"    \n[3] \"Active/Low\"              \"Average/Achiever\"       \n[5] \"Average/Intermediate\"    \"Average/Low\"            \n[7] \"Disengaged/Achiever\"     \"Disengaged/Intermediate\"\n[9] \"Disengaged/Low\"         \n\n\nWe just need to provide an appropriate colour scale where green colours represent active students, blue colours represent averagely engaged students, and red colours represent disengaged students, whereas the intensity of the colour represents the achievement level: darker colours represent higher achievement, and lighter colours represent low achievement.\n\ncoloursM <-  c(\"#115a20\",\"#24b744\",\"#78FA94\",\n              \"#3659bf\",\"#309bff\",\"#93d9ff\",\n              \"#E01A1A\",\"#EB8A8A\",\"#F5BDBD\")\n\ncpal(multi_seq) <- coloursM\n\nWe can finally plot the sequence distribution and index plots for the multi-channel sequence (Figure 13.5). We can already hint that the students’ multi-channel sequences are quite heterogeneous. In the next steps, we will use two distinct clustering techniques to detect distinct combined trajectories of engagement and achievement, beginning with the distance-based paradigm and then the using the more sophisticated MHMM framework.\n\nseqdplot(multi_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(multi_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 5. Sequence distribution plot and index plot of the multi-channel sequence.\n\n\n\n\n\n\n\n4.4 Clustering via multi-channel dissimilarities\nWe now describe how to construct a dissimilarity matrix for a combined multi-channel sequence object to use as input for distance-based clustering algorithms, using utilities provided in the TraMineR R package and relying on concepts described in Chapter 10. We begin by creating a list of substitution cost matrices for each of the engagement and achievement channels. For simplicity, we adopt the same method of calculating substitution costs for each channel, though this need not be the case. Here, we use the data-driven TRATE method which relies on transition rates. \n\nsub_mats <- list(engagement = seqsubm(eng_seq, method = \"TRATE\"),\n                 achievement = seqsubm(ach_seq, method = \"TRATE\"))\n\nSubsequently, we invoke the function seqMD() to compute the multi-channel dissimilarities. This requires the constituent channels to be supplied as a list of sequence objects, along with the substitution cost matrices via the argument sm. The argument what = \"diss\" ensures that dissimilarities are returned, according to the OM method, with equal weights for each channel specified via the argument cweight.\n\nchannels <- list(engagement = eng_seq, achievement = ach_seq)\n\nmc_dist <- seqMD(channels, \n                 sm = sub_mats, \n                 what = \"diss\", \n                 method = \"OM\", \n                 cweight = c(1, 1))\n\nThereafter, mc_dist can be used as input to any distance-based clustering algorithm. Though we are not limited to doing so, we follow Chapter 10 in applying hierarchical clustering with Ward’s criterion. However, we note that users of the distance-based paradigm are not limited to Ward’s criterion or hierarchical clustering itself either.\n\nClustered <- hclust(as.dist(mc_dist), method = \"ward.D2\")\n\nWe then obtain the implied clusters by cutting the produced tree using the cutree() function, where the argument k indicated the desired number of clusters. We also create more descriptive labels for each cluster.\n\nCuts <- cutree(Clustered, k = 6)\nGroups <- factor(Cuts, labels = paste(\"Cluster\", 1:6))\n\nGenerally speaking, the resulting clusters for the chosen k value may not represent the best solution. In an ideal analysis, one would resort to one or more of the cluster quality measures given in Table 9 of Chapter 10 to inform the choice of an optimal k value. In doing so, we determined the chosen number of six clusters is optimal according to the average silhouette width (ASW) criterion therein (see Chapter 10). However, we omit the details of these steps here for brevity and leave them as an exercise for the interested reader.\nThe obtained Groups can be used to produce visualisations of the clustering solution. We do so in two ways; first showing the combined multi-channel object with an extended alphabet (as per Figure 13.2) using the seqplot() function from TraMiner in Figure 13.6, and second using a stacked representation of both constituent channels using the ssp() and gridplot() functions from seqHMM in Figure 13.7. State distribution plots per cluster are depicted in each case.\n\nseqplot(multi_seq, type = \"d\", group = Groups, \n         ncol = 3, legend.prop = 0.1, border = NA)\n\n\n\n\nFigure 6. State distribution plots per cluster for the combined multi-channel sequences.\n\n\n\n\nThough there is some evidence of well-defined clusters capturing different engagement and achievement levels over time, a plot such as this can be difficult to interpret, even for a small number of channels and small numbers of per-channel states. A more visually appealing alternative is provided by the stacked state distribution plots below, which shows each individual channel with its own alphabet on the same plot, with one such plot per cluster. This requires supplying a list of the observations ìn each cluster in each channel to the ssp() function to create each plot, with each plot then arranged and displayed appropriately by the function gridplot().\n\nssp_k <- list()\nfor(k in 1:6) {\n  ssp_k[[k]] <- ssp(list(eng_seq[Cuts == k,], \n                         ach_seq[Cuts == k,]),\n                    type = \"d\", border = NA, \n                    with.legend = FALSE, title = levels(Groups)[k])\n}\ngridplot(ssp_k, ncol = 3, byrow = TRUE, \n         row.prop = c(0.4, 0.4, 0.2), cex.legend = 0.8)\n\n\n\n\nFigure 7. Stacked state distribution plots per cluster showing both constituent channels.\n\n\n\n\nFrom this plot, we can see six clusters, relating respectively to 1) mostly averagely engaged high achievers, 2) mostly actively engaged intermediate achievers, 3) mostly disengaged intermediate or high achievement, 4) mostly averagely engaged low achievers, 5) mostly disengaged low achievers, and 6) mostly actively engage high achievers. This clustering approach and this set of plots provides quite a granular view of the group structure and interdependencies across channels, which differs somewhat from the clustering solution obtained by the application of MHMMs which now follows.\n (e.g., eng_seq_df) and assign the Groups variable as follows:\n\neng_seq_df$ClusterDistance = Groups\n\n\n\n4.5 Building a mixture hidden Markov model\nThe seqHMM package supports several types of (hidden) Markov models for single- or multi-channel sequences of data. The package provides functions for evaluating and comparing models, and methods for the visualisation of multi-channel sequence data and hidden Markov models. The models are estimated through maximum likelihood, using the Expectation Maximisation (EM) algorithm and/or direct numerical maximisation with analytical gradients. For more details on the seqHMM package, refer to the “Markov models” chapter. We are going to construct a mixture hidden Markov model (MHMM). For this purpose, we will rely on the build_mhmm() function of seqHMM. In its most basic form, the build_mhmm() function takes as arguments a list of observations (i.e., a list of the sequences that make up the channels), and a vector n_states whose length indicates the number of clusters to estimate, for which the value of each position specifies the number of hidden states in each cluster. The build_mhmm() function defines that structure of the model (including possible restrictions) and also assigns random or user-defined starting values to model parameters for estimation. Though we are not restricted to assuming all clusters have an equal number of hidden states, we are going to build a model with 3 clusters and 2 hidden states per cluster, inspired by the results obtained in the previous section (i.e., 6 clusters). Therefore, our input for n_states is c(2, 2, 2). If instead, we wanted to build a model with 4 clusters and 3 hidden states, we would need to provide c(3, 3, 3, 3).\n\ninit_mhmm <- build_mhmm(observations = list(eng_seq, ach_seq), \n                        n_states = c(2, 2, 2))\n\nNow we can fit the model using the fit_model() function. Using the EM algorithm and the times argument, we can provide a number to control the number of times the model should be fit in a hope to find the globally optimal solution. We need to find a number that is not too low (or else we might be settling for a non-optimal solution too early) but also not too high (or the code will take forever to run). If we provide 100, for example, the model will be estimated 101 times: once from the user-defined or random starting values at the build stage and then 100 re-estimations with randomized starting values. The function will give the best model as an output. Generally, the more complex the model, the more re-estimation rounds are needed.\n\nset.seed(2294)\nmhmm_fit <- fit_model(init_mhmm, \n                      control_em = list(restart = list(times = 100)))\n\nOnce the function has finished fitting the model, we need to check if it has likely reached the optimal solution. We can do so by checking the variable mhmm_fit$em_results$best_opt_restart of the fitted model. For the results to be reliable, we would expect to find the best-fitting model a number of times from different starting values. In other words, we want to see the same value repeating at the beginning of the sequence of numbers returned by mhmm_fit$em_results$best_opt_restart for several times if we estimate the model 100 times. In this case, we observe the same number quite many times, so the stability of our results is acceptable. If the same number does not appear several times, we can increase the number provided to the times argument or we can provide more informative starting values (for example, using the parameters of the newly fitted suboptimal model as starting values for the new estimation).\n\nmhmm_fit$em_results$best_opt_restart\n\n [1] -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712\n [8] -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712\n[15] -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712\n[22] -1659.712 -1659.712 -1659.712 -1659.712\n\n\nNow we can plot our results using the mssplot() function. Figure 13.8 shows index plots for each of the three fitted clusters. These plots show the categories at each time point within each cluster, as well as the hidden states. We can see that Cluster 1 corresponds to students who are mostly low achievers, wherein State 1 represents students who are disengaged and State 2 represents students who are averagely engaged. Cluster 2 includes the Active students who are mostly high achievers (State 2), or intermediate achievers (State 1). Cluster 3 represents mostly low achievers, where State 1 mostly represents the disengaged and State 2 the mostly averagely engaged. Therefore, the results are quite interpretable and make sense given our data.\n\nHIDDEN_STATES <- c(\"darkred\", \"pink\", \"darkblue\", \"purple\", \n                   \"lightgoldenrod1\", \"orange\")\n\nmssplot(mhmm_fit$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 8. Multi-channel sequence index plots.\n\n\n\nWe may also plot the transitions between the hidden states for each cluster using the plot() function (Figure 13.9). Most arguments used here are purely cosmetic in nature, with the exception of combine.slice which corresponds to the highest probability for which emission probabilities are combined into one state. We adopt the default value of 0.05 to aid the legibility of the plots. We see that the probabilities of starting at each hidden state (initial probabilities), as shown at the bottom of each pie chart, are quite balanced: 0.61/0.39, 0.58/0.42, 0.62/0.38. The transition probabilities (i.e., the probabilities of transitioning from one hidden state to another within the same cluster) are in turn quite low (all < 0.07), meaning that in most cases students remain in the same hidden state throughout their whole academic trajectory. Lastly, the emission probabilities (i.e., the probabilities of each state within each hidden state) are quite diverse, where some hidden states are quite homogeneous (e.g, State 2 in Cluster 2 has a 0.84 probability of having Disengaged students) whereas others are more evenly distributed among all states (e.g., State 1 in Cluster 3).\n\nplot(mhmm_fit$model, \n     vertex.size = 60,\n     cpal = coloursM, \n     label.color = \"black\",\n     vertex.label.color = \"black\",\n     edge.color = \"lightgray\",\n     edge.label.color = \"black\",\n     ncol.legend = 1, \n     ncol = 3,\n     rescale = FALSE,\n     interactive = FALSE,\n     combine.slices = 0.05,\n     combined.slice.label = \"States with probability < 0.05\")\n\n\n\n\nFigure 9. Transitions between hidden states for each trajectory.\n\n\n\n\nIn the previous example, we have used build_mhmm() with the default values in which we only need to provide the number of clusters and hidden states. However, the function allows much more flexibility by allowing us to constrain the different probabilities (initial, transition, and emission) that we have just talked about. An interesting case study can be to investigate how students who start in the same hidden state evolve throughout their academic trajectory; whether they remain in the same hidden state or switch to a different one. To do that, we would need to fix the initial probabilities to be 1 for the first hidden state, and 0 for the other. To do this for the three clusters, we need to create a list with the vector c(1, 0) repeated three times. For the initial transition and emission probabilities, we can just simulate random ones using the functions simulate_transition_probs() and simulate_emission_probs() respectively. \n\ninitial_probs <- list(c(1, 0), c(1, 0), c(1, 0))\ntransition_probs <- simulate_transition_probs(n_states = 2, \n                                              n_clusters = 3, \n                                              left_right = TRUE)\nemission_probs <- simulate_emission_probs(n_states = 2, \n                                          n_symbols = 3, \n                                          n_clusters = 1)\n\nWe must now provide the probability objects to the build_mhmm() function along with the observations. Since the number of clusters and hidden states can be derived from the probabilities, we no longer need to provide the n_states argument. Specifically, this is because emission_probs is given as a list of length 3 with each element being a list of length 2, corresponding to three clusters each with two hidden states per cluster, as before. This new model is fitted in the same way, by using the fit_model() function.\n\nset.seed(2294)\ninit_emission_probs <- list(list(emission_probs, emission_probs),\n                            list(emission_probs, emission_probs),\n                            list(emission_probs, emission_probs))\ninit_mhmm_i <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = initial_probs, \n                          transition_probs = transition_probs, \n                          emission_probs = init_emission_probs)\n\nmhmm_fit_i <- fit_model(init_mhmm_i, \n                        control_em = list(restart = list(times = 200)))\n\nAgain, we need to check whether the model has converged to a valid solution by checking if the same number is repeated at the beginning of the best_opt_start output:\n\nmhmm_fit_i$em_results$best_opt_restart\n\n [1] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463\n [8] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463\n[15] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1735.624\n[22] -1735.624 -1735.624 -1735.624 -1735.624\n\n\nNow that we have checked that our model is correct, we can give the channels and clusters representative names so that they look better when plotting:\n\nmhmm_fit_i$model$cluster_names <- c(\"Trajectory 1\",\n                                    \"Trajectory 2\", \n                                    \"Trajectory 3\")\nmhmm_fit_i$model$channel_names <- c(\"Engagement\",\n                                    \"Achievement\")\n\nNow we may plot our results, as before, in the form of sequence index plots (see Figure 13.10 (a), Figure 13.10 (b), and Figure 13.10 (c)). We see that —due to the specified initial probabilities— all clusters start with the same hidden state and mostly remain there throughout the whole trajectory, although some students switch to the other hidden state as time advances. In Cluster 1, students are mostly low achievers and start by being mostly averagely engaged (State 1), while some transition to disengaged (State 2). In Cluster 2, students are mostly highly engaged and start being high achievers (State 1), while some students transition to being mostly intermediate achievers (State 2). Lastly, in Cluster 3, students are mostly averagely engaged or disengaged high or intermediate achievers (State 1), while a small fraction of students become more averagely engaged and more highly achieving over time (State 2).\n\nHIDDEN_STATES <- c(\"darkred\", \"pink\", \"darkblue\", \"purple\", \n                   \"lightgoldenrod1\", \"orange\")\n\nmssplot(mhmm_fit_i$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Trajectory 1.\n\n\n\n\n\n\n\n(b) Trajectory 2.\n\n\n\n\n\n\n\n(c) Trajectory 3.\n\n\n\n\nFigure 10. Multi-channel sequence index plots with fixed start.\n\n\n\nThere are countless combinations of numbers of clusters, hidden states, and initial/transition/emission probabilities that we can use to fit our model. The choice depends on our data, our research question, the interpretability of the results, and the goodness of fit. Regarding the latter, a common way to compare the performance of several models is by using the Bayesian Information Criterion (BIC; [19]). The BIC can help us score the models which better fit our data while penalising us if we overfit it by adding too many parameters (e.g, if we specified 50 clusters instead of 3, we would better capture the variability of our data but our model would hardly generalise to other scenarios). Therefore, as a general rule, the lower the BIC the better the model, although we need to take into account the aforementioned issues (research questions, interpretability, etc.) to make the final choice. Below we show an example of how to calculate the BIC of the models we have estimated. We see that the first model with the fixed initial probabilities has a somewhat higher BIC than the second one with the restricted initial probabilities. However, we might need to try different numbers of clusters and hidden states to make our final choice.\n\nBIC(mhmm_fit$model)\n\n[1] 3565.658\n\nBIC(mhmm_fit_i$model)\n\n[1] 3656.949\n\n\n\n\n4.6 Incorporating covariates in MHMMs\nThe BIC can also be used to select the optimal model when different candidate models with different subsets of covariates are fitted. Recall that the clustering and the relation of clusters to covariates is performed simultaneously. Thus, otherwise identical models without dependence on covariates, or using different covariates, may uncover different groupings with different probabilities and hence a different BIC score. Here, we take the optimal model –with three clusters, each with two hidden states, using fixed initial probabilities– and demonstrate how to add and select covariates to make the model more powerful and interpretable. We begin by extract the covariate information from the original, long format df. For each student, the available time-constant covariates are a numeric variable giving their previous grades from an earlier course, a numeric measure of their attitude towards learning, and their gender (male or female). The order of the rows of cov_df matches the order in the two eng_seq and ach_seq channels.\n\ncov_df <- df |> \n  arrange(UserID, Sequence) |> \n  filter(!duplicated(UserID)) |> \n  select(Prev_grade, Attitude, Gender)\n\nThe code below replicates the code used to fit the previously optimal MHMM model, and augments it by providing a data frame (via the data argument, using the cov_df just created) and the corresponding one-sided formula for specifying the covariates to include (via the formula argument). For simplicity, we keep the same number of clusters and hidden states and fit three models (one for each covariate in cov_df). To mitigate against convergence issues, we use the results of the previously optimal MHMM to provide informative starting values.\n\nset.seed(2294)\ninit_mhmm_1 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Prev_grade)\ninit_mhmm_2 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Attitude)\ninit_mhmm_3 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Gender)\n\nWe estimate each model 50 + 1 times (first from the starting values we provided and then from 50 randomised values).\n\nmhmm_fit_1 <- fit_model(init_mhmm_1, \n                        control_em = list(restart = list(times = 50)))\nmhmm_fit_2 <- fit_model(init_mhmm_2, \n                        control_em = list(restart = list(times = 50)))\nmhmm_fit_3 <- fit_model(init_mhmm_3, \n                        control_em = list(restart = list(times = 50)))\n\nNote that in an ideal analysis, however, one would vary the number of clusters and hidden states along with varying sets of covariates and initial probabilities in order to find the model settings which jointly optimise the BIC. Moreover, one is not limited to including only a single covariate; one could specify for example formula = ~ Attitude + Gender, but we omit this consideration here for simplicity. Furthermore, one should check in an ideal analysis that the optimal model was found in a majority of the 51 runs. We did so and can be satisfied that there are no convergence issues. We select among the three models with three different, single covariates using the BIC, and include the previous model with no covariates in the comparison also.\n\nBIC(mhmm_fit_i$model)\n\n[1] 3656.949\n\nBIC(mhmm_fit_1$model)\n\n[1] 3614.673\n\nBIC(mhmm_fit_2$model)\n\n[1] 3632.004\n\nBIC(mhmm_fit_3$model)\n\n[1] 3649.353\n\n\nRecalling that lower BIC values are indicative of a better model, we can see that each covariate yields a better-fitting model. Though a better model could be found by adding more covariates or modifying the numbers of clusters and hidden states, we now proceed to interrogate, interpret, and visualise the results of the best model only; namely the MHMM with Prev_grade as a predictor of cluster membership.\nWe begin by examining the clusters via sequence index plots using mssplot(), as before, in Figure 13.11.\n\nmssplot(mhmm_fit_1$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 11. Multi-channel sequence index plots for the optimal covariate-dependent MHMM.\n\n\n\nCompared to Figure 13.10, the results are notably similar. One observation previously assigned to the first trajectory is now assigned to the third, but the clusters retain precisely the same interpretations of Cluster 1 being mostly low achievers who move from averagely engaged to disengaged, Cluster 2 being mostly highly engaged high achievers, some of whom transition to being mostly intermediate achievers, and Cluster 3 being most averagely engaged or disengaged high or intermediate achievers, some of whom become more averagely engaged and highly achieving over time. It is worth noting that it is possible for the clusters and the interpretation thereof to change more drastically than this when covariates are added. As the clusters are very similar to what we found before, we give the clusters more informative labels as a reminder:\n\ncluster_names(mhmm_fit_1$model) <- c(\"LowEngLowAch\", \"HighEngHighAch\", \"LowEngHighAch\")\n\nWe now present a summary of the model which shows information about parameter estimates of covariates and prior and posterior cluster membership probabilities (these refer to cluster membership probabilities before or after conditioning on the observed sequences, respectively). \n\n\nsummary_mhmm_1 <- summary(mhmm_fit_1$model)\nsummary_mhmm_1\n\nCovariate effects :\nLowEngLowAch is the reference.\n\nHighEngHighAch :\n             Estimate  Std. error\n(Intercept)    -6.253       1.220\nPrev_grade      0.817       0.159\n\nLowEngHighAch :\n             Estimate  Std. error\n(Intercept)    -1.486       0.960\nPrev_grade      0.158       0.139\n\nLog-likelihood: -1708.843   BIC: 3614.673 \n\nMeans of prior cluster probabilities :\n  LowEngLowAch HighEngHighAch  LowEngHighAch \n          0.40           0.34           0.26 \n\nMost probable clusters :\n            LowEngLowAch  HighEngHighAch  LowEngHighAch\ncount                 57              48             37\nproportion         0.401           0.338          0.261\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n               LowEngLowAch HighEngHighAch LowEngHighAch\nLowEngLowAch        0.98577        0.00702       0.00721\nHighEngHighAch      0.00316        0.98499       0.01185\nLowEngHighAch       0.01335        0.01552       0.97113\n\n\nWe will first interpret the information on prior and posterior cluster membership probabilities and then proceed to interpreting covariate effects. Firstly, the section named Means of prior cluster probabilities gives information on how likely each cluster is in the whole population of students (40% in “LowEngLowAch”, 34% in “HighEngHighAch”, and 26% in “LowEngHighAch”). Secondly, the section Most probable clusters shows group sizes and proportions considering that each student would be classified into the cluster for which they have the highest cluster membership probability. Thirdly, the Classification table shows mean cluster probabilities (in columns) by the most probable cluster (in rows). We can see that the clusters are very crisp (the certainty of cluster memberships are very high) because the membership probabilities are large in the diagonal of the table.\nThe part titled Covariate effects shows the parameter estimates for the covariates. Interpretation of the values is similar to that of multinomial logistic regression, meaning that we can interpret the direction and uncertainty of the effect –relative to the reference level of “LowEngLowAch” – but we cannot directly interpret the magnitude of the effects. The magnitude of the Prev_grade coefficient is small with respect to its associated standard error, so we can say there is no significant relationship between previous grades and membership of the poorly engaged yet highly achieving “LowEngHighAch”. However, the large positive coefficient in “HighEngHighAch” indicates that students with high previous grades are more likely to belong to the highly engaged high achieving cluster, which makes intuitive sense.\nThe summary object also calculates prior and posterior cluster memberships for each student. We omit them here, for brevity, but demonstrate that they can be obtained as follows:\n\nprior_prob <- summary_mhmm_1$prior_cluster_probabilities\nposterior_prob <- summary_mhmm_1$posterior_cluster_probabilities\n\naccess the most_probable_cluster element from the summary object (summary_mhmm_1) and assign it to a dataframe in which the students are in the same order as in the dataframes used to create the sequence objects (eng_seq_df or ach_seq_df).\n\neng_seq_df$ClusterMHMM <- summary_mhmm_1$most_probable_cluster\n\nthe hidden_paths() function from seqHMM and pass the MHMM model as an argument (mhmm_fit_1$model). We then need to convert it to the long format to get a row per student and timepoint and then we extract only the value column to get only the hidden state. We can then assign the resulting vector to a column (e.g., HiddenState) in the original dataframe which follows the same data order.\n\ndf$HiddenState <- hidden_paths(mhmm_fit_1$model) |> pivot_longer(everything()) |> pull(value)\n\nLastly, we advise readers to consult Chapter 12 for more examples of interpreting covariate effects in the context of MHMM models."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#discussion",
    "href": "chapters/ch13-multichannel/ch13-multi.html#discussion",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "5 Discussion",
    "text": "5 Discussion\nThis chapter has delved into the intriguing world of multi-channel sequence analysis, a method that deals with the analysis of two or more synchronised sequences. The ability to analyse multiple sequences in tandem offers a valuable perspective to understand the complexities of a temporal process. Throughout this chapter, we have highlighted the growing significance of multi-channel sequence analysis in educational research. While earlier chapters in this book focused on sequence analysis of learner-related data, this method presents an innovative approach to leverage the increasing availability of student multimodal temporal data. Our review of the existing literature that has utilised multi-channel sequence analysis concluded that the applications of this method within educational research are still relatively scarce. However, the potential of this framework is immense, as it opens up new possibilities to address the challenges posed by the complex and dynamic nature of educational data.\nBy exploring a case study on students’ online engagement and academic achievement, we demonstrated how multi-channel sequence analysis can reveal valuable insights into the longitudinal associations between these constructs. The identification of distinct trajectories through mixture hidden Markov models allows for a deep understanding of how both constructs evolve together over time and the distance-based clustering approach offers an alternative perspective on the group structure within these data. The step-by-step tutorial on implementing multi-channel sequence analysis with R according to both clustering frameworks provides readers with practical guidance to explore and apply these methods in their own research. Armed with this knowledge, researchers and educators can unlock valuable insights from their data and make informed decisions to support student learning and success.\nMarkovian models are relatively scalable and can be used to cluster large sequence data. Then again, unlike data-mining type sequence analysis, probabilistic models require making assumptions about the data-generating mechanisms which may not always be realistic. Furthermore, analysis of complex multi-channel sequence data can be very time consuming. Although we have highlighted the advantages of the model-based Markovian approach, particularly with regard to the ability to incorporate covariates as predictors of cluster membership, it is difficult to say if one approach is definitively better than the other for all potential use-cases. Although the BIC can be used to guide the choice of MHMM, particularly regarding the choice of the number of clusters (and numbers of hidden states within each cluster, and/or initial/transition/emission probabilities), we did not comprehensively do so here. Rather, we opted for three clusters, each with two hidden states, in both applications of MHMM herein, and only considered different initial probability settings and different covariates to arrive at the best model in terms of BIC. In an ideal analysis, one would compare different MHMM fits with different numbers of clusters and hidden states, different restrictions on the probabilities, and different covariates to arrive at an optimal model in terms of BIC.\nConversely, the choice of six clusters for the hierarchical clustering solution was arrived at in a different fashion, driven by the average silhouette width cluster quality measure. Indeed, the BIC cannot be used to choose the number of clusters with hierarchical clustering, nor can it be used to compare a hierarchical clustering solution with MHMM. However, it is interesting to note that the six clusters obtained by hierarchical clustering map reasonably well to the six hidden states of the MHMM solutions. As examples, considering the MHMM with fixed initial probabilities, one could say that clusters 4 and 5 of the hierarchical clustering solution, capturing averagely engaged and disengaged low achievers respectively, matches the two hidden states of the first MHMM cluster, while clusters 2 and 6, capturing actively engaged intermediate and high achievers respectively, matches the two hidden states of the second MHMM cluster.\nOverall, we encourage readers to further explore the potential of multi-channel sequence analysis, the merits of both clustering approaches, and broader implications in the field of education by diving into the recommended readings. Together, we can harness the power of this framework to create meaningful and lasting impacts on student learning and educational research as a whole. The next section provides a list of valuable resources for readers to expand their knowledge on the topic."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#further-readings",
    "href": "chapters/ch13-multichannel/ch13-multi.html#further-readings",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "6 Further readings",
    "text": "6 Further readings\nFor more thorough presentations and comparisons of different options for the distance-based analysis of multi-channel sequences, we recommend the text book by Struffolino and Raab [35] and articles by Emery and Berchtold [8] and Ritschard et al. [12]. For assessing whether sequences in different channels are associated to the extent of needing joint analysis, Piccarreta and Elzinga [36, 37] have proposed solutions for quantifying the extent of association between two or more channels using Cronbach’s \\(\\alpha\\) and principal component analysis.\nFor general presentations of the MHMM for multi-channel sequences, see the chapter by Vermunt et al. [38] and the article by Helske and Helske [18]. For further examples and tips for visualization of multi-channel sequence data and estimation of Markovian models with the seqHMM package, see the seqHMM vignettes [39, 40].\nTo learn how to combine distance-based sequence analysis and hidden Markov models for the analysis of complex multi-channel sequence data with partially missing observations, see the chapter by Helske et al. [6]."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#acknowledgements",
    "href": "chapters/ch13-multichannel/ch13-multi.html#acknowledgements",
    "title": "13  Chapter 13. Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "7 Acknowledgements",
    "text": "7 Acknowledgements\nSH was supported by the Academy of Finland (decision number 331816), the Academy of Finland Flagship Programme (decision number 320162), and the Strategic Research Council (SRC), FLUX consortium (decision numbers: 345130 and 345130). MS was supported by the Academy of Finland (TOPEILA: Towards precision education: Idiographic learning analytics, grant 350560)."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html",
    "href": "chapters/ch14-process-mining/ch14-process.html",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#introduction",
    "href": "chapters/ch14-process-mining/ch14-process.html#introduction",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn today’s digital age, almost all learning platforms generate vast amounts of data that include every interaction a student has with the learning environment. This treasure trove of data offers a unique opportunity to analyze and understand the dynamics of the learning process. In the previous chapters of the book, we covered several methods for analyzing the temporal aspects of learning, such as sequence analysis [1, 2], Markovian modeling [3] or temporal network analysis [4]. In this chapter, we present an analytical method that is specifically oriented at analyzing time-stamped event log data: process mining. Process mining is a technique that allows us to discover, visualize, and analyze the underlying process from time-stamped event logs. Through process mining, we may uncover hidden patterns, bottlenecks, and inefficiencies in students’ learning journeys. By tracking students’ actions step-by-step, we can identify which resources are most effective, which topics are more challenging, and even predict possible problems before they may occur.\nProcess mining emerged as a business tool that allows organizations to analyze and improve their operational processes. The field has rapidly expanded with several modelling methods, algorithms, tools and visualization techniques. Further, the intuitive method has been met with enthusiasm from several researchers leading to a rapid uptake by other disciplines such as health care management and education. As the field currently stands, it is a blend of process management and data science with less emphasis on statistical methods. The field has found its place in educational research with the recent surge of trace log data generated by students’ activities and the interest that learning analytics and educational data mining have kindled.\nThis tutorial chapter will introduce the reader to the fundamental concepts of process mining technique and its application in the realm of learning analytics. We will first describe the method, the main terminology, and the common steps for analysis. Then, we will provide a review of related literature to gain an understanding of how this method has been used in learning analytics research. We then provide a step-by-step tutorial on process mining using the R programming language and the bupaverse framework. In this tutorial, we analyze a case study of students’ online activities in an online learning management system using the main features of process mining."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#basic-steps-in-process-mining",
    "href": "chapters/ch14-process-mining/ch14-process.html#basic-steps-in-process-mining",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "2 Basic steps in process mining",
    "text": "2 Basic steps in process mining\nThe goal of process mining is to extract process models from event data. The resulting models can then be used to portray students’ pathways in learning, identify common transitions, and find issues of their approach. In doing so, process mining promises to find deviations from the norm, suggest corrective actions, and optimize processes as an ultimate goal [5]. Process mining starts by the extraction of event data. In the case of educational process mining, event data often reflects students’ activities in learning management systems (LMSs), or in other types of digital tools that record time-stamped events of students’ interactions with the digital tools, such as automated assessment tools or online learning games. The said data is used to construct what is known as an event log. An event log has three necessary parts:\n\nCase identifier: A case represents the subject of the process. For example, if we are analyzing students’ enrollment process, each student would represent a different case. If we are analyzing students’ online activities in the LMS, we can also consider each student as a separate case; alternatively, if we want a greater level of granularity, each student’s learning session can be then considered a separate case. All event logs need to have a case identifier that unequivocally identifies each case and that allows to group together all the events that belong to the same case.\nActivity identifier: Activities represent each action or event in the event data. Continuing with the previous examples, an activity would represent each step in the enrollment process (e.g, application, revision, acceptance, payment etc.), or each action in the LMS (e.g., watch video, read instructions, or check calendar).\nTimestamp: The timestamp is a record of the moment each event has taken place. It allows to establish the order of the events. In the case of online activity data, for instance, the timestamp would be the instant in which a student clicks on a learning resource. In some occasions, activities are not instantaneous, but rather have a beginning and an end. For example, if we are analyzing student’s video watching, we might have an event record when they start watching and when they finish. If we want to treat these events as parts of the same activity, we need to provide additional information when constructing an event log. As such, we need to specify an activity instance identifier, which would allow us to unequivocally identify and group together all instances of the same overarching activity (watching a video, in our example). Moreover, we would need to provide a lifecycle identifier (e.g., start and end), to differentiate between all stages of the same activity. A common limitation of LMS data is that only one click is recorded per activity so this type of analysis is often not possible.\n\nOnce we have our event log defined, we can calculate multiple metrics that allow us to understand the data. For example, we can see the most frequent activities and the most frequent transitions. We can also see the case coverage for each activity, e.g., how many cases contain each activity, and the distribution of the length of each case (how many activities they have). We can also calculate performance metrics, such as idle time (i.e., time spent without doing any activities) or throughput time (i.e., overall time taken).\nFrom the event log, we often construct what is known as the Directly-Follows Graph (DFG), in which we graphically represent all the activities in the event log as nodes and all the observed transitions between them as edges [5]. Figure 14.1 shows an example with event log data from students, where each case identifier represents a student’s session. First, we build the sequence of activities for each case. As such, Layla’s path for her first learning session would be: Calendar → Lecture → Video → Assignment. The path for Sophia’s first session would be: Calendar → Instructions → Video → Assignment. We put both paths together and construct a partial DFG that starts from Calendar, then it transitions either to Lecture or Instructions and then it converges back into Video and ends in Assignment. We create the paths for the remaining student sessions. Then, we combine them together through an iterative process until we have the complete graph with all the possible transitions between activities. Our final graph with the four sessions shown in Figure 14.1 would start by Calendar, then transition either to Lecture or Instructions. Then the process could transition from Lecture to Instructions or viceversa, or to Video. In addition, Lecture has a self-loop because it can trigger another lecture (see Layla’s session 2). From Video, the only possible transition is to Assignment.\nIn real-life scenarios, building the DFG for a complete —or large— event log may turn to be overcrowded and hard to visualize or interpret [5]. Therefore, it is common to trim activities that do not occur often. Other options include splitting the event logs by group (e.g., class sections) to reducing the granularity of the event log to be able to compare processes between groups. We can also zoom into specific parts of the course (e.g, a specific assignment or lecture) to better understand students’ activities at that time. Moreover, we can filter the event log to see cases that contain specific activities or that match certain conditions.\n\n\n\nFigure 1. Step-by-step construction of a DFG\n\n\nThe DFGs are often enhanced with labels that allow us to understand the graph better. These labels are often based on the frequency of activities and transitions. For example, the nodes (representing each activity) can be labeled with the number of times (or proportion) they appear in the event data, or with the case coverage, i.e., how many cases (or what percentage thereof) they appear in. The edges (representing transitions between activities) can be labeled with the frequency of the transitions or the case coverage as well, among others. Another common way of labeling the graph is using performance measures that indicate the mean time (or median, maximum, etc.) taken by each activity and/or transition.\nThe DFG gives us an overarching view of the whole event log. However, in some occasions, we would like to understand the underlying process that is common to most cases of our event log. This step is called process discovery [5] and there are several algorithms to perform it such as the alpha algorithm [6], inductive techniques [7] or region-based approaches [8]. The discovered processes are then represented using specialized notation, such as Business Process Model and Notation (BPMN) [9] or Petri nets [10].\nIn some occasions, there are certain expectations regarding how the process should go. For instance, if we are analyzing students’ activities during a lesson, the teacher might have given a list of instructions that students are expected to follow in order. To make sure that the discovered process (i.e., what students’ data reveal) aligns with the expected process (i.e., what students were told to do in our example), conformance checking is usually performed. Conformance checking deals with comparing the discovered process with an optimal model or theorized process (i.e., an ideal process) [5]. The idea is to find similarities and differences between the model process and the real observed process, identify unwanted behavior, detect outliers, etc. As seen from our example, in educational settings, this can be used, for instance, to detect whether students are following the learning materials in the intended order or whether they implement the different phases of self-regulated learning. However, given that students are rarely asked to access learning materials in a strict sequential way, this feature has been rarely used. In the next section, we present a review of the literature on educational process mining where we discuss more examples."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#review-of-the-literature",
    "href": "chapters/ch14-process-mining/ch14-process.html#review-of-the-literature",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nA review of the literature by Bogarín et al. [11] mapped the landscape of educational process mining and found a multitude of applications of this technique in a diversity of contexts. The most common application was to investigate the sequence of students’ activities in online environments such as MOOCs and other online or blended courses, as well as in computer-supported collaborative learning contexts [11]. One of the main aims was to detect learning difficulties to be able to provide better support for students. For example, López-Pernas et al. [12] used process mining to explore how students’ transition between a learning management system and an automated assessment tool, and identified how struggling students make use of the resources to solve their problems. Arpasat et al. [13] used process mining to study students’ activities in a MOOC, and compared the behavior of high- and low-achieving students in terms of students’ activities, bottlenecks and time performance. A considerable volume of research has studied processes where the events are instantaneous, such as clicks in online learning management systems (e.g., [14–16]). Fewer are the studies that have used activities with a start time and an end time due to the limitations in data collection in online platforms. However, this limitation has been often overcome by grouping clicks into learning sessions, as is often done in the literature on students’ learning tactics and strategies, or self-regulated learning (e.g., [12, 17–19]).\nRegarding the methods used, much of the existing research is limited to calculating performance metrics and visualizing DFGs, whereby researchers attempt to understand the most commonly performed activities and the common transitions between them. For example, Vartiainen et al. [20] used video coded data of students’ participation in an educational escape room to visualize the transitions between in-game activities using DFGs. Oftentimes, researchers use DFGs to compare (visually) across groups, for example high vs. low achievers, or between clusters obtained through different methods. For instance, Saqr et al. [21] implemented k-means clustering to group students according to their online activity frequency, and used DFGs to understand the strategies adopted by the different types of learners and how they navigate their learning process. Using a different approach, Saqr and López-Pernas [22] clustered students groups according to their sequence of interactions using distance-based clustering, and then compared the transitions between different interactions among the clusters using DFGs.\nGoing one step further, several studies have used process discovery to detect the underlying overall process behind the observed data [11]. A variety of algorithms have been used in the literature for this purpose, such as the alpha algorithm [23], the heuristic algorithm [24], or the fuzzy miner [18]. Less often, research on educational proecss mining has performed conformance checks [11], comparing the observed process with an “ideal” or “designed” one. An example is the work by Pechenizkiy et al. [25], who used conformance checking to verify whether students answered an exam’s questions in the order specified by the teacher.\nWhen it comes to the tools used for process mining, researchers have relied on various point-and-click pieces of software [11]. For example, Disco [26] has been used for DFG visualization by several articles (e.g., [27]). ProM [28] is the dominant technology when it comes to process discovery (e.g., [19, 29]) and also conformance checking (e.g., [25]). Many articles have used the R programming language to conduct process mining, relying on the bupaverse [30] framework for basic metrics and visualization (the one covered in the present chapter), although not for process discovery (e.g., [12]) since the algorithm support is scarce."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#process-mining-with-r",
    "href": "chapters/ch14-process-mining/ch14-process.html#process-mining-with-r",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "4 Process mining with R",
    "text": "4 Process mining with R\nIn this section, we present a step-by-step tutorial with R on how to conduct process mining of learners’ data. First, we will install and load the necessary libraries. Then, we will present the data that we will use to illustrate the process mining method.\n\n4.1 The libraries\nAs a first step, we need two basic libraries that we have used multiple times throughout the book: rio for importing the data [31], and tidyverse for data manipulation [32]. As for the libraries used for process mining, we will first rely on bupaverse, a meta-package that contains many relevant libraries for this purpose (e.g., bupaR), which will help us with the frequentist approach [30]. We will use processanimateR to see a play-by-play animated representation of our event data. You can install the packages with the following commands:\n\ninstall.packages(\"rio\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"bupaverse\")\ninstall.packages(\"processanimateR\")\n\nYou can then load the packages using the library() function.\n\nlibrary(bupaverse)\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(processanimateR)\n\n\n\n4.2 Importing the data\nThe dataset that we are going to analyze with process mining contains logs of students’ online activities in an LMS during their participation on a course about learning analytics. We will also make use of students’ grades data to compare activities between high and low achievers. More information about the dataset can be found in the data chapter of this book [33]. In the following code chunk, we download students’ event and demographic data and we merge them together into the same dataframe (df). \n\ndf <- import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\") \nall <- import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/AllCombined.xlsx\") |> \n  select(User, AchievingGroup)\ndf <- df |> merge(all, by.x = \"user\", by.y = \"User\")\n\nWhen analyzing students’ learning event data, we are often interested in analyzing each learning session separately, rather than considering a longer time span (e.g., a whole course). A learning session is a sequence (or episode) of un-interrupted learning events. To do such grouping, we define a threshold of inactivity, after which, new activities are considered to belong to a new episode of learning or session. In the following code, we group students’ logs into learning sessions considering a threshold of 15 minutes (15 min. × 60 sec./min. = 900 seconds), in a way that each session will have its own session identifier (session_id). For a step-by-step explanation of the sessions, code and rationale, please refer to the sequence analysis chapter [1]. A preview of the resulting dataframe can be seen below. We see that each group of logs that are less than 900 seconds (15 minutes) apart (Time_gap column) are within the same session (new_session = FALSE) and thus have the same session_id. Logs that are more than 900 seconds apart are considered a new session (new_session = TRUE) and get a new session_id.\n\nsessioned_data <- df |>\n  group_by(user) |> \n  arrange(user, timecreated) |>  \n  mutate(Time_gap = timecreated - (lag(timecreated))) |>  \n  mutate(new_session = is.na(Time_gap) | Time_gap > 900) |> \n  mutate(session_nr = cumsum(new_session)) |> \n  mutate(session_id = paste0 (user, \"_\", \"Session_\", session_nr)) |> ungroup()\n\nsessioned_data\n\n\n\n  \n\n\n\n\n4.2.1 Creating an event log\nNow, we need to prepare the data for process mining. This is performed by converting the data into an event log. At a minimum, to construct our event log we need to specify the following:\n\ncase_id: As we have explained before, the case identifier allows to differentiate between all cases (i.e, subjects) of the process. In our dataset, since we are analyzing activities in each learning session, the case_id would represent the identifier of each session (the column session_id of our dataframe that we have just created in the previous step).\nactivity_id: It indicates the type of activity of each event in the event log. In our dataset, we will use the column Action that can take the values: “Course_view”, “Assignment”, “Instructions”, “Group_work”, etc. LMS logs often contain too much granularity so it is useful to recode the events to give them more meaningful names [33].\ntimestamp: It indicates the moment in which the event took place. In our dataset, this is represented by the timecreated column which stores the time where each ‘click’ on the LMS took place.\n\nNow that we have defined all the required parameters, we can create our event log. We will use the simple_eventlog() function from bupaR and supply the arguments to the function (corresponding to the aforementioned process elements) to the respective columns in our dataset.\n\nevent_log <- simple_eventlog(sessioned_data, \n              case_id = \"session_id\",\n              activity_id = \"Action\",\n              timestamp = \"timecreated\")\n\nIn our example, each row in our data represents a single activity. This is often the case in online trace log data, as each activity is a mere instantaneous click with no duration. As explained earlier, in other occasions, activities have a beginning and an end, and we have several rows to represent all instances of an activity. For example, we might have a row to represent that a student has begun to solve a quiz, and another one to represent the quiz’s submission. If the data looks like that, we might need to use the activitylog() function from bupaR to create our activity log, and indicate, using the lifecycle_id argument, which column indicates whether it is the start or end of the activity, or even intermediate states.\n\n\n4.2.2 Inspecting the logs\nNow that we have created our event log, we can use the summary() function to get the descriptive statistics. The results show that we have a total of 95,626 events for 9,560 cases (in our example, student sessions) and 12 distinct activities (the actions in the learning management system). We have 4,076 distinct traces (i.e., unique cases), which in our example means that there are sessions that have the exact same sequence of events. These traces are, on average, 10 events long, and span from September 9th 2019 to October 27th 2019.\n\nevent_summary <- summary(event_log)\n\nNumber of events:  95626\nNumber of cases:  9560\nNumber of traces:  4076\nNumber of distinct activities:  12\nAverage trace length:  10.00272\n\nStart eventlog:  2019-09-09 14:08:01\nEnd eventlog:  2019-10-27 19:27:41\n\n\nWe may now inspect what the most common activities are in our event log (Table 14.1). The absolute_frequency column represents the total count of activities of each type, whereas the relative_frequency represents the percentage (as a fraction of 1) that each activity represents relative to the total count of activities. We see that the working on the group project (Group_work) is the most frequent activity (with 32,748 instances), followed by viewing the course main page (Course_view) with 25,293 instances.\n\nactivities(event_log)\n\n\n\n\n\n\n\nTable 1.  Frequency of each activity \n  \n    \n    \n      Action\n      absolute_frequency\n      relative_frequency\n    \n  \n  \n    Group_work\n32748\n0.34245916\n    Course_view\n25293\n0.26449919\n    Practicals\n10020\n0.10478322\n    Assignment\n7369\n0.07706063\n    Instructions\n6474\n0.06770125\n    General\n3346\n0.03499048\n    Feedback\n3114\n0.03256437\n    Social\n2191\n0.02291218\n    La_types\n1891\n0.01977496\n    Theory\n1377\n0.01439985\n    Ethics\n1028\n0.01075021\n    Applications\n775\n0.00810449\n  \n  \n  \n\n\n\n\n\nWe can also view the most frequent activities in graphical form. We can even visually compare the frequency of activities between different groups (e.g., high achievers vs. low achievers).\n\nevent_log |> activity_frequency(\"activity\") |> plot()\nevent_log |> group_by(AchievingGroup) |> activity_frequency(\"activity\") |> plot()\n\n\n\n\n\n\n\n(a) Overall frequency\n\n\n\n\n\n\n\n(b) Frequency by group\n\n\n\n\nFigure 2. Activity frequency\n\n\n\nWe may be also interested in looking at activity presence, also known as case coverage. This refers to the percentage of cases that contain each of the activities at least once. Most sessions (85.8%) include visiting the main page of the course (Course_view). Slightly over half (54.6%) involve working on the group project (Group_work). Around one quarter include working on the Practicals (25.7%), Instructions (25.2%), and Assignment (23.2%). Other activities are less frequent.\n\nevent_log |> activity_presence() |> plot()\nevent_log |> group_by(AchievingGroup) |> activity_presence() |> plot()\n\n\n\n\n\n\n\n(a) Overall presence\n\n\n\n\n\n\n\n(b) Presence by group\n\n\n\n\nFigure 3. Activity presence\n\n\n\nIf we are interested in the transition between activities, we can plot the antecedent-consequent matrix. This visualization tells us how many transitions there has been from the activities on the left side (antecedent) to the activities on the bottom side (consequent).\n\nevent_log |> process_matrix() |> plot()\n\n\n\n\nFigure 4. Antedecent-consequent matrix\n\n\n\n\nFigure 14.4 shows that there are 6416 transitions from Course_view to Group_work within the same session. The most common transition is from Group_work to more Group_work, with 22060 instances. Note that, on the antecedent side (left), there is an additional row for Start. This allows to represent the activities that occur right at the beginning as a transition from Start, since they do not have any other activity as antecedent. In this case, we see that most sessions start with Course_view (4612). On the consequent side (bottom), we see that there is an additional column on the right side for End, representing the end of a session. This allows to represent the activities that occur at the very end of a session, since they do not have any other activities following. We see that sessions are most likely to end by Group_work (2762), closely followed by Course_view (2669). Another aspect worth noting is that some cells in our matrix are empty, which represents the fact that a transition between the activity on the left, and the activity on the bottom is not present in any of the cases in our dataset. For example, we see that there are no transitions between Applications and Theory. Another —obvious— example is the cell from Start to End, as sessions should have at least one event and therefore they can never go from Start to End without any other event in the middle.\n\n\n4.2.3 Visualizing the process\nNow, it is time to visualize the event log using the DFG. Within the bupaverse framework this graph is referred to as process map. We can use the process_map() function for this purpose (Figure 14.5).\n\nevent_log |> process_map() \n\n\n\n\nFigure 5. Default process map\n\n\n\nBy default, the process map contains every single activity and transition between activities. As the number of activities and cases increase, this can become unreadable, as is the case with our example. Therefore, we might need to trim some less frequent activities to be able to better visualize the process. We will choose to cover 80% of the activities. This means that we will select the most frequent activities in order, up to when we have accounted for 80% of all the activities in the event log. If we go back to Table 14.1, we can see that all activities ranging from the most frequent activity (Group_work) to Instructions account roughly to 80%, so we are left with the top five activities and exclude the rest. There is no standard for choosing a threshold for trimming and it is up to the researcher to define the threshold. As discussed in the introduction, such approach may yield different process maps according to the trimming threshold. A good rule of thumb for choosing this threshold is that the process map should be readable but at the same time no essential information should be removed. In our case, the activities filtered out are very infrequent and specific to certain lessons in the course, so they are not representative to the course as a whole. Another strategy would be to combine these activities into a single one such as Reading, which would allow to simplify our analysis while keeping all the information. For demonstration processes, we proceed by filtering the log and to operate with it in subsequent steps.\n\nevent_log |> filter_activity_frequency(percentage = 0.8) -> event_log_trimmed\n\nIf we now plot the process map of the trimmed event log, we will be able to see the process much more clearly (Figure 14.6). When interpreting a process map it is common to start by commenting on the nodes (the activities) and their frequency. For example, we would say that Group_work is the most frequent activity with 32,748 instances, followed by Group_work, etc., as we have already done in Table 14.1. Next, we would describe the transitions between activities, with emphasis on how sessions start. Most sessions (5,059) —in the trimmed dataset— start by viewing the course main page Course view. We can see this information on the edge (arrow) that goes from Start to Course_view. The second most common first step is Group_work (2,129). Most instances of Group_work are followed by more Group_work (22,429, as can be seen from the self-loop). The most common transitions between activities are between Course_view and Group_work, with 6,000+ instances between one another both ways, as we can infer from the labels of the edges between one another. Viewing the course main page (Course view) is the most central activity in the process map as it has multiple transitions with all the other activities, since students often use the main page to go from one activity to the next.\n\nevent_log_trimmed |> process_map() \n\n\n\n\nFigure 6. Process map trimmed\n\n\n\nAs we have seen, each node of the process contains the activity name along with its overall frequency, that is, the number of instances of that activity in our event log. Similarly, the edges (the arrows that go from activity to activity) are labeled with the number of times each transition has taken place. These are the default process map settings. Another way of labeling our process map is using the frequency(\"absolute-case\") option, which counts the number of cases (i.e., sessions in our event log) that contain each given activity and transition (Figure 14.7). Instead of seeing the count of activities and transitions, we are seeing the count of sessions that contain each activity and transition. For example, we could say that 8,203 sessions contain the activity Course_view.\n\nevent_log_trimmed |> process_map(frequency(\"absolute-case\"))\n\n\n\n\nFigure 7. Absolute case process map\n\n\n\nSometimes we may be interested in the percentage (relative proportion) rather than the overall counts because it may be easier to compare and interpret. In such a case, we can use the frequency(\"relative\") and frequency(\"relative-case\") options. In the former, the labels will be relative to the total number of activities and transitions (Figure 14.8 (a)), whereas in the latter, the labels will be relative to the number of cases (or proportion thereof) (Figure 14.8 (b)). For example, in Figure 14.8 (a), we see that Group_work accounts for 39.98% of the activities, whereas in Figure 14.8 (b) we see that it is present in 55.25% of the cases (i.e., sessions).\n\nevent_log_trimmed |> process_map(frequency(\"relative\"))\nevent_log_trimmed |> process_map(frequency(\"relative-case\"))\n\n\n\n\n\n\n\n(a) Relative activity frequency\n\n\n\n\n\n\n(b) Relative case coverage\n\n\n\nFigure 8. Relative process maps\n\n\n\nWe can combine several types of labels in the same process map, and even add labels related to aspects other than frequency. In the next example, we label the nodes based on their absolute and relative frequency and we label the edges based on their performance, that is, the time taken in the transition (Figure 14.9). We have chosen the mean time to depict the performance of the edges, but we could also choose median, minimum, maximum, etc. We see, for example, that the longest transition seems to be between Assingment and Group_work, in which students take on average 1.59 minutes (as seen on the edge label). The shortest transition is from Practicals to Practicals (self-loop) with 0.22 minutes on average.\n\nevent_log_trimmed |> process_map(type_nodes = frequency(\"absolute\"),\n                                 sec_nodes = frequency(\"relative\"),\n                                 type_edges = performance(mean))\n\n\n\n\nFigure 9. Relative nodes and performance edges process map\n\n\n\nAnother very useful feature is to be able to compare processes side by side. We can do so by using the group_by() function before calling process_map(). We see that the process map does not capture any meaningful difference between the two groups (Figure 14.10).\n\nevent_log_trimmed |> group_by(AchievingGroup) |> process_map(frequency(\"relative\"))\n\n\n\n\n\n\n\n\n(a) High achievers\n\n\n\n\n\n\n(b) Low achievers\n\n\n\nFigure 10. Comparison between two process maps\n\n\nWe can even reproduce the whole event log as a video using the function animate_process from processanimateR.\n\nanimate_process(event_log_trimmed)\n\n\n\n\nFigure 11. Process animation"
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#discussion",
    "href": "chapters/ch14-process-mining/ch14-process.html#discussion",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "5 Discussion",
    "text": "5 Discussion\nWe have demonstrated how to analyze event log data with process mining and how the visualization of process models can summarize a large amount of activities and produce intuitive insights. Process models are powerful, summarizing, visually appealing and easy to interpret and understand. Therefore, process mining as a method has gained large scale adoption and has been used in a large number of studies to analyze all sorts of data. This chapter has offered an easy to understand overview of the basic concepts including the composition of the event log and the modelling approach that process mining undertakes. We also offered a brief overview of some examples of literature that have used the method in the analysis of students’ data. Then, a step-by-step tutorial has shown different types of visualization in the form of process maps that are based on raw frequency, relative frequency, performance (time). Later, we have shown how process maps can be used to compare between groups and show their differences. The last part of the chapter offered a guide of how to animate the process maps and see how events occur in real-time. Please note that our approach is restricted to bupaverse, which is the most popular and well-maintained framework for process mining within the R environment.\nWhereas we share the enthusiasm for process mining as a powerful method, we also need to discuss the method from the rigor point of view. In many ways —as we will discuss later— process mining is largely descriptive and often times exploratory. While this is acceptable in the business domain, it needs to be viewed here from the viewpoint of scientific research and what claims we can make or base on a process map. As the tutorial has already shown, several parameters and decisions have to be made by the researcher and such decisions affect the output visualizations in considerable ways (e.g., filtering the event log). The said decisions are often arbitrary with no guidance or standards to base such decisions on. So, caution has to be exercised from over interpreting the process maps that have undergone extensive filtering. In fact, most researchers have used process mining as an auxiliary or exploratory method hand-in-hand with other tools such as sequence analysis or traditional statistics. In most cases, the conclusions and inferences were based on the other rigorous methods that are used, for instance, to statistically compare two groups. One can think of the process mining described in this chapter more of a visualization tool for the most common transitions or a method for visualizing the big picture of an event log in an intuitive way.\nAnother known problem of process mining is that most algorithms are black-boxed (i.e., the data processing is unclear to the researcher), with no fit statistics or randomization techniques to tell if the estimated process model is different from random. The absence of confirmatory tests makes the answer to the question of whether process mining actually recover the underlying process largely speculative. Moreover, all of the tools, algorithms, and visualization techniques have been imported verbatim from the business world and therefore, their validity for educational purposes remains to be verified. In the same vein, comparing across groups is rather done descriptively, without a statistic to tell whether the observed differences are actually significant or just happened by chance. In other methods such as psychological network analysis [34], several methods are available to create randomized models that assess the credibility of each edge, the stability of centrality measures, the differences between edges as well as to compare across networks regarding each of these parameters. Given that the algorithms, and the modelling technique necessitate trimming (discarding a considerable amount of data), it is bold to assume that the remaining process map (after trimming) is a true representation of the underlying process.\nIt is also important to emphasize here that, given that the field name is “process mining”, the method does not translate to efficient analysis of the learning process. In fact, it is unrealistic to assume that process mining as a method —or any other method at large— can capture the full gamut of the complexity of the learning process as it unfolds across various temporal scales in different ways (e.g., transitions, variations, co-occurrence). A possible remedy for the problem lies in the triangulation of process mining models with other methods, e.g., statistics or better use a Markovian process modeling approach [3]. Stochastic process mining models (covered in detail in Chapter 12 [3]) are more advantageous theoretically and methodologically. Stochastic process mining models are theoretically robust, account for time and variable dependencies, offer fit statistics, clustering methods, several other statistics to assess the models and a wealth of possibilities and a growing repertoire of inter-operable methods."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#further-readings",
    "href": "chapters/ch14-process-mining/ch14-process.html#further-readings",
    "title": "14  The why, how and when of process mining in learning analytics: A guided tutorial in R",
    "section": "6 Further readings",
    "text": "6 Further readings\nThere are multiple resources for the reader to delve deeper into process mining. To learn more about the method itself, the reader should refer to the book Process Mining: Data Science in Action [35] and the Process Mining Handbook [36]. For more practical resources, the reader can refer to the bupaverse [30] documentation as well as find out more about the existing bupaverse extensions. The tutorial presented in this chapter has dealt with process analysis and visualization but not process discovery or conformance checking as the tools available in R are limited for these purposes. For process discovery in R, the reader can use the heuristicsMineR package [37], and for conformance checking, the pm4py package [38], a wrapper for the Python library of the same name. A practical guide with Python is also available in A Primer on Process Mining: Practical Skills with Python and Graphviz [39]. For specific learning resources on educational process mining, the reader can refer to the chapter “Process Mining from Educational Data” [40] in the Handbook of Educational Data Mining, and “Educational process mining: A tutorial and case study using Moodle data sets” [41] in the book Data Mining and Learning Analytics: Applications in Educational Research. Moreover, the reader is encouraged to read the existing literature reviews on educational process mining (e.g., [11, 42])."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html",
    "href": "chapters/ch15-sna/ch15-sna.html",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#introduction",
    "href": "chapters/ch15-sna/ch15-sna.html#introduction",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nSocial Network Analysis (SNA) has emerged as a computational method for studying interactions, relationships, and connections among a vast array of different entities that include humans, animals, and cities, just to name a few. Two related and largely overlapping fields are also concerned with the network as a concept: network science and network analysis. Network science is concerned with the study of the structure of networks, finding patterns and universal laws that may explain or underpin such structure in a large variety of phenomena. Network analysis is a very closely related field that is concerned with the analysis of networks that are not necessarily “social”. In this chapter, we will simply use the terms social network analysis and network analysis interchangeably.\n\n1.1 What are networks?\nA quintessential concept in most analytical methods is that observations are —or should be — independent from each other whereas, in network analysis, observations can be related and interdependent and may interact with or influence each other Saqr_Poquet_Lopez-Pernas_2022. As such, network analysis offers a more realistic view of the interconnected world around us and allows us to paint an accurate picture of the relationships and interactions that underpin our world [1, 2]. For instance, when we study students’ engagement at a school, we may use a survey to measure each individual student’s engagement and compute statistics such as the correlation between engagement and grades. In doing so, we ignore that students interact with peers, teachers, and the environment around them [3, 4]. We also ignore that students get influenced by an engaged student, get supported by their friends, or face a problematic social environment that may hinder their engagement [5, 6]. Network analysis offers a rich set of methods for modeling and addressing such issues [7].\nA network is simply a group of entities (often called vertices, nodes, or actors) connected through a relationship (often called edges, links, or arcs) [2]. In this chapter, we will use the terms vertices and edges for simplicity. Vertices can be humans (students, teachers, or families), ideas, keywords, behaviors, emotions, feelings, concepts, schools, countries, or any entity that can be hypothesized to have a relationship with other entities. Vertices may be connected through a vast array of relations. For instance, students may be connected to each other by being friends, teammates, classmates, group members, neighbors, sporting club fans, competitors, sharing a desk, or working together on an assignment. There is virtually no limit to how a researcher can hypothesize a network. Nevertheless, the interpretation of network analysis relies heavily on how the network was constructed [2, 8].\nIn learning contexts, the most common type of networks comes from digital data, and in particular online communications, the most common of which are discussion forums, where the conversation occurs between forum participants (vertices). Each reply from one participant to another forms a relationship between the two vertices, creating an edge in the network. In such a situation, we are talking about a directed network, where the interaction has a source (the person who replies) and a target (the replied-to) [9]. Other examples of directed networks are citation networks (where documents cite other documents), or users that follow other users in social media [10]. In turn, an undirected network contains non-directional relationships such as a network of siblings, friends, teammates, husband and wife, or co-workers [2.] Representing interactions as a network equips the researchers with a rich toolset to harness the power of social network analysis methods. Please note that the researcher can choose to model a directed network as undirected in case direction is deemed inappropriate according to theory, connect or research question."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#analysis-of-social-networks",
    "href": "chapters/ch15-sna/ch15-sna.html#analysis-of-social-networks",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "2 Analysis of social networks",
    "text": "2 Analysis of social networks\nTwo types of analysis methods are commonly used: network visualization and mathematical network analysis [2.] Visualization summarizes all the interactions in the network, giving the researcher a bird’s eye view of the structure of relationships, who is interacting with whom, who is leading the conversation, and who is isolated. Visualization is so powerful that it can summarize a whole semester of interactions in one visualization and still give meaningful information that can be used for intervention e.g., [11]. Mathematical network analysis offers quantification of all the network properties and the individual actors. The network properties are commonly known as graph-level measures. The mathematical measures of individual nodes are known node-level measures (often referred to as centrality measures) [2, 8, 12] Visualization is not discussed in details in this chapter, but interested readers are encouraged to see the excellent visualization tutorial maintained by Kateto Ognyanova [13].\n\n2.1 Mathematical analysis\nThree types of mathematical analysis can be obtained with SNA: graph-level measures, node-level measures (centrality measures) and edge-level measures. Graph level measures describe the network as a whole in terms of size, interactivity and components. Centrality measures quantify the connectivity and importance of individual actors such as the degree of involvement in interactions, the distance to others, or the importance of position among other interacting actors [12, 14, 15]. However, each of these “importance” examples can be measured in different ways. For example, students can be considered central if they frequently contribute to discussions, reply to multiple unique threads of discussions, their contributions receive multiple replies, or their contributions trigger several threads of discussions. As such, there are several centralities with diverse methods that allow us to quantify the degree of connectedness. The website centiserver.org counts more than 400 centrality measures to this date and counting. In this book chapter, we will review and learn the most commonly used centralities in education. We rely on the recent meta-analysis 2022 for listing the used centrality measures, their classification, and operationalization in the literature. Edge measures describe the edge strength, edge weights or edges or edge centralities.\n\n2.1.1 Graph-level measures\nGraph-level measures are a type of analysis used in social network analysis that describes the overall structure and characteristics of a network. They can be used to compare different networks or track changes in a network over time.\nSize or vertex count is the number of vertices (individuals or groups) in the network.\nEdge count is the number of edges (interactions) between vertices in the network.\nDensity represents the number of edges that are present in the network divided by the number of all possible connections. High density indicates that many vertices in the network are connected to one another.\nReciprocity is another important graph-level measure that also reflects group cohesion. A reciprocated edge is an edge where two vertices have a reciprocal relationship (e.g., they are simultaneously source and target) [16]. The higher the ratio of the reciprocated edges, the more collaborative the network is, less centralized (dominated by few) and more participatory.\nTransitivity (or global clustering coefficient) measures the tendency of the vertices to cluster together or form tightly knit cliques. There is a large volume of research that associates the ratio of cliques with cohesion in collaborative groups, strong ties and productive knowledge construction [16]\nOther graph-level measures are aggregations of vertex-level or edge-level measures. For example, the mean degree is the mean number of edges of all the vertices in the network. In a collaborative group, the higher the mean degree, the more interactive the group is.\n\n\n2.1.2 Local centrality measures\nLocal centrality measures are centralities that map the direct or immediate connections of an actor. Put another way, the local centralities quantify the neighbors of a certain vertex where each of these neighbors is directly connected to the target vertex without any intermediates (Figure 15.1).\nIn-degree centrality represents the total number of incoming interactions or relationships of a vertex [12, 14]. Several examples exist in the literature with different operationalization and interpretations. In the case of collaborative learning, in-degree centrality has been interpreted as the worthiness of a participant’s contributions to receive replies, popularity, or influence [17–19].\nOut-degree centrality represents the total number of outgoing interactions or links from an actor to other actors in the network [12, 14, 20]. Out-degree has been commonly interpreted as an indicator of participation, effort, and activity [21, 22].\nDegree centrality refers to the total number of connections or interactions (incoming or outgoing) a vertex has. In undirected networks, it is simply the number of all connections of the vertex. Degree centrality can be interpreted in a similar way to the previous similar centralities as an indication of interactivity, communication, and social role in the collaborative process [23–25].\n\n\n\nFigure 1. Representation of a vertex in-degree (left), out-degree (middle), and degree (right)\n\n\n\n\n2.1.3 Measures based on shortest paths\nOther relevant measures in assessing social network graphs are the shortest paths. These are based on the shortest distance between a pair of points (vertex) in the graph and represent how easy it is for a vertex to access others’ resources [12, 14, 26]. Shortest paths can be used to better measure and understand centrality, as it is based on the distribution and distance of different points in a network which can help understand other educational insights [8., 27] The most relevant measures in this sense are represented in Figure 15.2 and described below.\nCloseness centrality is a measure based on the farness between the vertices of a network; more specifically between a specific vertex and the rest of the network [12, 14]. Small closeness values indicate greater proximity to other vertices, whereas larger values indicate greater distances from other vertices. In the field of education, it can be seen as a way to measure: 1) closure in interaction [11, 28–32], 2) ease of interaction [11, 24, 29, 33, 34], 3) time for accessing information [23, 31, 35], 4) dependencies [30], 5) control over resources [25, 28, 33], and 6) awareness of opportunities [28].\nBetweenness centrality is a measure to show the frequency of a vertex lying on the shortest path between two other vertices. In the field of education, it indicates how actors mediate communication among themselves [8, 27]. It can be a way to understand who are the leaders of the interactions, that is, those who can moderate interactions, reach unconnected groups (inter-group connection) , influence the information flow (information brokering) and manage that information to solve problems effectively [28, 29, 36].\nEccentricity can help to see the vertices not involved in the interactions. It is calculated as the distance to the farthest other vertices in the network. It can be used in the educational field to understand which are the students at risk of dropout or those that are not participating in the activities [11, 34, 36].\n\n\n\nFigure 2. Examples of networks where the highlighted vertex has a high value of betweenness (left), closeness (middle), and eccentricity (right)\n\n\n\n\n2.1.4 Eigenvector-Based centralities\nOther useful metrics in social network graphs are those based on eigenvector centralities, i.e., those related to the value of a connection between vertices. It is based on the idea that it is preferable to have fewer connections to strong well-connected vertices than to have many connections to weak isolated vertices. There are several measures based on this principle:\nEigenvector centrality assesses the importance of a vertex based on the centralities of the vertex to which it is connected, that is, how many connections it has to influential vertices. It has been used to understand social capital, ego, and connection strength [21, 23, 34, 37].\nPagerank is based on link analysis and allows for defining the popularity of an individual in a network. It can be used in education to understand the reputation or influence upon a group of stakeholders based on his/her contacts [36, 38].\nHub centrality is based on Kleinberg’s HITS algorithm [39] that measures who interacts more with the most influential vertices in a network. In the educational field, it could help to understand the type of students’ interaction [23, 38, 40].\nAuthority is also based on Kleinberg’s HITS algorithm but it is used to identify which vertices might be considered an authority on a specific topic score. It is calculated taking into account if its incoming links connect the vertex to others that have an important number of outgoing links [23, 38].\n\n\n2.1.5 Other measures\nThere are other possible metrics to be applied in social network graphs:\nClustering coefficient is a measure of how individuals, represented as vertices, are embedded in their neighborhood [36], with interactions with other individuals forming triangles. In the educational field, it shows how an individual works with peers in groups or clusters, which shows network cohesion [11, 25, 34, 38].\nDiffusion Centrality belongs to the diffusion measures that explore the structural properties that facilitate the diffusion and uptake process; that is, the diffusion resulting from the interaction. It tries to measure how well a vertex can diffuse a property given the semantics and structure of a social network and a model of diffusion [41, 42]. In education, it can be used to show the possibility of an interaction to generate replies and these other replies [19].\nCross Clique Centrality assesses the number of cliques or triangles a vertex belongs to. It is related to the degree of embeddedness, connectivity with other vertices, and strength of ties among vertices. In education, it allows understanding, for example, whether a post is going to be replied to and spread throughout the network [19].\nCoreness (k-core or linkage) is similar to h-index metrics for publications, and is based on networks where all vertices have at least a k degree [43]. It is interesting because if an individual may produce promising contributions, he or she is going to attract others with similar contributions and establish strong connections. In the field of education, those students involved in active interactions/discussions will attract other active users, which will enrich the discussion and therefore the interaction, collaboration, the quality of content, etc. [19].\n\n\n\n2.2 Network visualization\nAs we discussed earlier, networks represent the relationships (edges) between actors (vertices). A vertex is commonly represented as a circle —although other shapes are also used— and the edge as an arrow (in the case of a directed network) from the source of interaction to the target of the interaction (Figure 15.3). For example, a phone call from the caller to the call receiver is represented by an arrow from the caller to the receiver. In the case of an undirected network, the edge is represented by a line connecting both vertices, for instance, two siblings, where the relationship is mutual.\n\n\n\nFigure 3. Left: a directed edge where an interaction happens from A->B. Right: an undirected edge where A-B are connected\n\n\n\n\n2.3 Network analysis\nLet us start with a simple example that uses a common analysis scenario. Our example is a fictional discussion among students where we model interactions between students as a network. Building a network from interactions in discussion forums has been commonly performed by building a post-reply network where an edge is constructed from the author (source of the reply) to the replied-to (target of the reply). For instance, in Figure 15.4, B replies to A by saying “I agree, this could slow the progression of the disease but does it prevent the spread completely or terminate the pandemic?”. This can be represented as an edge from B to A. In the same token, C replies to B which represents another edge C to B. We can compile all of these interactions in a list of edges as shown in Table 15.1 which is often referred to as an edge list. An edge list is a simple way —among many others— to represent the network and therefore we will use it in this chapter. Constructing the network can be performed by aggregating all edges as a network as Figure 15.4.\n\n\n\nFigure 4. A conversation between students in a discussion forum\n\n\n\n\nTable 1. Edge list of students’ interactions in the discussion forum\n\n\nSource\nTarget\n\n\n\n\nB\nA\n\n\nC\nB\n\n\nD\nC\n\n\nE\nD\n\n\nF\nE\n\n\nB\nD\n\n\nC\nA\n\n\nA\nC\n\n\nD\nA\n\n\n\n\nThere are other ways to construct the network that depend on the researcher’s conceptualization, context, and research question. For instance, researchers may consider all students present in a discussion (co-present) and therefore linked together, which means that all vertices will have connections to each other [9, 22, 44]. Similarly, several ways exist to aggregate the network. Figure 5 shows three identical networks where we have duplicate ties (i.e., repeated interactions between the same pair of vertices). In Figure 15.5 A, all interactions are represented. We can see, for instance, that there are six edges between A and F vertices. We can also see that there is a “loop” or “self-loop” around C and E. A loop exists when an interaction occurs between the vertex and itself, as is the case when one replies to their own post. This type of network configuration where multiple edges and self-loops are allowed is often referred to as multigraph. Another possible way to aggregate the network is to create a weight for each edge which represents the frequency of interactions between each pair of vertices (Figure 15.5 B). For instance, the edge between A and F will have a weight of 6 which is the number of edges between A and F, whereas the weight of the edge between H and B is 1 since this interaction only happened once. The network in Figure 5B has an edge with a thickness that corresponds to the weight of 6, i.e., six times as thick as the edge between H and B. In other instances, we may disregard these repeated edges when they do not make a difference to our conceptualization of the network (Figure 15.5 C). Such a type of network is often referred to as a simplified network. For a discussion about network configurations and how they influence research results see [22].\n\n\n\nFigure 5. Several ways of constructing a network: A. Multigraph, B. Weighted, C. Simplified."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#network-analysis-in-r",
    "href": "chapters/ch15-sna/ch15-sna.html#network-analysis-in-r",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "3 Network analysis in R",
    "text": "3 Network analysis in R\nThe R language has a large number of libraries for the analysis of networks. The igraph package —the one used in our chapter— seems to be the preferred package by the R community given the number of dependencies, i.e., the number of other packages that rely on igraph or work with the igraph format [45–47]. The igraph package is fast, efficient, and well-respected within the academic community. The igraph package is also well maintained, continuously updated, has a large community, and has been released for other platforms besides R, e.g., Python. Other packages, such as sna and network, have a large user base, especially among those who are interested in statistical network modeling. Any of these packages —sna, network or igraph— can effectively perform the analysis described in this chapter. However, we will use igraph based on its relative ease of use and convenience for the chapter objectives.\nExample 1\nLet us start with a simple example where we analyze the network created for Figure 6. Before doing anything else, we need to import the necessary packages. We will use igraph to construct and represent networks, and we will use rio to download and import the data files that we need to use as an input for igraph.\n\nlibrary(igraph)\nlibrary(rio)\n\nWe can now use the import function from rio to download the data for the example (the data shown in Table 1), and assign it to a variable named SNA_example1.\n\nSNA_example1 <- \n  import(\"https://github.com/lamethods/data/raw/main/8_examples/SNA_example1.xlsx\")\n\nThe function graph_from_data_frame from igraph converts the edge list in Table 15.1 into a network. R expects a dataframe where the first two columns are used as edges (the first column is used as source column, and the second is used as source column). Please also note that the two columns can have any name. Also, all extra columns —if they are there— will be used as edge attributes. We can print it to see if it has been created correctly. The print function is commonly used to test if the graph creation has been successful. In other words, does the created network have the expected number of vertices, edges, and attributes?\n\nNet <- graph_from_data_frame(SNA_example1)\nprint(Net)\n\nIGRAPH 4dcb64e DN-- 6 9 -- \n+ attr: name (v/c)\n+ edges from 4dcb64e (vertex names):\n[1] B->A C->B D->C E->D F->E B->D C->A A->C D->A\n\n\nThe output of the print function gives a glimpse of the network properties. First, igraph states that the object is an igraph object (a network). Then, igraph gives a unique seven-letter identifier for the network (not usually needed for analysis). Next, igraph tells us that the network was directed (D) and named (N), i.e., vertices have a name attribute. Then, igraph lists the attributes and the edges of the network. We can also visualize the created network (Figure 15.6) by using the function plot.\n\nplot(Net)\n\n\n\n\n\n\nFigure 6. Example of a simple network plotted with igraph\n\n\n\n\nWe have seen here the most basic functions we can use in a graph with no arguments. As shown, networks work with little effort with R. In the next section, we will take a deeper look into these functions and others using another network from a published paper.\nExample 2\nThe next example is a larger network that comes from the interactions of a group of teachers in a Massive Open Online Course (MOOC). The MOOC included 445 participants from different places in the United States. The dataset has an edges file where the first two columns are the sender (source) and receiver (target). There is also a file for the vertices that contains demographic and other relevant information about each vertex: gender, location, and their role, etc. For more information about this dataset, please refer to the data chapter [48]. To get the data into R, we first need to read the data, store it in a dataframe and then build a network with the appropriate arguments.\nThe first line of the code reads the edges list data with their attributes into a dataframe with the name net_edges. The second line imports the vertex data with their attributes into a dataframe with the name net_nodes.\n\nnet_edges <- \n  import(\"https://github.com/lamethods/data/raw/main/6_snaMOOC/DLT1%20Edgelist.csv\")\nnet_nodes <- \n  import(\"https://github.com/lamethods/data/raw/main/6_snaMOOC/DLT1%20Nodes.csv\")\n\nTo create the network, we again use the function graph_from_data_frame. This time we have to specify the edges dataframe using the argument d=net_edges. The second argument (optional) tells igraph that the network should be directed, if not provided, the network is created directed by default. The third argument which is also optional vertices = net_nodes tells igraph to use the dataframe net_nodes for vertex attributes. If the vertices argument is not provided, igraph will extract vertex names from the edges data. In case there are important vertex attributes for the analysis, providing the vertices data can be useful. Building the network and explicitly setting all arguments —as we did— helps avoid the problems that could happen from the default settings of the function. For instance, the network could be created as directed where we aim at creating an undirected network. Note that igraph generates a multigraph network by default (see Figure 15.7).\n\nDLT1 <- graph_from_data_frame(d=net_edges, directed = TRUE, vertices = net_nodes)\n\nLet us now explore the network and see if it was built correctly using the function print. The print function output shows that the network is an igraph object, directed and named (DN) has 445 vertices, 2529 edges and then igraph lists the attributes of the vertices and the edges. Vertex attributes are listed along with their type. For instance, name (v/c) means the name attribute is a (v)ertex attribute and a (c)haracter. Edge attributes are listed in the same way. For instance, timestamp (e/c) means that it is an (e)dge and a (c)haracter.\n\nprint(DLT1)\n\nIGRAPH 572bc57 DN-- 445 2529 -- \n+ attr: name (v/c), Facilitator (v/n), role1 (v/c), experience (v/n),\n| experience2 (v/c), grades (v/c), location (v/c), region (v/c),\n| country (v/c), group (v/c), gender (v/c), expert (v/c), connect\n| (v/c), Timestamp (e/c), Discussion Title (e/c), Discussion Category\n| (e/c), Parent Category (e/c), Category Text (e/c), Discussion\n| Identifier (e/c), Comment ID (e/n), Discussion ID (e/n)\n+ edges from 572bc57 (vertex names):\n [1] 360->444 356->444 356->444 344->444 392->444 219->444 318->444 4  ->444\n [9] 355->356 355->444 4  ->444 310->444 248->444 150->444 19 ->310 216->19 \n[17] 19 ->444 19 ->4   217->310 385->444 217->444 393->444 217->19  256->219\n+ ... omitted several edges\n\n\nA network can also be plotted with the function plot() (Figure 15.7). However, plotting with R is a vast field and will not be discussed in detail here.\n\nplot(DLT1,  layout = layout.fruchterman.reingold, \n     vertex.size = 5, vertex.label.cex = 2)\n\n\n\n\n\n\nFigure 7. Example of a complex network plotted with igraph\n\n\n\n\n\n3.1 Graph level analysis\nNow that we have seen how to build a network from edge and vertex data, we are ready to understand some of the most commonly performed analyses in learning settings. The first type of analysis will look at the network level, or the whole group of collaborators. Analyzing the network level can tell us how interactive the group is, how cohesive, and how distributed the interactions are. We will go through each of these graph-level measures with a brief explanation of what they actually mean. We will use the data from example 1 (DLT1).\nLet us first start by calculating the basic measures of the network. The number of vertices can be queried using the function vcount, which adds up to 445, and the number of edges can be queried using the function ecount, which is 2,529. We can get the average number of interactions by a participant by dividing the number edges by the number of vertices which is 5.68.\n\nvcount(DLT1) ## 445\necount(DLT1) ## 2529\necount(DLT1) / vcount(DLT1) ## 5.683146\n\nThe density of a graph is an important parameter of a collaborative network that refers to the ratio of existing edges to the maximum possible among all participants. Density is maximum (1) when every vertex has interacted with every other vertex in the network. Graph density can be measured using the function graph.density.\n\ngraph.density(DLT1) ## 0.01279988\n\nHowever, the graph.density function may result in erratic results if the network is multigraph; this is because the igraph algorithm will count the repeated edges and loops. Thus, we need to simplify the network (delete all repeated edges and loops) before computing the density and use the simplified network to compute the graph density. The results of the density of the graph of 0.0097 which is rather a low value.\n\ngraph.density(simplify(DLT1)) ## 0.009798563\n\nReciprocity is another important graph-level measure that also reflects group cohesion. A reciprocated edge is an edge where two vertices have a reciprocal relationship (e.g., they are simultaneously source and target) [16]. The higher the ratio of the reciprocated edges, the more collaborative the network is, less centralized (dominated by few) and more participatory. Reciprocity can be computed using the function reciprocity, which automatically removes the loops (i.e., does not consider when a person replies to oneself). The reciprocity by igraph definition is the fraction of reciprocated edges in a directed graph. The value here is 0.1997544 which means that only 20% of all edges were reciprocated.\n\nreciprocity(DLT1) ## 0.1997544\n\nWe can also compute the dyad.census which returns the number of mutual interactions (reciprocated between a pair of vertices), the number of asymmetric interactions (interactions that are not reciprocated), and the number of non-connected pairs. The number of mutual interactions in our network is 212, which is relatively small given the asymmetric (1512) and non-connected pairs (97066).\n\ndyad.census(DLT1) ## $mut [1] 212     $asym [1] 1512     $null [1] 97066\n\nTransitivity (or global clustering coefficient) measures the tendency of the vertices to cluster together or form tightly knit cliques. In igraph, transitivity is measured as the probability that the neighboring vertices of a vertex are also connected to each other or, more accurately, the ratio of triangles in the network to the total count of triplets (all occurrences of three vertices connected by two edges). There is a large volume of research that associates the ratio of cliques with cohesion in collaborative groups, strong ties and productive knowledge construction Block_2015. There are several methods for the estimation of transitivity. Here, we are going to focus on global transitivity (i.e., at the network-level) using the igraph method. The transitivity can be calculated by the function transitivity; the default function returns the global transitivity measure by default. The transitivity of our network here is 0.08880774.\n\ntransitivity(DLT1) ## 0.08880774\n\nAnother possible way is to use the related function triad_census which reports the numbers of triangles and their different types. The reader may need to refer to the package manual to dig deeper in the results.\n\ntriad_census(DLT1) \n\n [1] 13901588   486626   124805     4227    35745    11186    15929     3668\n [9]      932       81     1857      376      223      334      345       68\n\n\nGroup productivity or intensity of interactivity can be explored using the degree measures and its variants. The average degree of the network measures how much on average each group member has contributed and received interactions. To compute the average degree, we first have to compute the degree for each member and then compute the mean.\nIn directed networks (like the one in this example), we can also compute the average in-degree and out-degree. For the same set of vertices, the network average in-degree should be equal to out-degree and both combined should be equal to the total degree. However, if we, for instance, compute a subset of vertices (only students excluding the teachers), in-degree and out-degree may be different. The code below computes the mean and median of the three measures, using the function degree with the argument mode=\"total\" for the total degree, mode=\"in\" for the in-degree, and mode=“out” for out-degree.\n\nMean_degree <-  mean(degree(DLT1, mode = \"total\")) ## 11.36629\nMean_in_degree <-  mean(degree(DLT1, mode = \"in\")) ## 5.683146\nMean_out_degree <-  mean(degree(DLT1, mode = \"out\")) ## 5.683146 \nMedian_degree <-  median(degree(DLT1, mode = \"total\")) ## 4\nMedian_in_degree <-  median(degree(DLT1, mode = \"in\")) ## 1\nMedian_out_degree <-  median(degree(DLT1, mode = \"out\")) ## 2\n\nThe mean degree is 11.36629 and the mean in-degree and out-degree are 5.683146. The median degree is 4, the median in-degree is 1, and the median out-degree is 2. The median differs significantly from the mean and may be more relevant here in this large network, where participation may not be well-distributed (see next section).\nCollaboration is participatory by design but, oftentimes, some students may dominate and contribute disproportionately more than others. In the same vein, some may prefer to be isolated and thus rarely participate. Several measures allow us to measure the distribution of interactions across the network and how skewed the network contribution patterns are. An obvious method that comes straight from statistics is the standard deviation (SD) of the degree centrality. We can compute the SD like we calculated the mean and median in the previous step. The SD of degree centrality in our case is 34.2, SD for in-degree centrality is 26.7, and SD for out-degree centrality is 9.8. The SD is higher than the mean which suggests that calculation and inspection of the median was justified. We can also see that the SD of the in-degree centrality is much higher than the SD of the out-degree, which means that the variability in receiving replies is higher than that of contributions. This variability is rather common since students are selective about whom they respond to and choose the reply-worthy contributions.\n\nSD_degree <-  sd(degree(DLT1, mode = \"total\")) ## 34.20511\nSD_in_degree <-  sd(degree(DLT1, mode = \"in\")) ## 26.73596\nSD_out_degree <-  sd(degree(DLT1, mode = \"out\")) ## 9.84249\n\nSNA has dedicated indices for measuring dominance in networks, known as centralization indices. Centralization indices are 0 when every vertex contributes equally and reaches the maximum of 1 when a single vertex dominates. A centralization index exists for many centralities —e.g., degree, closeness, and betweenness. Nevertheless, most of the literature has reported degree centralization, which we will demonstrate here.\nThe code below computes the degree, in-degree and out-degree centralization. Please note that we use a simplified network to avoid the loops and repeated edges. The results of degree centralization confirm the previous results. The degree centralization is 0.38, out-degree centralization is 0.16, and in-degree centralization is 0.60. In our network, we see that the in-degree centralization is the highest index (0.60), which means that only a few students received replies.\n\nCentralization_degree <- centralization.degree(simplify(DLT1), \nmode =  \"all\", loops = FALSE)$centralization  ## 0.3826871 \nCentralization_in_degree <-  centralization.degree(simplify(DLT1), \nmode = \"in\", loops = FALSE)$centralization    ## 0.6064291\nCentralization_out_degree <- centralization.degree(simplify(DLT1), \nmode =  \"out\", loops = FALSE)$centralization  ## 0.1572214\n\nAnother way to see how the interactions are distributed is to plot the degree distribution using the hist function, as demonstrated in Figure 15.8.\n\npar(mfrow=c(1,2))\nhist(degree(DLT1, mode = \"in\"), breaks = 100)\nhist(degree(DLT1, mode = \"out\"), breaks = 100)\n\n\n\n\nFigure 8. Distribution of degree. Left: In-degree. Right: Out-degree\n\n\n\n\n\n\n3.2 Network connectivity\nWe can also examine how connected the whole group is; this can be performed using the function is.connected which returns FALSE in our case, meaning that the graph has some disconnected components or subgroups of vertices that are isolated. We can check these subgroups by the function components which tells us that there are four components:\n\nis.connected(DLT1)\n\n[1] FALSE\n\nComponents <- components(DLT1)\nprint(Components)\n\n$membership\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n441 442 443 444 445 \n  2   3   4   1   1 \n\n$csize\n[1] 442   1   1   1\n\n$no\n[1] 4\n\n\nUsing the function decompose, we can look at each of the components. The largest component has 442 vertices, and three others have one vertex. These isolated vertices are simply students who did not contribute at all and do not represent a real subgroup.\n\nDecomposed <- decompose(DLT1)\nDecomposed[[1]]\n\nIGRAPH e8cd1d2 DN-- 442 2529 -- \n+ attr: name (v/c), Facilitator (v/n), role1 (v/c), experience (v/n),\n| experience2 (v/c), grades (v/c), location (v/c), region (v/c),\n| country (v/c), group (v/c), gender (v/c), expert (v/c), connect\n| (v/c), Timestamp (e/c), Discussion Title (e/c), Discussion Category\n| (e/c), Parent Category (e/c), Category Text (e/c), Discussion\n| Identifier (e/c), Comment ID (e/n), Discussion ID (e/n)\n+ edges from e8cd1d2 (vertex names):\n [1] 360->444 356->444 356->444 344->444 392->444 219->444 318->444 4  ->444\n [9] 355->356 355->444 4  ->444 310->444 248->444 150->444 19 ->310 216->19 \n[17] 19 ->444 19 ->4   217->310 385->444 217->444 393->444 217->19  256->219\n+ ... omitted several edges\n\n\nWe can also look at the network diameter or largest number of steps between vertices to see how far distant vertices are (using the distance function). A more representative variable would be the average distance between vertices, which we can obtain from the mean_distance function. The network diameter is 8 and the mean distance is 3. Both numbers are relatively high. Global efficiency is another network-level measure that examines how effective is the network structure as a conduit for information exchange using the distances between vertices. When all vertices are close to each other, reachable with a few number of steps, the network is said to be efficient. The value of efficiency is high in well connected groups, and low otherwise. We can examine the efficiency using the function global_efficiency.\n\ndiameter(DLT1) ## 8\n\n[1] 8\n\nmean_distance(DLT1) ## 3.030694\n\n[1] 3.030694\n\nglobal_efficiency(DLT1) ## 0.1961034\n\n[1] 0.1961034\n\n\n\n\n3.3 Network operations\nThere are many functions and tools to manipulate networks in igraph, which makes a comprehensive discussion of all of them beyond the scope of this introductory chapter. Nonetheless, we will discuss the most important functions. Oftentimes, we need to set an attribute to the vertices —e.g., setting the gender attribute for vertices— to be used in the analysis. Setting an attribute can be performed by using the V function followed by the $ character and the attribute that we want to set. Similarly, setting edge attributes can be done using the function E. In the next example, we define an attribute called weight for the vertices and we do the same for the edges. Using the skills we learnt, we can use them to create a simplified weighted network. We do so by concatenating all repeated edges into a single edge with the weight as the frequency. For that, we start by first assigning weights of 1 to each node and edge. Lastly, we use the function simplify to remove the duplicated edges and aggregate the weights (edge.attr.comb = list(weight = \"sum\", \"ignore\")) while all other edge attributes will be ignored.\n\nV(DLT1)$weight <- 1\nE(DLT1)$weight <- 1\nsimple.DLT1 <- simplify(DLT1, remove.multiple = TRUE, remove.loops = TRUE, \n                        edge.attr.comb = list(weight = \"sum\", \"ignore\"))\n\nThere are two important functions that we may need if we want to divide or create a subnet of the network. The function subgraph.edges allows us to create a subset of a network based on edge characteristics. In the following example, we create a subgraph with the discussions involving Curriculum & Instruction by using the argument E(DLT1)$'Discussion Category'== 'Curriculum & Instruction'. In the same way, the induced_subgraph function allows us to specify a subgraph based on the vertex characteristics. In the next example, we create a network for North Carolina teachers using V(DLT1)$location== \"NC\".\n\nk <- subgraph.edges(DLT1, \n    eids = which(E(DLT1)$'Discussion Category' == 'Curriculum & Instruction'))\nNC_network <- induced_subgraph(DLT1, vids = which(V(DLT1)$location == \"NC\"))\n\n\n\n3.4 Individual vertex measures (centrality measures)\nWe have discussed the centrality measures in the introductory section and how they can be used in an educational context. Centrality measures may serve many functions, —e.g., indicators of performance in collaborative environments [49] or indicators for roles in collaboration [21]. The igraph package allows the calculation of several centrality measures, many of which have been used in educational research and some of which may not be relevant. Other packages, such as centiserve, allow an even larger number of centrality measures [50]. In this section, we will focus on the common centrality measures according to the recent meta-analysis by [49] and other recently used measures, such as diffusion centrality measures.\nDegree centrality measures can be computed using the function degree and the argument mode specifies the type of degree where mode=\"in\" returns in-degree, mode=\"out\" returns out-degree, mode=\"total\" returns total degree centrality. In case of undirected networks, the mode argument is ignored and the function returns only the degree centrality, since there is no direction. We can combine all the centralities that we calculate together in a dataframe using the tibble function from the tibble package.\n\nInDegree  <- degree(DLT1, mode = \"in\")\nOutDegree <- degree(DLT1, mode = \"out\")\nDegree    <- degree(DLT1, mode = \"total\")\nDegree_df <- tibble::tibble(name = V(DLT1)$name, InDegree, OutDegree, Degree)\nprint(Degree_df)\n\n# A tibble: 445 × 4\n   name  InDegree OutDegree Degree\n   <chr>    <dbl>     <dbl>  <dbl>\n 1 1           20        33     53\n 2 2            2         5      7\n 3 3            2         4      6\n 4 4            2        14     16\n 5 5           16        17     33\n 6 6            9        24     33\n 7 7           32        26     58\n 8 8           13        18     31\n 9 9            2        12     14\n10 10           8        12     20\n# ℹ 435 more rows\n\n\nNote that igraph has another function called graph.strength that computes the degree centrality and takes the edge weight attribute into account. In multigraph networks —like ours— degree centrality and graph.strength should return the same result. However, in networks where the edges have a weight attribute both functions (degree and graph.strength) return different results. In such weighted networks —like the simplified network we created in the previous example— the degree function will return the unique connections of every vertex or the size of the vertex direct collaborators —known as the size of the ego network. The graph.strength function will return the number of interactions a vertex has made. See the next example and compare the results. For more information about the different calculation methods of degree centrality of weighted networks, readers are advised to refer to the seminal article by [51].\n\nInStrength  <- graph.strength(DLT1, mode = \"in\")\nOutStrength <- graph.strength(DLT1,mode = \"out\")\nStrength    <- graph.strength(DLT1, mode = \"total\")\nStrength_df <- tibble::tibble(name=V(DLT1)$name,InStrength,OutStrength,Strength)\nprint(Strength_df)\n\n# A tibble: 445 × 4\n   name  InStrength OutStrength Strength\n   <chr>      <dbl>       <dbl>    <dbl>\n 1 1             20          33       53\n 2 2              2           5        7\n 3 3              2           4        6\n 4 4              2          14       16\n 5 5             16          17       33\n 6 6              9          24       33\n 7 7             32          26       58\n 8 8             13          18       31\n 9 9              2          12       14\n10 10             8          12       20\n# ℹ 435 more rows\n\n\nCloseness and betweenness centralities are the most commonly used centrality measures according to [49] and both rely on the position of the vertex on the shortest paths between others. Closeness centrality can be calculated using the function closeness, which is directional; this means that we can compute in-closeness, out-closeness and total closeness centrality. Betweenness centrality can be computed using the function betweenness and the function is directional: the argument directed=TRUE computes the directional version, and vice versa. Another commonly used centrality measure is eigenvector centrality, which can be computed using the function eigen_centrality. The eigen_centrality function default is directed=FALSE, since it is less suited for directed networks [14]. Pagerank is another closely related centrality measure that uses a similar algorithm and is more suitable for directed networks. The Pagerank centrality can be calculated using the function pagerank. Please note that to obtain the value of the centrality, you need to use $vector at the end as demonstrated in the code.\nAn important question here is whether to compute these centralities with a simplified network, weighted network or a multigraph network. The answer depends on the context, the network structure and the research question. However, evidence suggests that multigraph configuration may render the most accurate results when centralities are used as indicators for performance [22]. The code below computes the aforementioned centralities, you may need to read the help of each centrality function for more options and arguments for customization:\n\nCloseness_In <- closeness(DLT1, mode = c(\"in\"))\nCloseness_Out <- closeness(DLT1, mode = c(\"out\"))\nCloseness_total <- closeness(DLT1, mode = c(\"total\"))\n\nBetweenness <- betweenness(simple.DLT1, directed = FALSE)\nEigen <- eigen_centrality(simple.DLT1, directed = FALSE)$vector\nPagerank <- page.rank(DLT1, directed = FALSE)$vector\n\nDiffusion centralities have been introduced recently in several studies and seem to offer a more robust estimation of a vertex role in spreading information [19, 52]. Diffusion centrality can be computed in the same way as degree centrality. However, there is no function in the igraph package to calculate this centrality, so we rely on the diffusion.degree function from the centiserve package. The function diffusion.degree accepts the mode argument to compute different variants, i.e., \"in\", \"out\" and \"total\" diffusion degrees.\n\nlibrary(centiserve)\nDiffusion.degree_in  <- diffusion.degree(DLT1, mode = c(\"in\"))\nDiffusion.degree_out  <- diffusion.degree(DLT1, mode = c(\"out\"))\nDiffusion.degree  <- diffusion.degree(DLT1, mode = c(\"all\"))\n\nCoreness and cross-clique connectivity are related centralities that estimate the embeddedness of the vertex in the network can be calculated using the functions coreness and crossclique. Both coreness and crossclique centralities have been shown to better correlate with performance as well as with productive and reply-worthy content [19].\n\nCoreness <- coreness(DLT1)\nCross_clique_connectivity <- crossclique(DLT1)\n\nWarning in cliques(graph): At core/cliques/cliquer_wrapper.c:57 : Edge\ndirections are ignored for clique calculations.\n\n\nWe can also combine the rest of the centralities together in a single dataframe:\n\nCentdf  <- tibble::tibble(name=V(DLT1)$name,Closeness_total,Betweenness,\nEigen,Pagerank,Diffusion.degree,Coreness,Cross_clique_connectivity)\nprint(Centdf)\n\n# A tibble: 445 × 8\n   name  Closeness_total Betweenness   Eigen Pagerank Diffusion.degree Coreness\n   <chr>           <dbl>       <dbl>   <dbl>    <dbl>            <dbl>    <dbl>\n 1 1            0.00110       1258.  0.206    0.00840             1865       18\n 2 2            0.000808        26.5 0.0107   0.00140              218        6\n 3 3            0.000799        30.6 0.00862  0.00130              191        6\n 4 4            0.00102         72.5 0.0803   0.00273              965       13\n 5 5            0.00106        309.  0.162    0.00525             1508       18\n 6 6            0.00108        250.  0.155    0.00539             1607       18\n 7 7            0.00111       1935.  0.230    0.00931             2088       21\n 8 8            0.00106        164.  0.136    0.00503             1483       18\n 9 9            0.00104         69.5 0.119    0.00251             1216       13\n10 10           0.00106        716.  0.0875   0.00343             1432       17\n# ℹ 435 more rows\n# ℹ 1 more variable: Cross_clique_connectivity <int>\n\n\nThe calculation of graph level measures and centrality measures are usually a step in the analysis to answer a research question. For instance, density can tell how distributed the interactions between students are and therefore, how it is collaborative [5]. Centrality may be calculated to identify who are the most important students in the discussions or used to infer the roles e.g., who are the leaders who drive the discussion [21, 23]. Several studies have calculated centrality measures to investigate their relationship with performance [18]. All of such types of analysis can be performed using the analysis we have demonstrated. Of course, there are no limits to the potentials of SNA and researchers have a wide range of possibilities and potentials that they can achieve by building on the aforementioned tutorials."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#discussion",
    "href": "chapters/ch15-sna/ch15-sna.html#discussion",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "4 Discussion",
    "text": "4 Discussion\nThe present chapter offered a primer on social network analysis as well as a tutorial on the most common types of SNA analysis. SNA is a vast field with diverse applications that are far beyond a chapter or even a whole book. Readers who are interested in expanding their knowledge about SNA are advised to read the literature cited in this chapter. Furthermore, several systematic reviews have tried to offer a synthesis of the extant literature and can help the readers get an idea about the status of SNA research in education. Two systematic reviews, [27, 53]. – despite being relatively old– they give a useful review on the uses and applications of SNA in learning settings. For instance, the methods used by SNA researchers have been addressed in a dedicated systematic review by [8], where the authors offered a detailed review of methodological approaches used in SNA research. Centrality measures were the topic of a recent systematic review and meta-analysis that synthesized the literature and offered evidence of the association of centrality measures with academic achievement [49].\nA more recent scientometric study by [6] offers a comprehensive review of all research on network analysis and network science across the past five decades. The study also offers a review of authors, countries, research themes and research foundations. Whereas not a traditional systematic review, the recent paper by [9] offers a review of the seminal papers of SNA with a methodological approach. The paper also offers recommendations for a reporting scheme for research using SNA. It is also important to mention that our chapter covered only static networks. Readers who are interested in the more advanced time varying networks, the temporal network chapter offers a great starting point [54]. Also several guides and empirical papers demonstrate examples of temporal network analysis [20, 55, 56]. Readers who want to go deeper in analysis of learning communities, the community detection chapter can be a good place [57]. Also, for readers interested in the novel methods of psychological networks, they are encouraged to read the psychological network chapter [58]."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#more-reading-resources",
    "href": "chapters/ch15-sna/ch15-sna.html#more-reading-resources",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "5 More reading resources",
    "text": "5 More reading resources\nBooks related to SNA that the readers can consult are:\n\nKolaczyk, E. D., & Csárdi, G. (2014). Statistical analysis of network data with R (Vol. 65). New York: Springer.\nLuke, D. A. (2015). A user’s guide to network analysis in R (Vol. 72, No. 10.1007, pp. 978-3). New York: Springer.\nNewman, Mark. Networks. Oxford university press, 2018.\nHanneman, R. A., & Riddle, M. (2005). Introduction to social network methods. (Link)\nCarolan, B. V. (2013). Social network analysis and education: Theory, methods & applications. Sage Publications.\nNetwork Science by Albert-László Barabási (Link)"
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#acknowledgements",
    "href": "chapters/ch15-sna/ch15-sna.html#acknowledgements",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nThis paper is co-funded by the Academy of Finland the project Towards precision education: Idiographic learning analytics (TOPEILA), Decision Number 350560"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html",
    "href": "chapters/ch16-community/ch16-comm.html",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#introduction-to-community-detection-in-social-networks",
    "href": "chapters/ch16-community/ch16-comm.html#introduction-to-community-detection-in-social-networks",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "1 Introduction to Community Detection in Social Networks",
    "text": "1 Introduction to Community Detection in Social Networks\nThe world is essentially social. Interactions, relationships and connections form the social fabric of the structure that makes it “social” [1]. Nevertheless, the world is far from uniform or random. The social actors (e.g., humans) tend to aggregate, coalesce, or work together in teams, groups, and communities [2, 3]. In humans, communities form neighborhoods, villages, cities, and even counties. On a smaller scale, communities can be a group of friends, teams, or work colleagues. These are —more or less— organized communities. However, communities can also emerge spontaneously among, for instance, people who happen to work together more than they work with others. Other examples include groups of universities that collaborate with each other more than they collaborate with other universities. In learning situations, communities can be groups of students within a whole cohort who collaborate with each other to a larger extent than with other students in a discussion forum. In the broader context of network analysis, community structure, or simply 'community', denotes those nodes within the network that can be categorized into distinct sets, ensuring that nodes within each set exhibit a high degree of internal connectivity.\nFinding these communities is integral to understanding the interaction process, the structure of the formed groups and how they contribute to the overall discussion. Community detection is not limited to discussion boards but extends to all types of networks where we would like to understand their structure. For instance, communities of key terms in a semantic network, where each concept is represented as a node, would allow us to understand the main themes of the conversation [4]. Communities of behaviors or strategies would allow us to understand the structure of human behavior and discern a better classification. The power of community detection has made it one of the most commonly used network approaches in education, social sciences and in fact, in scientific method at large [3].\nAs such, the main aim of community detection is to identify different groups or clusters of nodes within the network that show strong interconnectivity. Generally, most community detection algorithms are based on the idea that some nodes are more densely connected to each other than to nodes outside of the group. As a result, the application of community detection algorithms partitions the network into different groups, clusters or communities, based on the strength of their connections. An alternate branch of community detection, blockmodeling, adopts the perspective that people with similar patterns of ties are likely to act in similar ways within the network. This approach seeks to identify those similarly connected actors to abstract the network into different positions or roles [5, 6.] A special type of communities are the rich club communities [7, 8]. A rich club is a subset of nodes in the network who are working together —and who are therefore strongly connected— far more than with others in the network. The rich club entails a few doing most of the interaction and a majority who are far less involved and less favorable in collaborative networks. This chapter explores the first and more common approach, that of identifying densely connected subgraphs.\nAs mentioned above, a basic approach understands communities in social networks as subgraphs where the number of internal edges is larger than the number of external edges, and therefore they usually group nodes that have a higher probability of being connected to each other than to members of other groups [9]. The goal of identifying communities in social networks depends on the research question and context and, same as in the case of network analysis in general [10], the communities found in the analysis depend on how the network was constructed, and therefore their meaning must be explained by the researcher in each different context of application. For instance, a researcher might want to find communities to understand a social shift in some larger group [11, 12], to map political polarization [13], to interrupt disease transmission or to spread health-related behaviors [14], to study how students in a course self-segregate and stabilize into groups [15], or for many other reasons.\nMany partitioning algorithms use a quantitative measure called modularity index, which is, up to a multiplicative constant, the number of edges falling within groups minus the expected number in an equivalent network with edges placed at random [16]. Modularity, which ranges between -0.5 and 11 [18] and of which positive values indicate the possible presence of community structure [16] is not itself a clustering method, but a measure for comparing different partitionings to judge the best. There are no established values/thresholds for optimum modularity, but in practice it is found that a value above about 0.3 is a good indicator of significant community structure in a network [19].\nIn the previous chapter on Social Network Analysis [10], we learned how to build a network from a set of vertices (or nodes) and edges, how to visualize it, and how to calculate centrality measures that help us understand and describe the structure of the network and the position of the nodes with respect to others. In this new chapter, we focus on detecting communities (groups of highly connected nodes) within a wider network, and how to visualize them using R."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#community-detection-in-social-networks-based-on-educational-data",
    "href": "chapters/ch16-community/ch16-comm.html#community-detection-in-social-networks-based-on-educational-data",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "2 Community detection in social networks based on educational data",
    "text": "2 Community detection in social networks based on educational data\nThe application of community detection techniques in social networks analysis of educational data (for further information about educational data and sources of educational data, we refer the reader to Chapter 2 in this book [20]) dates back to the emergence of learning analytics as a discipline. For example, in a 2012 study, Rabbany et al. [21] implemented community mining in their implementation of Meerkat-ED to monitor how student interactions and collaborations occurred and changed over time.\nCommunity detection can serve multiple purposes in learning analytics. For example, Pham et al. [22] explored the community structure of the eTwinning project collaboration network and how the quality of the contributors’ work relates to their level of participation. Similarly, Suthers and Chu [23] unveiled communities emerging within the Tapped In network of educational professionals from the associations between members of this network distributed across media (chat rooms, discussion forums and file sharing). Orduña et al. [24] applied modularity analysis to identify collaborative behaviors in a mobile remote laboratory. Skrypnyk et al. [25] and Gruzd et al. [26] identified emerging communities from Twitter-based interactions in a cMOOC. Joksimovic et al. [27] extracted clusters of concepts into topics (concept clusters) exchanged through social media also in the scope of a cMOOC. Hernández-García et al. [28] used connected components to analyze group cohesion. Adraoui et al. [29] identified student groups through their social interactions in Facebook groups. Nistor et al.[30] used cohesion network analysis to predict newcomer integration in online learning communities. López Flores et al. [31] performed community detection analysis to identify the learning behavior profiles of undergraduate computer science students. Abal Abas et al. [32] determined study groups during peer learning interaction. Li et al. [33]\nexamined whether students formed communities dominated by a specific gender or race, and Nguyen [34] explored youth's perspectives to frame climate change education by using community detection to find hashtag groups and identify different discourse themes in TikTok. The reader may find more examples of community detection in online learning environments in the recent review by Yassine et al. [4]. Another important aspect of community detection relates to the algorithms used to perform the detection, which will be discussed in the next section."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#algorithms-for-community-detection",
    "href": "chapters/ch16-community/ch16-comm.html#algorithms-for-community-detection",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "3 Algorithms for community detection",
    "text": "3 Algorithms for community detection\nCommunity detection in social network analysis involves the application of specific algorithms (the number of community detection algorithms is rather high and will not be addressed in this chapter; for further information on the nature and performance of community detection algorithms, we suggest reading Lancichinetti and Fortunato [35], Fortunato [36], Fortunato and Hric [9] and Chunaev [37]), the most popular being Louvain [38], the one implemented in the software applications Gephi, Pajek and visone by default, and Girvan-Newman’s edge betweenness [39] and clique percolation [40], used in CFinder.\nIn this chapter, we will focus on how to perform community detection with the R programming language. In R, community detection algorithms are implemented as functions of the igraph library [41], which was already introduced in the Social Network Analysis chapter [10]. The library includes functions to apply the following methods:\n\nLouvain (cluster_louvain): A multi-level modularity optimization algorithm that aims to discover community structure [42].\nGirvan-Newman (cluster_edge_betweenness): The concept behind this method is that edges connecting distinct communities tend to have a high edge betweenness and all the shortest paths from one community to another must pass through these edges [43].\nFast greedy optimization (cluster_fast_greedy): A fast greedy modularity optimization method to identify communities within a network [19].\nFluid communities (cluster_fluid_communities): This method is based on the concept of multiple fluids interacting in a non-uniform environment (represented by the graph topology). The method identifies communities by observing their expansion and contraction patterns, driven by their interactions and density [44].\nRandom walk-based:\nInfomap (cluster_infomap): This method discovers the community structure that minimizes the expected description length of a random walker's trajectory [45].\nWalktrap (cluster_walktrap): This method is based on the underlying concept is that short random walks have a tendency to remain within the same community [46].\nLabel propagation (cluster_label_propagation): This method for detecting community structure in networks is efficient, with a nearly linear time complexity [47]. It operates by assigning unique labels to the vertices and subsequently updating these labels through majority voting within the vertex's neighborhood.\nLeading eigenvector (cluster_leading_eigen): This method identifies densely connected subgraphs within a graph by computing the principal non-negative eigenvector of the modularity matrix associated with the graph [48].\nLeiden (cluster_leiden): The Leiden algorithm is similar to the Louvain algorithm but offers superior speed and delivers higher-quality solutions. It has the capability to optimize both modularity and the Constant Potts Model, which overcomes the resolution limit challenge [49].\nOptimal modularity clustering (cluster_optimal): This method computes the optimal community structure of a graph by maximizing the modularity measure across all potential partitions [18].\nSimulated annealing (cluster_spinglass): This method leverages the spin-glass model and simulated annealing techniques to explore the graph's structure and identify cohesive communities [50].\n\nOther available libraries to perform community detection include nett, which allows for spectral clustering (function spec_clust()) and implements methods for hypothesis testing.\nWhen to choose one algorithm over another usually depends on the characteristics of the network data, whether or not directed edges are allowed and the goals of the analysis. For example, if the network is very large, Louvain or Label Propagation algorithms may offer satisfactory results with a small computational effort, whereas Girvan-Newman might be more appropriate if the network has a hierarchical structure or when networks are large. If the network has a flow of information, Infomap may be better suited, and spectral clustering might be preferrable when community structures are complex. Guidance about which algorithms to use is not robust, and issues such as computational efficiency are more often discussed than the match with research questions [9, 51]. Ideally, the choice should be made to align with the processes believed to drive community formation and the research purpose in seeking this structure [51]. In any case, we would recommend trying different algorithms and comparing their results before choosing one.\nIt is important to note the limitations of some community detection algorithms in igraph. For example, fast greedy optimization, leading eigenvector, Leiden, Louvain and simulated annealing only work with undirected graphs —even though some of these algorithms, such as Leiden, work in directed networks, their implementation in igraph does not allow for community detection in directed networks—, whereas fluid communities or simulated annealing only work with simple and connected graphs.\nFinally, it is worth mentioning that a limitation of some methods is that they are non-overlapping community detection algorithms; that is, they consider that a node belongs only to one group, partition or community. However, that is not often the case, which is why some overlapping community detection algorithms have been proposed (for example, random walk-based algorithms may handle overlapping), and the reader might want to check whether the chosen algorithm finds overlapping or non-overlapping communities when choosing what algorithm to use. Fortunato and Hric [9] offer additional guidance if overlapping communities are required. Now that we have covered the igraph library along with its features and limitations, in the next section we will present an example of how to use this library."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#community-detection-in-r-an-annotated-example-using-igraph",
    "href": "chapters/ch16-community/ch16-comm.html#community-detection-in-r-an-annotated-example-using-igraph",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "4 Community detection in R: An annotated example using igraph",
    "text": "4 Community detection in R: An annotated example using igraph\nTo illustrate the use of igraph to perform community detection algorithms in R, we will use data from one of the courses reported in an article by Hernández-García and Suárez-Navas [52]. More specifically, this example focuses on the data from the “Programming Basics” undergraduate course. The data set includes forum activity of 110 students from the BSc Degree on Biotechnology, which follows the Comprehensive Training Model of the Teamwork Competence (CTMTC) methodology [53]. In this course, students work in groups of between five and seven members each. The activity in the forum includes Q&As, technical and academic support in general forums, and message exchanges between group members in group-exclusive forums.\nThe original data set including all forum activity was divided into three different subsets, with the help of GraphFES [54]: views (how many times user a read a message posted by user b), replies (which user replies to a different user, and how many times) and messages (which message is a reply to another message).\nIn this example, we will focus on the Replies data set, a directed graph. The node list of the Replies data set includes a total of 124 nodes with three attributes: initPosts (number of first posts of a discussion sent by a user), replyPosts (number of posts replying to a previous post in the discussion) and totalPosts (the sum of initPosts and replyPosts). Weights in the edge list (attribute w) represent the number of times that user Target replied to user Source. The data set includes a total of 662 weighed edges. First off, we load the required libraries for data loading (rio) and analysis (igraph). Install them first if you have not done so before.\n\nlibrary(igraph) \nlibrary(rio)\n\nThen, we import the node and edges data, and we build the graph object, indicating that the network is directed (with the argument directed = TRUE in the call to graph_from_data_frame()):\n\nrepo <- \"https://github.com/lamethods/data/raw/main/10_snaProgramming/\"\nds.nodes <- import(paste0(repo,\"hg_data_nodes.xlsx\"))\nds.edges <- import(paste0(repo,\"hg_data_edges.xlsx\"))\nds <- graph_from_data_frame(d = ds.edges, directed = TRUE, vertices = ds.nodes)\n\nWe can now observe the structure of the graph:\n\nstr(ds)\n\nClass 'igraph'  hidden list of 10\n $ : num 124\n $ : logi TRUE\n $ : num [1:662] 101 73 48 51 84 58 48 101 62 27 ...\n $ : num [1:662] 73 1 51 1 1 1 101 62 27 51 ...\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ :List of 4\n  ..$ : num [1:3] 1 0 1\n  ..$ : Named list()\n  ..$ :List of 5\n  .. ..$ name      : chr [1:124] \"54\" \"55\" \"56\" \"57\" ...\n  .. ..$ User      : chr [1:124] \"user_54\" \"user_55\" \"user_56\" \"user_57\" ...\n  .. ..$ initPosts : num [1:124] 0 23 0 0 0 0 1 0 3 3 ...\n  .. ..$ replyPosts: num [1:124] 0 2 0 0 0 0 3 0 19 53 ...\n  .. ..$ totalPosts: num [1:124] 0 25 0 0 0 0 4 0 22 56 ...\n  ..$ :List of 1\n  .. ..$ weight: num [1:662] 1 2 1 2 5 4 1 1 1 1 ...\n $ :<environment: 0x55fb4723e5d8> \n\n\nWe can also inspect the main attributes of the graph, which is shown as a directed, named and weighted network with 124 nodes and 662 edges:\n\nprint(ds)\n\nIGRAPH d2cefd2 DNW- 124 662 -- \n+ attr: name (v/c), User (v/c), initPosts (v/n), replyPosts (v/n),\n| totalPosts (v/n), weight (e/n)\n+ edges from d2cefd2 (vertex names):\n [1] 192->164 164->55  139->142 142->55  175->55  149->55  139->192 192->153\n [9] 153->118 118->142 160->160 158->55  163->175 152->55  182->161 161->55 \n[17] 210->55  149->138 138->55  117->178 178->55  127->55  160->55  197->55 \n[25] 155->55  122->55  189->55  145->55  135->55  207->55  203->55  140->55 \n[33] 159->55  126->55  123->55  139->55  133->55  153->55  201->55  139->139\n[41] 139->180 192->134 134->206 206->192 192->205 205->205 205->192 192->113\n[49] 113->192 134->192 192->206 113->205 205->206 192->192 113->134 206->205\n+ ... omitted several edges\n\n\nIn addition, we may use the plot function to visualize the network, A first glance at the graph does not offer much information about the underlying community structure of the graph:\n\nplot(ds)\n\n\n\n\nFigure 1. Graph of the Replies from the data set of [52] using the plot function.\n\n\n\n\nAt any moment, we can apply a community detection algorithm to the graph. In Figure 16.1, we observe that the graph is not connected and is directed. Therefore, we can only apply a subset of community finding algorithms, such as Girvan-Newman, Infomap, Label propagation, or Walktrap (note that R will trigger a warning for Girvan-Newman, because edge weights have different meaning in modularity calculation and edge betweenness community detection). For the purpose of this example, we will apply the Infomap algorithm —because it is a random walk-based algorithm, we provide a random seed for reproducibility.\n\nset.seed(1234)\ncomm.ds <- cluster_infomap(ds)\n\nThe basic call to the cluster_infomap() function takes the graph as an argument (other arguments include edge weights, node weights, number of attempts to partition the network and whether modularity needs to be calculated; in this case, the function takes the ‘weight’ edge attribute as default edge weight) and returns a community object. We can observe its structure:\n\nstr(comm.ds)\n\nClass 'communities'  hidden list of 6\n $ membership: num [1:124] 1 2 3 4 5 6 7 8 9 10 ...\n $ codelength: num 3.95\n $ names     : chr [1:124] \"54\" \"55\" \"56\" \"57\" ...\n $ vcount    : int 124\n $ algorithm : chr \"infomap\"\n $ modularity: num 0.927\n\n\nThe attributes of the community object include the group each node belongs to, the code length or average length of the code describing a step of the random walker (this parameter only adopts a value in random walk algorithms), node names, number of nodes and algorithm used to partition the network in communities. It is also possible to access values of the community object using different functions such as length(), sizes() or membership(), which return the number of communities, sizes of each community and membership of each node, respectively. At this point, it is possible to plot the graph and the communities (Figure 16.2):\n\nplot(comm.ds, ds)\n\n\n\n\nFigure 2. Communities emerging from the Replies data set using the Infomap community finding algorithm.\n\n\n\n\nSee help(plot.communities) for more details on this method, which takes a communities object as its first argument and an igraph object as its second argument. By default, it colors the nodes and their surrounding “bubbles” with a different color for each community, and marks community-bridging edges in red. In many networks this produces a very overlapping and indistinct picture2.\nBecause Figure 16.2 does not provide useful information (yet), we have to proceed to clean the graph. First, we simplify the graph by removing multiple edges between nodes and turning them into a single weight (this is not strictly necessary in this case because the original edge data already included calculated weights) and self-edges. Additionally, and because isolated nodes do not belong to any community (they are their own community), we can remove them from the graph. We then re-calculate the Infomap clustering for the simplified graph and plot it again (Figure 16.3). We see that the self-loops and repeated edges have disappeared.\n\nsimple.ds <- simplify(ds, remove.multiple = TRUE, remove.loops = TRUE,\n                      edge.attr.comb = list(weight = \"sum\", \"ignore\"))\nsimple.ds <- delete.vertices(simple.ds, which(degree(simple.ds) == 0))\ncomm.simple.ds <- cluster_infomap(simple.ds)\nplot(comm.simple.ds, simple.ds)\n\n\n\n\nFigure 3. Visualization of the communities emerging from the simplified Replies data set using the Infomap community finding algorithm.\n\n\n\n\nWe can now further refine the graph visualization to better highlight the communities:\n\nlo <- layout_with_fr(simple.ds, niter = 50000, \n                     weights = E(simple.ds)$weight * 0.05)\nplot(comm.simple.ds,\n     simple.ds,\n     layout = lo,\n     vertex.size = V(simple.ds)$totalPosts * 0.025,\n     vertex.label = NA,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 4. Fine-tuned visualization of the simplified graph.\n\n\n\n\nFrom Figure 16.4, we can clearly observe 19 communities, of which 18 correspond to the student groups and the remaining one (in the center of the graph) corresponds to the course instructor. Even in a network such as this with a relatively clear modular structure, different community detection algorithms can return different results. For example, cluster_spinglass() (which allows for adjustment of the importance of present vs. absent edges through a gamma parameter) returns a different partitioning:\n\nset.seed(4321)\ncomm.simple2.ds <- cluster_spinglass(simple.ds, gamma = 1.0)\nplot(comm.simple2.ds,\n     simple.ds,\n     layout = lo,\n     vertex.size = V(simple.ds)$totalPosts * 0.025,\n     vertex.color = membership(comm.simple2.ds),\n     vertex.label = NA,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 5. Visualization of simplified graph with spinglass communities.\n\n\n\n\nFigure 16.5 shows that the instructor is now included in one of the student communities, and a weakly connected member of another student group has split into their own community. Two groups with no direct bridging edges are also clustered together. This behavior is more common in blockmodeling, which looks for similarity of ties rather than direct links to identify groups; however, it can occur in standard community detection methods as well.\nAdditionally, it is possible to further simplify the network graph, by plotting only the communities and their inter-relationships. To do so, we build a condensed graph using the Infomap clustering where each node summarizes the information from all the members of the community (Figure 16.6).\n\ncomms <- simplify(contract(simple.ds, membership(comm.simple.ds)))\nplot(comms,\n     vertex.size = 2.5 * sizes(comm.simple.ds),\n     vertex.label = 1:length(comm.simple.ds),\n     vertex.cex = 0.8,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 6. Network graph of the links between Infomap communities using the simplified version of the Replies data set.\n\n\n\n\nIt is also worth noting that igraph incorporates a function, compare(), that takes different community objects from different partitioning methods, and allows for their comparison, based on different methods, such as variation of information, normalized mutual information, split-join distance or Rand and adjusted Rand indices. We have not included an example here because additional knowledge of the comparison metrics is needed to interpret the results, but see help(igraph::compare) for references.\n\n4.1 Interactive visualization of communities in R\nIn the first sections of this chapter, we have highlighted the uses and applications of community finding using educational data, as well as the main principles and methods, complemented with an example in igraph. However, the last section also highlights the limitations of the igraph library to provide advanced graphic features, such as interactive plotting. To overcome these limitations, we will further explore interactive visualization of communities using two different libraries: visNetwork and networkD3.\n\n4.1.1 visNetwork\nvisNetwork is an R package for network visualization that uses the vis.js javascript library. It is based on htmlwidgets, and therefore it is compatible with Shiny, R Markdown documents, and RStudio viewer. To access its functions, we must first load the visNetwork package\n\nlibrary(visNetwork)\n\nThen, we need to build a data set that visNetwork can read. To do so, we need to create a data frame with all the original data (in this example, the data set corresponding to the simplified graph), to which we add the group that each node belongs to.\nThe first step to create this data frame is to build a data frame with the community of the node (in the column group) and a column id with a list of all nodes. The last line resets the row columns to a sequence starting in 1. In this step, it is critical to rename the column that represents the group assignment to “group”, as this field is internally interpreted by visNetwork as the different communities of the network.\n\nmemberships <- as.data.frame(as.matrix(membership(comm.simple.ds)))\ncolnames(memberships)[1]<- \"group\"\nmemberships$id <- rownames(memberships)\nrownames(memberships) <- 1:nrow(memberships)\n\nThe memberships data frame now has a column of group (community) numbers and a column of the original node id numbers:\n\nhead(memberships)\n\n\n\n  \n\n\n\nNext, we retrieve the original node and edge list as data sets, using the as_data_frame function. While we could extract both data sets in a single step using the argument what = \"both\", in this example we extract them separately for clarity of the manipulations required for each data set.\n\nsimple.ds.nodes <- as_data_frame(simple.ds, what = \"vertices\")\nsimple.ds.edges <- as_data_frame(simple.ds, what = \"edges\")\n\nIn the node list, and while it is not absolutely necessary, we reset the row columns to a sequence starting in 1. After that, we need to rename the original ‘name’ and ‘User’ columns to ‘id’ and ‘title’. The former manipulation will allow us to add the group number to the dataset with the information included in the memberships object, while the latter is used by visNetwork to identify the different nodes.\n\nrownames(simple.ds.nodes) <- 1:nrow(simple.ds.nodes)\ncolnames(simple.ds.nodes)[1] <- \"id\"\ncolnames(simple.ds.nodes)[2] <- \"title\"\n\nFinally, we combine the node data set with the membership data set.\n\nvis.comm <- merge(simple.ds.nodes, y = memberships, by = \"id\", all.x = TRUE)\n\nFor visualization purposes, we add a column with the size of the nodes in the visualization.\n\nvis.comm$size <- vis.comm$totalPosts * 0.2\n\nFinally, we proceed to visualize the graph. To do so, we call the visNetwork function, and we pipe different visualization options (|>3), which we apply to the visualization object: (1) the manual random seed ensures reproducibility; (2) the legend displays the different communities in the right part of the viewer; (3) we highlight connected nodes on selection, and allow for selection in a dropdown menu by ‘id’ and ‘group’; and (4) we allow drag/zoom on the network, with navigation buttons that are displayed on the upper part of the viewer.\n\nvisNetwork(vis.comm, simple.ds.edges, width = \"100%\", height = \"800px\", \n           main = \"Interactive Communities\") |>    \n  visLayout(randomSeed = 1234) |> \n  visLegend(position = \"right\", main = \"group\") |> \n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, selectedBy = \"group\") |>  \n  visInteraction(hideEdgesOnDrag = TRUE, dragNodes = TRUE, dragView = TRUE, \n                 zoomView = TRUE, navigationButtons = TRUE)\n\n\n\n\n\n\n\n4.1.2 networkD3\nThe networkD3 is an advanced library for the interactive visualization of networks. It creates D3 network graphs and it is also based on the htmlwidgets framework, therefore simplifying the package’s syntax for exporting the graphs and allowing integration with RStudio, RMarkdown and Shiny web apps. To access its functions, we first load the library.\n\nlibrary(networkD3)\n\nAn advantage of the networkD3 library is that it provides a function, igraph_to_networkD3(), that allows direct loading of an igraph network as a networkD3 compatible graph. In this case, analogously to the previous example in visNetwork, we will visualize the simplified network. We provide two arguments to the igraph_to_networkD3 function: the original igraph object and the node membership list, also obtained before with the help of igraph.\n\ngraph.d3 <- igraph_to_networkD3(simple.ds, group = membership(comm.simple.ds))\n\nAnalogously to the example in visNetwork, we add node sizes for improved visualization.\n\ngraph.d3$nodes$size <- simple.ds.nodes$totalPosts * 0.2\n\nAnd finally, we use the forceNetwork function to display the graph. In this case, we will store the interactive visualization in an object to enhance the behavior of the legend later by using the htmlwidgets framework. From the example below, no additional manipulation of the original data sets was required4.\nThe forceNetwork() function requires the edge (Links) and node (Nodes) data frames, as well as the name of the source and target columns in the edge data frame, the node id and group columns in the node data frame. In the following code, it is also worth noting that the argument provided to display the size of the nodes refers to the column number —and not the column name—, and that we can provide different repulsion values using the charge parameter —the strength of node repulsion (negative values) or attraction (positive values).\n\nd3.comm <- forceNetwork(Links = graph.d3$links, Nodes = graph.d3$nodes, \n                        Source = 'source', Target = 'target', \n                        NodeID = 'name', Group = 'group',\n                        linkColour = \"#afafaf\", fontSize = 12, zoom = T, \n                        legend = T, Nodesize = 3, opacity = 0.8, \n                        charge = -25,  width = 800, height = 800)\nd3.comm\n\n\n\n\n\nIn the previous visualization, the legend moves when we zoom in or out, or drag the graph. A possible workaround to fix the legend is to use the htmlwidgets library [55]. The final result of the interactive visualization is shown below:\n\nlibrary(htmlwidgets)\n\n\nhtmlwidgets::onRender(d3.comm, jsCode = '\n  function (el, x) {\n    d3.select(\"svg\").append(\"g\").attr(\"id\", \"legend-layer\");\n    var legend_layer = d3.select(\"#legend-layer\");\n    d3.selectAll(\".legend\")\n      .each(function() { legend_layer.append(() => this); });\n  }\n')"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#concluding-notes",
    "href": "chapters/ch16-community/ch16-comm.html#concluding-notes",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "5 Concluding notes",
    "text": "5 Concluding notes\nIn this chapter we have introduced the literature on community detection in social network analysis, highlighted its uses in learning analytics, and worked through an example of finding and visualizing communities in R. The process begins outside of R, by identifying possible mechanisms of community formation in this network and why they are of interest. In educational settings, researchers may be concerned with information flow, dissemination of norms or attitudes, or other social forces. It is also important to consider whether the directionality and frequency (or other strength measure) of the interactions is important and that these considerations align with the theory and contextual peculiarity [56, 57]. Once these factors are thought out, there will still probably be a few options for community detection algorithms. It is worth trying more than one algorithm and comparing their groupings, as well as reading up on the method to see if it has tunable parameters such as the tightness of a random walk or the relative importance of missing links. Moreover, once the communities have been detected they can be explored in several ways. For example, one can investigate the demographic differences between the communities and determine whether they are formed based on shared characteristics between the nodes (e.g., gender, race, and nationality). Other aspects to look into are the content of the interactions, the difference in performance (e.g., final grade) between communities, and their temporal evolution to understand how they formed. Furthermore, each community can be visualized and analyzed as a network of its own using the methods explained in the Social Network Analysis chapter of this book.\nThough we began with the visualization tools available in the igraph package, in many cases researchers will want to go further. In these cases, the results of clustering algorithms can be used with libraries like visNetwork or networkD3. In the end, the goal of the visualization is to explore or present insights about network subgroups that speak to the original research questions, and it is helpful to be familiar with a range of tools for this purpose.\nThis chapter can be considered an introduction to the topic of community detection. However, interested users can resort to our cited papers and, for further readings, the selected papers and books provided in the following section can be a good start."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#further-readings",
    "href": "chapters/ch16-community/ch16-comm.html#further-readings",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "6 Further readings",
    "text": "6 Further readings\nInterested readers can refer to the following resources about community detection in general:\n\nFortunato, S., & Hric, D. (2016). Community detection in networks: A user guide. Physics Reports, 659, 1-44.\nTraag, V. A., Waltman, L., & Van Eck, N. J. (2019). From Louvain to Leiden: Guaranteeing well-connected communities. Scientific Reports, 9(1), 5233.\nXie, J., Kelley, S., & Szymanski, B. K. (2013). Overlapping community detection in networks: The state-of-the-art and comparative study. ACM Computing Surveys, 45(4), 1-35.\n\nFor specific resources using R, the reader can consult the following:\n\nBorgatti, S. P., Everett, M. G., Johnson, J. C., & Agneessens, F. (2022). Analyzing social networks using R. SAGE.\nKolaczyk, E. D., & Csárdi, G. (2014). Statistical analysis of network data with R (Vol. 65). New York: Springer.\nLuke, D. A. (2015). A user's guide to network analysis in R (Vol. 72, No. 10.1007, pp. 978-3). Cham: Springer."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#introduction",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#introduction",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning is social and therefore, involves relations, interactions and connections between learners, teachers and the world at large. Such interactions are essentially temporal and unfold in time [1]; that is, facilitated, curtailed or influenced at different temporal scales [2, 3]. Therefore, time has become a quintessential aspect in several learning theories, frameworks and methodological approaches to learning [3–5]. Modeling learning as a temporal and relational process is, nevertheless, both natural, timely and more tethered to reality [4, 6]. Traditionally, relations have been modeled with Social Network Analysis (SNA) and temporal events have been modeled with sequence analysis or process mining [3, 7].Yet, researchers have rarely combined the two aspects (the temporal and relational aspects) in an analytics framework [1]. Considering how important the timing and order of the learning process are, it is all-important that our analysis lens is not time-blind [8, 9]. Using time-blind methods flattens an essentially temporal process where the important details of progression are lost or distorted [10, 11]. In doing so, we miss the rhythm, the evolution and devolution of the process, we overlook the regularity and we may fail to capture the events that matter [9–11].\nTemporal networks\nRecent advances in network analysis have resulted in the emergence of the new field of temporal network analysis which combines both the relational and temporal dimensions into a single analytical framework: temporal networks, also referred to as time-varying networks, dynamic networks or evolving networks [10]. Today, temporal networks are increasingly adopted in several fields to model dynamic phenomena, e.g., information exchange, the spread of infections, or the reach of viral videos on social media [12]. Whereas temporal networks are concerned with the modeling of relationships similar to traditional social networks (i.e., static or aggregate networks), they are conceptually fundamentally different [10, 11, 13]. Additionally, temporal networks are not a simple extension of social networks, nor are they time-augmented social networks or time-weighted networks. In that, temporal networks are based on different representations of data, have a different mathematical underpinning, and use distinct visualization methods. In temporal networks, edges emerge (get activated or born) and dissolve (get deactivated or die) compared to always present edges in static social networks. Also, in temporal networks, an edge represents temporary interaction, contact, co-presence, or concurrency between two nodes interacting at a specific time. The fact that static networks represent nodes as being connected together all the time exaggerates connectivity [14, 15]. For instance, in Figure 17.1, we have five network visualizations, each network belonging to a weekday. We see that Monday, Tuesday, and Wednesday networks are relatively connected, whereas Thursday and Friday networks are disconnected. The corresponding aggregated or static network on the right is densely connected. The example in Figure 17.1 shows how a static network both conflates connectivity and obfuscates dynamics, you can read more about this example in [15]. Similarly, network measures calculated in static networks are inflated and biased -skewed towards higher values - because they ignore the temporal direction of edges allowing the edges to run back in time. Another characteristic of temporal networks is that edges have a starting time point and ending time point, the end of each edge is understandably later than the start, i.e., follows the forward-moving direction of time. Therefore, the paths in the temporal network are unidirectional or time-restricted [10, 11]. The next section discusses the temporal networks in detail.\n\n\n\nFigure 1. Interactions are aggregated per day showing that some days show an active group e.g., Monday, and other days have an inactive group e.g., Friday. Meanwhile, the static network on the right appears dense [15]"
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#the-building-blocks-of-a-temporal-network",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#the-building-blocks-of-a-temporal-network",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "2 The building blocks of a temporal network",
    "text": "2 The building blocks of a temporal network\n\n2.1 Edges\nIn temporal networks, edges are commonly referred to as events, links, or dynamic edges. Two types of temporal networks are commonly described based on their edge type [12].\n\nContact temporal networks: In contact temporal networks, edge duration is very brief, undefined, or negligible. For example, instant messages have no obvious duration but have a clear source (sender), target (receiver), and timestamp. Figure 17.2 shows a contact temporal network where the edges are represented as sequences of contacts between nodes with no duration.\n\n\n\n\nFigure 2. Example of a contact temporal network. Edges form momentarily and have no obvious duration.\n\n\n\nInterval temporal networks: In interval temporal networks, each interaction has a duration. An example of such a network would be a conversation where each of the conversants talks for a certain length of time. In the interval temporal network, the duration of interactions matters and the modeling thereof helps understand the process. In Figure 17.3, we see an interval temporal network where each edge has a clear start and clear end. For example, an edge forms between node A and node B at time point 1 and dissolves at time point 3, i.e., lasts for two time points.\n\n\n\n\nFigure 3. An example of an interval network. On the top, we see an example of an edge. In the bottom arrows pointing to two edges B-D and A-D, we can see that the two edges do not overlap and therefore, although they share a connection (A), we can’t assume that B is connected to A through D.\n\n\n\n\n2.2 Paths, concurrency, and reachability\nPaths represent the pathways that connect edges, the identification of which can help solve essential problems like the shortest path between two places in a route planning application, e.g., Google maps. In a dynamic process, the paths represent a time-respecting sequence of edges i.e., where the timing of each edge follows one another according to time passage, that is, the timestamps are incrementally increasing [10, 11]. For instance, let’s assume we have a group of students interacting about a problem, starting by defining the problem, argumenting, debating, and finding a solution. The temporal path that would represent the sequence of interactions among students in this process will be a defining->argumenting->debating->solving. We expect that the timestamp of defining precedes argumenting and argumenting precedes debating and so on. In that way, the path is unidirectional, follows a time-ordered sequence, and requires that each node is temporally connected, i.e., the two nodes coexist or interact with each other at the same time [11]. Such temporal co-presence is known as concurrent. Concurrency defines the duration of the nodes where they were co-present together and therefore can be a measure of the magnitude of contact between the two nodes. This is particularly important when we are modeling processes where the path length matters e.g., social influence. A student is more likely to be influenced by an idea when the student discusses the idea with another for a longer period of time. Similarly, self-regulation could be more meaningful when phases are more concurrent rather than disconnected [3]. Reachability is the proportion of nodes that can be reached from a node using time-respecting paths. A node is more influential or central, if it can reach a larger number of nodes [12].\n\n\n2.3 Nodes\nNodes in temporal networks are similar to static networks at large. Such nodes can be humans, objects, semantics, historical events or chemical reactions to mention a few. Perhaps, the possible difference —if it at all exists— is that temporal network tend to be studied in fields where temporal order is consequential e.g., epidemics, linguistics and spread of ideas."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#previous-work-and-examples-of-temporal-network-analysis",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#previous-work-and-examples-of-temporal-network-analysis",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "3 Previous work and examples of temporal network analysis",
    "text": "3 Previous work and examples of temporal network analysis\nFew studies have addressed temporal network analysis. Yet, some examples exist that may shed light on the novel framework and how it can be harnessed in education. In a study by Saqr and Nouri [15], the authors investigated how students interact in a problem-based learning environment using temporal networks. The study estimated temporal centrality measures, used temporal network visualization, and examined the predictive power of temporal centrality measures. The study reported rhythmic changes in centrality measures, network properties as well as the way students mix online. The study also found that temporal centrality measures were predictive of students” performance from as early as the second day of the course. Models that included temporal centrality measures have performed consistently better and from as early as the first week of the course. Another study by [9] analyzed students’ interactions in an online collaborative environment where students interacted in Facebook groups. The authors compared centrality measures from traditional social networks to temporal centrality measures and found that temporal centralities are more predictive of performance. Another study from the same group has used chat messages to study how students interact online and how temporal networks can shed light on different dynamics of students interacting using Discord instant messaging platform compared to students interacting using the forums in Moodle. Temporal networks were more informative in capturing the differences in dynamics and how such dynamics affected students” way of communicating [16]."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#tutorial-building-a-temporal-network",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#tutorial-building-a-temporal-network",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "4 Tutorial: Building a temporal network",
    "text": "4 Tutorial: Building a temporal network\nTemporal network is a relatively new field with an emerging repertoire of methods that are continuously expanding. As we currently stand, a coherent tutorial that combines all possible steps of the analysis does not exist, and that is what this chapter aims to fill. The tutorial will introduce the R packages, visualization and mathematical analysis e.g., graph and node level centrality measures.\nThe first step is to load the needed packages. Unlike the SNA chapter [17] where we relied on the igraph framework, we will rely on the statnet framework that has a rich repertoire of temporal network packages. We will use three main packages, namely tsna (Temporal Social Network Analysis) which provides most functions for dealing with temporal networks as an extension for the popular sna package. The package networkDynamic offers several complementary functions for the network manipulation, whereas the package ndtv (Network Dynamic Temporal Visualization) offers several functions for visualizing temporal networks. To learn more about these packages, please visit their help pages. The next code chunk loads these packages as well as tidyverse packages to process the network dataframe [20]. We also need tidyverse for manipulating the file and preparing the data.\n\nlibrary(tsna)\nlibrary(ndtv)\nlibrary(networkDynamic)\nlibrary(tidyverse)\nlibrary(rio)\n\nTo create a temporal network, we need a timestamped file with interactions. The essential fields are the source, target and time, and perhaps also some information about the interactions or the nodes (but these constitute extra information that is good to have). A temporal network is created by combining a base static network (that has the network base information) and a dynamic network with time information. As such, we need to prepare the Massive Open Online Course (MOOC) dataset described in detail here [21] and prepare it for creating a static network that will serve as a base network.\nThe next code chunk loads the dataset files (edges and nodes data) from the MOOCs dataset. Some cleaning of the data is necessary. \n\nnet_edges <- import(\"https://raw.githubusercontent.com/lamethods/data/main/6_snaMOOC/DLT1%20Edgelist.csv\")\nnet_nodes <- import(\"https://raw.githubusercontent.com/lamethods/data/main/6_snaMOOC/DLT1%20Nodes.csv\")\n\nFirst, we have to clean the column names from extra spaces using the function clean_names from the janitor package. Next, we have to remove loops, or instances where the source and target of the interaction are the same since it makes little sense that a person responds to oneself in a temporal network (this is not essential). Third, we need to create a dataframe where we replace duplicate edges with a weight equal to the frequency of repeated interactions, we will need this file for the creation of the base network (see later). Fourth, we recode the expertise level in the nodes file to meaningful codes (from its original numerical coding as 1,2,3) so that we can use them later in the analysis. The fifth step is to convert the timestamp to sequential days starting from the first day of the course; this makes sense for easy interpretation. Also, time works better in networkDynamic when it is numeric. The final step is to remove discussions where there are no replies. This cleaning is necessary since we have a dataset that was not essentially prepared for temporal networks.\n\nnet_edges <- net_edges |> janitor::clean_names() #1 cleaning column names\nnet_edges_NL <- net_edges |> filter(sender != receiver) #2 removing loops\n\n## Removing duplicates and replacing them with weight\nnet_edges_NLW <- net_edges_NL |> group_by(sender, receiver) |> tally(name = \"weight\") #3\n\n## Recoding expertise\nnet_nodes <- net_nodes |> \n  mutate(expert_level = case_match(experience, #4\n        1 ~\"Expert\",\n        2 ~ \"Student\",\n        3 ~ \"Teacher\"))\n\n## A function to create serial days\ndayizer = function(my_date) {\n  numeric_date = lubridate::parse_date_time(my_date, \"mdy HM\")\n  Min_time = min(numeric_date)\n  my_date = (numeric_date - Min_time) / (24*60*60)\n  my_date = round(my_date,2)\n  return(as.numeric(my_date))\n}\n\nnet_edges_NL$new_date = dayizer(net_edges_NL$timestamp) #5\n\n## Remove dicussions with no interactions\nnet_edges_NL <- net_edges_NL |> group_by (discussion_title) |> filter(n() > 1)\n\nAs mentioned before, the first step in creating a temporal network is creating a static base network (base network) which carries all the information about the network, e.g., the nodes, edges as well as their attributes. The base network is typically a static weighted network. Here we define the base network file (the weighted edge file we created before), we use directed = TRUE to create our network as directed and we tell the network function that the vertices attributes are in the net_nodes file.\n\nNetworkD <- network(net_edges_NLW, directed = TRUE, matrix.type = \"edgelist\", \n                    loops = FALSE, multiple = FALSE, vertices = net_nodes)\n\nFor creating a temporal network, we need more than the source and the target commonly needed for the static network. In particular, the following variables are required to be defined.\n\ntail: the source of the interaction\nhead: the target of the interaction\nonset: The starting time of the interaction\nterminus: the end time of the interaction\nduration: the duration of the interaction\n\nOur dataset —which comes from forum MOOC interactions, see net_edges below— has an obvious starting time (which is the timestamp of each interaction) but has no clear end time. There is no straightforward answer to this question. Nonetheless, a possible way to consider the duration of every post is the duration the post was active in the discussion or continued to be discussed. That is the time from the post in a discussion thread to the last post in the same threads of replies. Such a method —while far from perfect— offers a rough method for estimating the time during which this interaction has been “active” in the discussion [15, 22]. For an illustration, see Figure 17.4 which shows the duration for the first and second posts.\n\nnet_edges\n\n\n\n  \n\n\n\n\n\n\nFigure 4. A sample discussion demonstrating a way to compute the duration of the post. The first post lasts active in the discussion from starting time to the last reply it stimulated in the same thread (D1)\n\n\nThe next code chunk creates a variable for the starting time of each interaction, computes the ending time where this post was part of an active discussion, and then computes the duration.\n\n# Create the required variables (start, end, and duration) defined by\n\nnet_edges_NL <- net_edges_NL |> group_by(discussion_title) |>\nmutate(start = min(new_date), end = max(new_date), duration = end - start)\n\nIn the same way, the second duration is computed in the same way (D2). The next step of the analysis creates a dataframe where all the needed information for the network is specified and in the next step we simply use the networkDynamic with two arguments, the base network and the dataframe with all the temporal network information created in the previous step. Nonetheless, dealing with around 450 nodes in a network is hard, it becomes impossible to visualize or get insights from a large number of crowded nodes. So, for the sake of simplicity of demonstration in this tutorial, we will create a smaller subset of the network of people who had a reasonable number of interactions (degree more than 20) using get.inducedSubgraph argument. The resulting network is then called Active_Network which we will analyze.\n\n## Creating a dataframe with needed variables\n\nedge_spells <- data.frame(\"onset\" = net_edges_NL$start ,\n                         \"terminus\" = net_edges_NL$end, \n                         \"tail\" = net_edges_NL$sender, \n                         \"head\" = net_edges_NL$receiver, \n                         \"onset.censored\" = FALSE,\n                         \"terminus.censored\" = FALSE, \n                         \"duration\" = net_edges_NL$duration)\n\n## Creating the dynamic network network\nDynamic_network <- networkDynamic(NetworkD, edge.spells = edge_spells)\n\nEdge activity in base.net was ignored\nCreated net.obs.period to describe network\n Network observation period info:\n  Number of observation spells: 1 \n  Maximal time range observed: 0 until 72.01 \n  Temporal mode: continuous \n  Time unit: unknown \n  Suggested time increment: NA \n\nActive_Network <- get.inducedSubgraph(Dynamic_network,\n                                v = which(degree(Dynamic_network) > 20))\n\nWe can then confirm that the network has been created correctly using the print function. As the output shows, we have 521 distinct time changes, 72 days, 445 vertices and 1936 edges. We can also use the function plot to see how the network looks. The argument pad helps us remove the additional whitespace around the network. Plotting a temporal network helps summarize all the interactions in the network. As we can see in Figure 17.5, the network is dense with several edges between interacting students.\n\nprint(Dynamic_network)\n\nNetworkDynamic properties:\n  distinct change times: 495 \n  maximal time range: 0 until  72.01 \n\nIncludes optional net.obs.period attribute:\n Network observation period info:\n  Number of observation spells: 1 \n  Maximal time range observed: 0 until 72.01 \n  Temporal mode: continuous \n  Time unit: unknown \n  Suggested time increment: NA \n\n Network attributes:\n  vertices = 445 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  net.obs.period: (not shown)\n  total edges= 1936 \n    missing edges= 0 \n    non-missing edges= 1936 \n\n Vertex attribute names: \n    connect country experience experience2 expert expert_level Facilitator gender grades group location region role1 vertex.names \n\n Edge attribute names not shown \n\nplot.network(Active_Network, pad = -0.5)\n\n\n\n\nFigure 5. A plot of the full network as plotted by the plot function.\n\n\n\n\n\n4.1 Visualization of temporal networks\nTo take advantage of the temporal network, we can use a function to extract the network at certain times to explore the activity. In the next example in Figure 17.6, we chose the first four weeks one by one and plotted them alongside each other. The function filmstrip can create a similar output with a snapshot of the network at several time intervals.\n\nplot.network(network.extract(Active_Network, onset = 1, terminus = 7))\nplot.network(network.extract(Active_Network, onset = 8, terminus = 14))\nplot.network(network.extract(Active_Network, onset = 15, terminus = 21))\nplot.network(network.extract(Active_Network, onset = 22, terminus = 28))\n\n\n\n\nFigure 6. A plot of the network at weeks 1, 2, 3, and 4\n\n\n\n\nA similar result, yet with three-dimensional placing, can also be obtained with the timePrism function, as shown in Figure 17.7. You may need to consult the package manual to get more information about the arguments and options for the plots.\n\ncompute.animation(Active_Network)\n\nslice parameters:\n  start:0\n  end:72.01\n  interval:1\n  aggregate.dur:1\n  rule:latest\n\ntimePrism(Active_Network, at = c(1, 7, 14, 21),\n  spline.lwd = 1,\n  box = TRUE,\n  angle = 60,\n  axis = TRUE,\n  planes = TRUE,\n  plane.col = \"#FFFFFF99\",\n  scale.y = 1,\n  orientation = c(\"z\", \"x\", \"y\"))\n\n\n\n\nFigure 7. A three-dimensional visualization of the network as a sequence of snapshots in space and time prism\n\n\n\n\nHowever, a better way is to take advantage of the capabilities of the package ndtv and the network temporal information by rendering a full animated movie of the network and explore each and every event as it happens.\n\nrender.d3movie(Active_Network)\n\n\n\n\n\n\n\nAs we mentioned in the introduction section, in temporal networks, edges or relationships form “get activated” and dissolve “get deactivated”. We can plot such the dynamic edge formation and dissolution process using the functions tEdgeFormation which as the name implies plots the edges forming at the given time point. The function tEdgeDissolution returns the edges terminating and can be plotted in the same way as seen in Figure 17.8. Obviously, at the beginning of the MOOC, we see new relationships form and, at the end, most relationships dissolve.\n\nplot(tEdgeFormation(Active_Network, time.interval = 0.01), ylim = c(0,50))\nplot(tEdgeDissolution(Active_Network, time.interval = 0.01), ylim = c(0, 50))\n\n\n\n\n\n\n\n(a) Edge formation\n\n\n\n\n\n\n\n(b) Edge dissolution\n\n\n\n\nFigure 8. Edge formation and dissolution. We see that edge formation occurs towards the start and increases till almost t=50 and edge dissolution starts scanty at the beginning and increases with time, peaking at t=70\n\n\n\nAnother way to visualize a temporal network is to use the proximity timeline, the proximity.timeline function tries to draw the temporal network in two dimensions, that is, it draws nodes at each time point taking into account how closely connected they are and renders them accordingly, nodes that are interacting are rendered close to each other, and nodes that not interacting are rendered apart. Technically as described in the function manual: “The passed network dynamic object is sliced up into a series of networks. It loops over the networks, converting each to a distance matrix based on geodesic path distance with layout.distance. The distances are fed into an MDS algorithm (specified by mode) that lays them out in one dimension: essentially trying to position them along a vertical line. The sequence of 1D layouts are arranged along a timeline, and a spline is drawn for each vertex connecting its positions at each time point. The idea is that closely-linked clusters form bands of lines that move together through the plot” [20]. The result is a timeline of temporal proximity. The next code draws the proximity timeline but also adds some colors, and a start and end for the plot, see the result in Figure 17.9.\n\nproximity.timeline(Active_Network, default.dist = 1, mode = \"sammon\",\n                   labels.at = 1, vertex.col = grDevices::colors(),\n                   start = 1, end = 30, label.cex = 0.5)\n\n\n\n\nFigure 9. A proximity timeline shows connected interacting nodes closer to each other.\n\n\n\n\n\n\n4.2 Statistical analysis of temporal networks\n\n4.2.1 Graph level measures\nGraph properties in temporal networks are dynamic and vary by time. When graph measures are computed we get a time series of the computed measures. Such fine-grained measures allow us to understand how the networks and their structure evolve or unfold in time and thus, such information can help us understand collaboration or interaction dynamics as they occur [9, 15].\nThe function tSnaStats from the package tsna has a large number of measures that can be computed by specifying the argument snafun. In the next example, we compute the graph level density with the argument snafun=gden. The function allows the choice of a range of time, for instance, from the end of the second week to the end of second month by supplying the arguments start = 14 to end = 60. We can also use the time.interval to specify the granularity of the calculation. The argument aggregate.dur specifies the period of the aggregation, for instance, aggregate.dur = 7 will compute the density for every seven days. We can also plot the density time series by simply using the function plot.\n\nDensity <- tSnaStats(\n  nd = Active_Network,\n  snafun = \"gden\",\n  start = 14,\n  end = 60,\n  time.interval = 1,\n  aggregate.dur = 7)\nplot(Density)\n\n\n\n\nFigure 10. A plot of temporal density from t=14 to t=60\n\n\n\n\nYou can see, in the resulting graph in Figure 17.10, that the density increases until day 50 and then starts to drop. Of note, another type of density can be computed, known as temporal density, which computes the observed total duration of all edges and divides it by the maximum duration possible. Temporal density can be computed using the command tEdgeDensity.\n\ntEdgeDensity(Active_Network) \n\n[1] 0.3901841\n\ngden(Active_Network) \n\n[1] 0.2186869\n\n\nSimilar to density, we can compute reciprocity, i.e., the ratio of reciprocated edges to asymmetric edges. Note, that we calculate reciprocity here from day 1 to day 73 on a daily basis (this is just for demonstration of different periods). Since the function default is to calculate the reciprocated dyads, we specify the argument measure = “edgewise” to calculate the proportion of reciprocated edges. As the graph in Figure 17.11 (a) shows, reciprocity increases steadily for the first 50 days pointing to a build up of trust between collaborators. The dyad.census function offers a more granular view of the dyads and their reciprocity as shown in Figure 17.11 (b). Similarly, mutuality is a very similar function and returns the number of complete dyads (reciprocated dyads), plotted in Figure 17.11 (c). All of the aforementioned functions deal with reciprocity, for differences and usages, readers are encouraged to read the functions’ help to explore the differences, arguments as well as the equation for each function.\n\nReciprocity <- tSnaStats(\n    nd=Dynamic_network,\n    snafun = \"grecip\" ,\n    start = 1,\n    end = 73,\n    measure = \"edgewise\",\n    time.interval = 1,\n    aggregate.dur = 1)\nplot(Reciprocity)\n\nDyad.census <- tSnaStats(Active_Network, \n                         snafun = \"dyad.census\")\nplot(Dyad.census)\n\ndynamicmutuality <- tSnaStats(\n  Active_Network,\n  snafun = \"mutuality\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1\n  )\nplot(dynamicmutuality)\n\n\n\n\n\n\n\n(a) Reciprocity over time\n\n\n\n\n\n\n\n(b) Dyad census over time\n\n\n\n\n\n\n\n(c) Mutuality over time\n\n\n\n\nFigure 11. Network descriptive statistics over time\n\n\n\nCentralization measures the dominance of members within the network and can be traced temporally using the function snafun = “centralization”. Please note that we can choose the period, the interval and the aggregation periods using function arguments as mentioned before. The next example computes the degree centralization as demonstrated in Figure 17.12. Also note that we can also compute other centralization measures such as centralization indegree, centralization outdegree, centralization betweenness, centralization closeness and eigenvector.\n\nDegree_centralization <- tSnaStats(\n  Active_Network,\n  snafun = \"centralization\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  FUN = \"degree\")\n\nplot(Degree_centralization)\n\n\n\n\nFigure 12. A plot of degree centralization over time\n\n\n\n\nSeveral other graph-level measure can be computed in the same way using the following arguments passed to the snafun: - components: count of Components within the graph over time - triad.census: the triad census and types of triads over time - connectedness: the connectedness score of the network efficiency network efficiency over time - gtrans: network transitivity over time - hierarchy: network Hierarchy over time - lubness: network LUBness over time - efficiency: network efficiency over time - hierarchy: network hierarchy over time\n\n\n4.2.2 Node-level measures (Temopral centrality measures)\nCentrality measures has been used to identify important actors, as a proxy indicator for academic achievement or to identify students’ collaborative roles [23–26]. In temporal networks, centrality measures are fine-grained estimates of students” real-time centralities or importance, i.e., shows who were central and when considering the temporarily of their interaction. In doing so, we can see exactly when and for how long, at what pace, and with which rhythm a behavior happens (compare this to traditional social network analysis where centralities are computed as a single number). There are several possible uses of temporal centrality measures. For instance [15] have used them to create predictive models of students’ performance. In another study by the same authors, they demonstrated that temporal centrality measures were more predictive of performance compared to traditional static centrality measures [9]. Since the temporal centralities are computed as time series, their temporal characteristics can also be used to compute other time series properties e.g., stability, variabilities, and pace, you can see for example [15]. There is a growing list of temporal centrality measures e.g., [13]. We will study here the most commonly used ones according to the latest review [25], but readers are encouraged to explore the tsna manual for more centrality measures.\nTemporal degree centrality measures can be computed in the same way as we computed the graph level properties shown before. The next code defines the function snafun = \"degree\", the start, the end date, and aggregation (which you can modify). An important argument here is the cmode argument which defines the type of centrality: “freeman”, “indegree” or “outdegree” for the calculation of total in or out degree centralities. The result is a time series with the 73 values for each day from start to end, each day having a unique value for the degree centrality for each node. The rest of the code is intended to organize the results. We convert the time series to a dataframe, create a variable for the day number to make it easier to identify the day and create a variable to define the type of centrality, and then combine all centrality measures into a single data frame as below.\n\nDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"freeman\")\n\ninDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"indegree\")\n\nOutDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"outdegree\")\n\nDegree_Centrality_DF <- Degree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73, centrality = \"Degree_Centrality\", .before = 1L)\n\n\ninDegree_Centrality_DF <- inDegree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73,centrality = \"inDegree_Centrality\", .before = 1L)\n\n\nOutDegree_Centrality_DF <- OutDegree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73,centrality = \"OutDegree_Centrality\", .before = 1L)\n\nrbind(Degree_Centrality_DF, inDegree_Centrality_DF, OutDegree_Centrality_DF) \n\n\n\n  \n\n\n\nIn the same way, closeness, betweenness and eigenvector centralities can be computed. Please note that we have a new argument here gmode=\"graph\" which tells snafun that we would like to compute these centralities considering the network as undirected. You can also use the gmode=\"digraph\" to compute the measures on a directed basis. You may need to use the previous code to convert each of the resulting time series into a data frame and add the day of the centralities. The snafun can compute other centrality measures in the same way e.g., information centrality, Bonacich Power Centrality, Harary Graph Centrality, Bonacich Power Centrality among others.\n\nCloseness_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"closeness\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nBetweenness_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"betweenness\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nEigen_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"evcent\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nReachability is another important measure that temporal networks offer. You can calculate who, when, and after how many steps an interaction reaches from a certain vertex to another or to every other node in the network (if it at all does). In the next code, we compute the forward pathway (earliest reachable node) from node 44 (one of the course facilitators) using the function tPath. The output is a time series with all the time distance values and the number of steps.\n\nFwdPathway <- tPath(\n  Active_Network,\n  v = 44,\n  start = 0,\n  graph.step.time = 7,\n  end = 30,\n  direction = \"fwd\")\n\nFwdPathway \n\n$tdist\n [1]  7.09 32.92 34.76 14.09 16.07 30.92 28.09   Inf  7.28 24.25 23.32 28.07\n[13] 23.92 14.09 27.11 24.17 17.35 33.20 28.29 26.08 22.14 30.07 21.09 25.34\n[25] 24.25 28.09   Inf   Inf 28.89 25.74 16.32   Inf 23.07 32.68 20.42 17.35\n[37] 24.26 14.09 14.28 15.32  9.25  7.28   Inf  0.00 21.09\n\n$previous\n [1] 44 45 31  1 44 13 23  0 44  9 31 44 44  1 14 14  1 14 14  1 44 33 14 44  9\n[26] 45  0  0 31 14  1  0  5 25 44  9 44  1  9  9 44 44  0  0  4\n\n$gsteps\n [1]   1   4   3   2   1   2   4 Inf   1   2   3   1   1   2   3   3   2   3   3\n[20]   2   1   3   3   1   2   4 Inf Inf   3   3   2 Inf   2   3   1   2   1   2\n[39]   2   2   1   1 Inf   0   3\n\n$start\n[1] 0\n\n$end\n[1] 30\n\n$direction\n[1] \"fwd\"\n\n$type\n[1] \"earliest.arrive\"\n\nattr(,\"class\")\n[1] \"tPath\" \"list\" \n\n\nMore importantly, we can plot the hierarchical path from the given node, by simply using plot function, see Figure 17.13.\n\nplot(FwdPathway)\n\n\n\n\nFigure 13. A forward pathway (earliest reachable node) pathway of interactions with node 44\n\n\n\n\nWe may also use Graphviz to make the plot look better and hierarchical (Figure 17.14).\n\nplot(FwdPathway, edge.lwd = 0.1, vertex.col= \"blue\", pad = -4,\n     coord=network.layout.animate.Graphviz(as.network(FwdPathway), \n                   layout.par = list(gv.engine='dot',\n                              gv.args = '-Granksep=2')))\n\n\n\n\nFigure 14. An improved forward pathway with Graphviz\n\n\n\n\nAnother option is to plot the network diffusion or transmission hierarchical tree with generation time vs. clock/model time using transmissionTimeline function as in Figure 17.15. For more on these functions, readers are invited to read the function manuals and for usage, these papers offer a starting point [3, 9].\n\ntransmissionTimeline(FwdPathway, jitter = F, displaylabels = TRUE, \n                    main = \"Earliest forward path\" )\n\n\n\n\nFigure 15. Transmission hierarchical tree showing the earliest forward interaction with node 44\n\n\n\n\nAnother useful package that offers a wealth of graph level measures is tErgmStats, you may need to consult the help files which are available by using the command ?tErgmStats. One of the important functions that we can try here is the nodemix. We can use nodemix to examine who and when different actors interact with each other, see here for an example where the authors examined how low and high achievers mix with each other and with the teachers [15]. The next code demonstrates how to compute the mixing patterns between expertise levels. We then convert the time series as a dataframe, clean the names and then plot the results as demonstrated in Figure 17.16.\n\nMix_experience <- tErgmStats(Active_Network,\n  \"nodemix('expert_level')\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1)\n\nMixing <- as.data.frame(Mix_experience)\n\ncolnames(Mixing) <- gsub(\"mix.expert_level.\", \"\", colnames(Mixing))\n\nMixing$Day <- 1:73\n\nMixing_long= pivot_longer(Mixing, contains(\".\"))\n\nnames(Mixing_long)= c(\"Day\", \"Mixing\", \"Frequency\")\n\nggplot(Mixing_long, aes(Day, Frequency, group = Mixing, color = Mixing)) + \n  geom_line(alpha = .95) + theme_bw()\n\n\n\n\nFigure 16. Mixing patterns between expertise levels and the teachers."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#discussion",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#discussion",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "5 Discussion",
    "text": "5 Discussion\nLearning can be viewed as relational, interdependent, and temporal and therefore, methods that account for such multifaceted dynamic processes are required [1, 3]. We have shown the main advantages of temporal networks and the potentials it offers for modeling dynamic learning processes. These potentials or features can facilitate the modeling of the complex natural processes–including the emergence, evolution, diffusion or disappearance of learners’ activities, communities or social processes that unfold over time. Such features can augment the existing analytics method and help shed light on many learning phenomena [16]. Taking advantage of time dynamics allows us temporal evolution of co-construction of knowledge, the flow of information and the building of social relationships, to name a few examples. What is more, temporal networks allow the longitudinal modeling and analysis of interactions across longer periods of time e.g., full duration of a course, project or meeting using time-respecting paths [9, 15].\nThere are several methods that can harness the temporal dimensions of a learning process, e.g., process and sequence mining, time series methods and epistemic network analysis [27, 28]. While such methods have given a wealth of information and insights about learning processes, they fall short when it comes to the relational aspects [9]. We review here and in short the main differences between such methods of temporal networks. Process mining is a method for the discovery and modeling of a temporal process [1, 29].Yet, the relational aspect is completely ignored. The case is similar for sequence mining where the time-ordered sequences are modeled regardless of the theri interactions [27, 30, 31]. Epistemic network analysis is another method that allows the study of co-temporal interactions. However, the “temporal aspect” is limited to combining data within a temporal window and later modeling the interactions as a static network. Put another way, Epistemic network analysis is a special type of static networks where edges are defined based on co-occurrence [32]. For a comparison between the various methods see [3]."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#learning-resources",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#learning-resources",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "6 Learning resources",
    "text": "6 Learning resources\nA good place to start is to get acquainted with the cited research in the literature review section. Other good references could be the methodological paper that gives a detailed overview of temporal networks which some parts of this chapter has been built around it [33]. There are few, yet very informative tutorials that we can suggest, most notable are the tutorials by [34] and [18]. The packages used in this chapter have very informative manuals: TSNA[19], NDTV[20] and networkDynamic[35]. Some seminal papers can be recommended here, especially the following papers and books.\n\nHolme, P. (2015). Modern temporal network theory: a colloquium. European Physical Journal B, 88(9).\nHolme, P., & Saramäki, J. (2019). A Map of Approaches to Temporal Networks (pp. 1–24).\nHolme, P., & Saramäki, J. (Eds.). (2019). Temporal network theory (Vol. 2). New York: Springer."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#introduction",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#introduction",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "1 Introduction",
    "text": "1 Introduction\nEducational scientists are usually after the invisible: sense of belonging, intelligence, math ability, general aptitude, student engagement…the list of crucial, multifaceted, but not directly observable student characteristics goes on. But how do researchers measure and study what they cannot see? One solution is not looking for the invisible cause itself but for its consequences. Just like one cannot see the wind but can tell it is there from the moving leaves, researchers can indirectly infer student engagement by looking at observable aspects of students' online behavior. For example, the more engaged students are, the more effort they invest in learning (i.e., longer online time), and the more regular and frequent they post on fora. These observable variables are the gateway to the underlying engagement construct that is theorized to drive students to be engaged, and factor analysis is the key to opening that gate. If scores on a set of observed variables are all caused by the same underlying construct of interest, the relations between these variables (i.e., the covariances that indicate how increases/decreases in one behavior are related to increases/decreases in another) are an expression of this underlying construct. This is exactly the core tenet of factor analysis. The method examines the covariances between the observed variables and, from this information, extracts the underlying unobserved or latent construct (as well as students' relative scores on it).\nThe inventor of factor analysis is Spearman, who was trying to make sense of the fact that tests of widely varying cognitive abilities all positively correlated with each other. He reasoned that the cause of this was an underlying construct (or factor) called \"general intelligence\" that was causing people's performance on all of those tests [1]. Spearman's work on factor analysis was later extended by Thurstone, who believed that people's performance was influenced by more than just one latent dimension. Thurstone [2], therefore, expanded factor analysis to enable the extraction of multiple underlying constructs based on the covariances between variables. The extension allowed performance on a math test to, for example, be influenced both by math ability (the key ability that should be measured) and reading ability (the ability to accurately understand questions on the math test in order to answer them correctly).\nJöreskog introduced the final major addition to factor analysis. Although Spearman's and Thurstone's versions of factor analysis already allowed for exploring the factor structure for a given dataset, it was not yet possible to confirm if the factor structure fit well to the data and, thus, if the covariances between variables implied by the factor structure match the observed covariances in the dataset. Jöreskog [3] figured out how to estimate factor models in a way that made this possible. An added benefit of this estimation method was that it allowed for factor models in which observed variables (e.g., behaviors, tasks, or questionnaire items) were not influenced by all the assumed underlying dimensions but, for example, only by one, which was not possible in the methods of Spearman and Thurstone. This extension allowed adding theory and/or practical experience to the factor analysis. For instance, one could test the hypothesis that students' math ability consists of the separate sub-dimensions addition, subtraction, division, and multiplication by letting all tasks of a math test that involve addition belong to just one underlying addition factor, all the tasks involving multiplication to just one underlying multiplication factor, et cetera. If this multidimensional model of math ability fits the data well, students can subsequently be evaluated on each of these dimensions separately (instead of on an overall math ability factor). The approach in which researchers confirm if a specific factor model (i.e., for which both the number of underlying dimensions and the pattern of relations between these dimensions and the observed variables) fits to the data is nowadays called confirmatory factor analysis (CFA). In contrast, the more data-driven approach in which the number of underlying constructs is inferred from the data and all underlying constructs are assumed to influence all observed variables is called exploratory factor analysis (EFA).\nNowadays, both EFA and CFA are readily available in modern statistical (open-source) software and applied regularly across the social sciences in general and educational research in particular. For example, Krijnen et al. [4] used EFA to refine a typology of home literacy activities. They cautiously anticipated four hypothetical categories of home literacy activities (oral language exposure, oral language teaching, code skills exposure, and code skills teaching activities). However, since the authors did not have a strong theory or prior factor-analytical results to support this anticipated factor structure, they refrained from testing if that specific factor structure fit to the data with CFA and instead used the more data-driven EFA approach. Their results suggested that there were actually three factors underlying the observed variables in their dataset (oral language teaching, oral language exposure, and general code activities).\nIn contrast, Hofhuis et al. [5] used factor analysis to validate a shortened version of the Multicultural Personality Questionnaire (MPQ; [6]), a measure of intercultural competence, among a sample of students in an international university program. The authors wanted to determine if this short form of the MPQ (MPQ-SF) could be used instead of the longer original and if item scores of both versions of the questionnaire were influenced by the same five theorized aspects of inter-cultural competence (cultural empathy, emotional stability, flexibility, openmindedness, and social initiative). Since previous research on the original MPQ provided insight into both the number of underlying factors and the specific relations between these factors and the set of items retained on the MPQ-SF, the authors used CFA in this study. They found that the factor structure of the MPQ-SF fit well to the data and that the structure was thus comparable to that of the original MPQ.1\nIn the above, it was stressed that CFA is used when researchers have an idea about the factor structure underlying the data that they want to confirm, while EFA is used more exploratively when researchers have less concrete guiding insights for specifying the model. In practice, CFA and EFA are both used in confirmatory as well as exploratory settings and often even in the same study. Even if researchers have a well-substantiated idea about the number of constructs underlying their observations, they can use EFA to see if the number of factors found by this analysis matches their hypothesis. Similarly, researchers with competing theories about the factors underlying their observed behaviors can still use CFA to explicitly compare these competing models in terms of fit. Flora and Flake [8] discuss how neither EFA nor CFA is purely confirmatory or exploratory in more detail, arguing that, in essence, it comes down to one's specific research context. This will not further be discussed in this chapter. Instead, an integrated use of EFA and CFA often encountered in Educational Sciences is presented. The presentation is kept applied and focuses on conducting factor analysis in R. For more (technical) details, see the readings listed at the end of the chapter."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#literature-review",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#literature-review",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "2 Literature review",
    "text": "2 Literature review\nSeveral examples of factor analysis exist in learning analytics, which can be grouped broadly under two categories: factor analysis of self-reported data instruments (e.g., surveys) and factor analysis to explore students' online data. Analysis of self-reported instruments seems to be most widely used within the emerging research field of learning analytics. For instance, Whitelock‐Wainwright et al. [9] used EFA and CFA to validate an instrument that measures students' expectations of ethics, privacy, and usage of their learning analytics data. The analysis suggested a two-factor model that represented two categories of variables of ideal and predicted expectations of learning analytics. Similarly, Oster et al. [10] used EFA to validate an instrument that measures learning analytics readiness among institutions. The authors found that a five-factor model best represents the data (with the dimensions data management expertise, data analysis expertise, communication and policy application, and training). Similarly, factor analysis has been used to create an instrument for measuring variables influencing learning analytics dashboard success. For instance, Park and Jo [11] measured learning analytics dashboard success through an instrument based on Kirkpatrick's four evaluation levels (i.e., Reaction, Learning, Behavior, and Results). Through EFA, they found that five dimensions were more appropriate than four, as suggested by the original instrument, which CFA later confirmed. Similarly, Kokoç and Kara [12] used the Evaluation Framework for Learning Analytics to evaluate the impact of a learning analytics dashboard on learner performance. After conducting CFA, they found that the three-factor model of the Evaluation Framework for Learning Analytics for learners provided the best model fit for the collected data, confirming the structure of the original instrument.\nBesides the aforementioned traditional examples, factor analysis has also been used in learning analytics with students' online data logs. For instance, Hernández-García et al. [13] used factor analysis to identify predictors derived from data about students' interactions. The authors found that a three-factor model best represented students' interaction variables (with dimensions groups' team message exchanges, distribution of postings, and reciprocity of interactions). In that case, factor analysis helps find groups of predictors, understand their underlying structure, and reduce dimensionality. Similarly, Fincham et al. [14] used EFA and CFA to build a theorized model of engagement based on three groups of variables derived from online log data, analysis of sentiments, and metrics derived from the discourse of online posts. Others have applied similar approaches to study the structure of log data. For instance, Baikadi et al.[15] applied EFA to learner activities within four online courses to identify emergent behavior factors. The findings suggested a four-factor model including activity, quiz activity, forum participation, and participation in activities. Another example is the work by Wang [16], who used factor analysis for dimensionality reduction of log data, deriving predictors of learning performance from students' fine-grained actions on the learning management systems.\nThe remainder of this chapter first provides a brief description of the factor analysis model, the model that is at the basis of both EFA and CFA. Second, the steps needed to perform factor analysis in R are presented by applying EFA and CFA to education data that is openly available. The application begins with data preparation, requirements, and initial checks. Additionally, it is shown how to set aside a holdout sample from the original dataset (which is necessary for establishing the generalizability of results from an EFA and/or CFA to future samples, as explained below). After these preliminary steps, it is shown how to run an EFA and interpret the outcomes. The part ends with a thorough description of how to do a CFA and assess generalizability. The chapter ends with a discussion and recommendation for further reading."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#recap-of-the-factor-analysis-model",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#recap-of-the-factor-analysis-model",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "3 Recap of the Factor Analysis Model",
    "text": "3 Recap of the Factor Analysis Model\nFactor analysis can be seen as a set of (multiple) linear regression models where each observed variable is regressed on one or more latent factors (see Figure 1 in [17]). Like in regression, researchers get regression weights, intercepts, and error terms. They just get a set of these parameters for each observed variable in the analysis. The regression weights are referred to as loadings in factor analysis (the straight arrows in Figure 21.1) and indicate how strongly the observed variables are related to the underlying factors. The intercepts (not explicitly indicated in Figure 21.1) are the expected scores on the observed variables when the factor means are equal to zero. Finally, the error terms (the \\(\\epsilon\\)'s in Figure 21.1) capture the variance unexplained by the factors and, thus, the unique variance of the individual observed variables. The difference between the factor analysis model and a regular regression model is that the values of the factors are unobserved. Therefore, estimating a factor analysis model is more complicated than estimating a regular regression analysis model with observed predictors and requires specialized software.\n\n\n\nFigure 1. A basic 1-factor model. Each observed variable (X1 – X3) is regressed on the factor. Straight arrows indicate factor loadings (the regression weights obtained by regressing the observed variable on the factor). The \\(\\epsilon\\)'s represent errors. Curved single-headed arrows indicated variances. Each observed variable also has an intercept, but these are not explicitly indicated in the figure.\n\n\nAs mentioned in the introduction, there are two types of factor analyses: Exploratory factor analysis (EFA; Figure 20.2) and confirmatory factor analysis (CFA; Figure 20.3). The key difference is that all loadings (i.e., all variable-factor relations) are estimated in EFA. As a result, variables may load on more than one factor (referred to as cross-loadings). In contrast, in CFA, several loadings are set to zero (as thus not estimated) based on researchers' a priori hypotheses about which variables are unrelated to the underlying factors. Therefore, the CFA model is considered a restricted version of the EFA model.\n\n\n\nFigure 2. An EFA Model with two latent factors. Each observed variable (X1 - X6) is influenced by both factors. Straight arrows indicate factor loadings. Loadings for Factor 2 are depicted with dashed lines for visual clarity. The \\(\\epsilon\\)'s represent errors. Curved single-headed arrows indicate variances and curved double-headed arrows indicate covariances.\n\n\nAnother difference is the determination of the number of factors. In EFA, the number of underlying factors is determined using a data-driven approach; that is, the likely number of underlying factors for the sample data is first estimated, for example, using parallel analysis [18]. Researchers then fit a set of factor models, with the number of underlying factors in these models based on the parallel analysis result. For example, if the parallel analysis suggests 5 factors, researchers can fit models with 4, 5, and 6 underlying factors. These three models are then compared in terms of both model fit—using the Bayesian information criterion [19]—and in terms of the interpretability of the models (i.e., “Do the relations between the factors and the observed variables they load on make sense?”). All this will be discussed in the Section “Factor Analysis in R” of this chapter.\nIn contrast, the number of factors in CFA is determined based on strong a priori hypotheses. When researchers have multiple competing hypotheses, each can be translated into a separate CFA model. These models can then again be compared in terms of how well they fit the data. The hypothesis underlying the best-fitting model can be considered the most likely explanation of the data according to the sample at hand.\n\n\n\nFigure 3. A CFA Model with two latent factors. Each observed variable (X1 - X6) is influenced by only one of the underlying factors. Straight arrows indicate factor loadings. Loadings for Factor 2 are depicted with dashed lines for visual clarity. The \\(\\epsilon\\)’s represent errors. Curved single-headed arrows indicate variances and curved double-headed arrows indicate covariances."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#integrated-strategy-for-a-factor-analysis",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#integrated-strategy-for-a-factor-analysis",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "4 Integrated Strategy for a Factor Analysis",
    "text": "4 Integrated Strategy for a Factor Analysis\nAs described in the introduction, there is no clear delineation between when to use either EFA or CFA, and both methods often co-occur within the same study. Therefore, this section provides a detailed description of a principled modeling strategy that integrates both EFA and CFA. More specifically, the three steps that researchers should go through whenever latent constructs are part of their research (either because the instrument is the main focus of the study or because the latent construct is a predictor or outcome in an analysis) are discussed: (1) exploring the factor structure, (2) building the factor model and assessing fit, and (3) assessing generalizability. As a starting point, it is assumed that the researcher has already completed the initial instrument development phase for a construct of interest such as inter-cultural competence (i.e., that the researcher is using an instrument with variables/tasks/behaviors from previous research, has adjusted an instrument from previous research (e.g., by shortening, extending, translating, or revising content), or has newly developed an instrument (e.g., based on theory)). Furthermore, it is assumed that the researcher has gathered data from their population of interest (e.g., students).\n\n4.1 Step 1: Exploring the Factor Structure\nOnce the variables/tasks/behaviors have been selected and data on them have been obtained using a sample from the population of interest, you, the researcher, should start with an EFA. If a previously validated instrument is used, or if strong prior hypotheses about the underlying factor structure of the instruments are available, you should investigate whether the number of factors and the way the variables load on the factors are in line with anticipated results. Thus, the key questions are “Do variable scores believed to be caused by the same underlying factor indeed all load on the same factor?” and, if only a single underlying factor is assumed to cause the scores on a set of variables, “Do these variables indeed have strong factor loadings with only a single factor?”\nIf a new instrument is the starting point, you should determine if the factors and the pattern of loadings can be interpreted. The key questions are “Do all variables (primarily) loading on the same factor indeed have something in common substantively?” and “Are variables that load on different factors indeed qualitatively different somehow?”\nReferring back to the example about a math test, you could see that tasks involving addition, subtraction, division, and multiplication loaded on 4 distinct factors (which could then preliminarily be identified as addition-, subtraction-, division-, and multiplication ability), with each set of tasks being primarily influenced by a single factor. At this stage, you may have to make some adjustments, like removing variables without substantial loadings (e.g., loadings smaller than an absolute value of .3) on any dimensions and reestimating the EFA. Note that you should always think about the reasons for low loadings (e.g., because of an ambiguously formulated item) and not just remove variables.\n\n\n4.2 Step 2: Building the Factor Model and Assessing Fit\nAfter choosing a model with EFA, you need to refine this model and use CFA to assess the model fit (and thus how well the covariances between variables implied by the factor structure match the observed covariances in the dataset). In the EFA in the previous step, all variables were allowed to load on all factors, but often, you have theoretical or (prior) empirical information that you want to include in your analyses, and that restricts the number of these cross-loadings. In this model-building step, you could remove all factor-variable relations (i.e., factor loadings) that do not fit your theory or make substantive sense, but attention must be given to the size of the factor loadings. Close-to-zero factor loadings can be removed relatively risk-free, but larger factor loadings require more careful consideration. Even if these factor-variable relations do not make substantive sense (straight away), the data tell you they should be there. So consider them carefully; if, after closer inspection, they can be incorporated into your prior theory or assumptions, you might want to keep them; if not, you can always remove them and see if the model still fits your data.\nAfter selecting which variable-factor relationships should be removed, you can build the corresponding CFA model and fit it to the data to determine if it fits sufficiently well. If the model does not fit well, you can return to your EFA results and see if you might have to allow additional non-zero loadings (or apply other modifications discussed further below). Note that you should only add relationships to the model that make substantive sense. This process may require several rounds of adjustments until your model fits well and is substantively sound.\n\n\n4.3 Step 3: Assessing Generalizability\nAfter the previous step, you have a preliminary model that fits well with both your new or existing theory and the current data. However, since the ultimate goal should be to present instruments that can be used to measure constructs in future studies, it is essential that you also establish that your preliminary model fits new yet unknown data and, thus, that your model does not describe only your initial sample properly but also other samples from your population. Therefore, in the final step, the preliminary model must be fitted to a new dataset from the same population as the data used to build the preliminary model. This final validation step can be referred to as cross-validation.\nTo assess the generalizability, one could collect a second dataset. However, in practice, gathering data more than once for instrument development purposes is often unfeasible or undesirable. A suitable alternative is to randomly split one dataset into two parts: one sample on which you perform Steps 1 (exploring the factor structure using EFA) and 2 (building the factor model and assessing fit with CFA) and one so-called holdout sample, which you set aside for Step 3 (assessing generalizability). If the CFA model fits the holdout sample, you can be more confident that your instrument can be used in future studies and settle on it as your final model. On the other hand, if the preliminary model does not fit the holdout sample well, you have to conclude that you have not found an adequate instrument yet. In that case, the sources of misfit between the CFA and the holdout sample need to be investigated (more on this below when local fit and modification indices are discussed), and findings from this inspection need to be used to update your theory/model. This updated theory/model then needs to be investigated again by going through all the steps discussed above on a new (split) dataset.\nThe above steps present a suitable factor modeling strategy for any study using instruments to measure latent dimensions. The only situation in which you could theoretically fit a CFA model immediately and assess its fit is when using an existing instrument on a sample from a population for which the instrument had already been validated in previous research. However, even in that situation, going through all the above steps is advisable because your sample might differ in important ways from the ones on which the instrument was validated, which could bias your results."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#factor-analysis-in-r",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#factor-analysis-in-r",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "5 Factor Analysis in R",
    "text": "5 Factor Analysis in R\nIn the following, you are taken through the essential steps of investigating the factor structure using both EFA and CFA in the open-source software R. In describing the steps, the following is addressed: checking data characteristics for suitability for EFA/CFA, deciding on the number of factors, assessing global and local model fit, and evaluating the generalizability of the final factor model. To this end, a dataset is used to which factor analysis has been applied before [20]. The dataset contains survey data about teacher burnout in Indonesia. In total, 876 respondents have answered questions on five domains: Teacher Self-Concept (TSC, 5 questions), Teacher Efficacy (TE, 5 questions), Emotional Exhaustion (EE, 5 questions), Depersonalization (DP, 3 questions), and Reduced Personal Accomplishment (RPA, 7 questions). Thus, the total number of variables equals 25. The questions were assessed on a 5-point Likert scale (ranging from 1 = “never” to 5 = “always”). For more information on the dataset, see the data chapter of this book [21].\nThe first section shows the necessary preparation, which involves reading in the data, evaluating whether the data are suited for factor analysis, and setting apart the holdout sample needed for assessing the generalizability. The next two sections show how to conduct an EFA to arrive at a preliminary factor model/factor structure (Step 1) and refine this model using CFA (Step 2). The final section shows how to test the generalizability of the refined factor model using cross-validation (Step 3).\n\n5.1 Preparation\nIn order to follow all the steps, you have to install the following packages with install.packages()(you only have to install packages the first time you want to use them; therefore, the commands are hashtagged below) and load them with library() whenever you open the R script.\n\nlibrary(lavaan) # install.packages(\"lavaan\")\nlibrary(psych) # install.packages(\"psych\")\nlibrary(semTools) # install.packages(\"semTools\")\nlibrary(effectsize) # install.packages(\"effectsize\")\nlibrary(rio) # install.packages(\"rio\")\nlibrary(tidyr) # install.packages(\"tidyr\")\nlibrary(ggplot2) # install.packages(\"ggplot2\")\nlibrary(devtools) # install.packages(\"devtools\")\nlibrary(sleasy) # install.packages(\"combinat\"); install_github(\"JoranTiU/sleasy\")\n\n\n5.1.1 Reading in the data\nThe data can be read in, and the variable names can be extracted with the following commands: \n\ndataset <- import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")\nvar_names <- colnames(dataset)\n\nIf not all variables in the dataset should be used for the factor analysis, you should only add the relevant variables to the var_name object. The commands below will then take this into account automatically.\n\n\n5.1.2 Are the data suited for factor analysis?\nSeveral data characteristics are necessary for both EFA and CFA. First, the variables must be continuous. Variables are seldom truly continuous, but they can be treated as such if they were assessed on a scale with at least five response categories [22]. If the variables are not continuous, factor analysis can still be conducted, but a specific type of estimation for categorical data is required. Note that this is beyond the scope of this chapter (interested readers are referred to [23]). Moreover, the scale on which the observed variables are assessed should be the same, which may not hold in certain educational data. If the variables have been measured on different scales, or if the variables are measured on the same scale, but the range of observed scores on the variables differs substantially between variables (e.g., some variables have scores ranging from 1 to 5, while others have scores ranging only from 2 to 4), the variables should be transformed before the factor analysis to make their scales more comparable. The following command can be used to inspect each variables’ range:\n\ndescribe(dataset)\n\nAll variables were assessed on 5-point Likert scales, and from the output, you can see that all variables have very similar observed score ranges. Therefore, you can treat them as continuous, and transformation is not necessary (for information on how to transform data in R, see [24]).\nSecond, the sample size needs to be sufficiently large. There are several rules of thumb in the literature. Some simply state that a sample size of about 200 should be targeted, although smaller samples may be sufficient for simpler models (e.g., models with fewer factors and/or stronger relations between the factors and observed variables), while more complicated models (e.g., models with more factors and/or weaker relations between the factors and observed variables) will require larger samples. Other rules are based on the ratio between sample size and the number of estimated parameters (i.e., factor loadings, intercepts, and error variances). Bentler and Chou [25] recommend having 5 observations per estimated parameter, while Jackson [26] recommends having 10, and preferably 20 observations, for each parameter you want to estimate (e.g., for a one-factor model with 10 variables, one should aim for 30 (10 factor-loadings + 10 intercepts + 10 error-variances) * 10 = 300 cases. Remember that these recommendations are for the data the model is fitted to. Since you also need a holdout sample to assess the generalizability of your model, you need to have about twice the number of observations! The sample size can be assessed by asking for the number of rows in your dataset with the following command:\n\nnrow(dataset) # 876\n\n[1] 876\n\n\nFor the example data with 25 variables that are assumed to measure 5 latent constructs, you have to estimate 25 intercepts, 25 residual variances, and 5 * 25 = 125 factor loadings. This results in a total of 175 parameters. Looking at the output, you can conclude that the sample size is sufficiently large for both EFA and CFA according to the guidelines by Bentler and Chou [25] (5 * 175 = 875) but not those of Johnson [27]. Since this dataset does not have twice the recommended sample size, you should not set aside a holdout sample and save the validation of the model for a future study. However, for illustration purposes, it will nevertheless be shown how to create a holdout subset for evaluating the generalizability of the final factor model.\nNext, there need to be sufficiently large correlations between the variables. Otherwise, there is no point in looking at the factor structure. You can rule out that the variables in the dataset are uncorrelated with Bartlett’s test [28], which tests whether the correlation matrix is an identity matrix (a matrix with off-diagonal elements equal to zero) and, thus, whether the variables are uncorrelated. The null hypothesis of this test is that the correlation matrix is an identity matrix. If the null hypothesis is rejected, it can be concluded that the variables are correlated and, thus, that you can continue with the factor analysis. With the following command, you test whether the p-value for the Bartlett’s test is smaller than an alpha level of 0.05 and, thus, if the null hypothesis of “no correlation between variables” can be rejected:\n\n(cortest.bartlett(R = cor(dataset[, var_names]), n = nrow(dataset))$p.value) < 0.05 \n\n[1] TRUE\n\n\nWith the argument “R”, you provide the correlation matrix for the data (specifically for the variables that shall be part of the factor analysis), and with the argument “n” you determine the sample size, which is equal to the number of rows. The p-value is indeed smaller than 0.05. Thus, the variables correlate.\nIn addition to checking for correlations between variables, it is also relevant to determine if there is enough common variance among the variables. You can assess this with the Kaiser-Meyer-Olkin (KMO) test [29]. The KMO statistic measures what proportion of the total variance among variables might be common variance. The higher this proportion, the higher the KMO value, and the more suited the data are for factor analysis. Kaiser [29] indicated that the value should be at least .8 to have good data for factor analysis (and at least .9 to have excellent data). With the following command, you obtain the results:\n\nKMO(dataset)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = dataset)\nOverall MSA =  0.94\nMSA for each item = \nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5  DE1 \n0.96 0.96 0.95 0.94 0.96 0.93 0.96 0.94 0.94 0.96 0.95 0.94 0.95 0.94 0.97 0.87 \n DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n0.86 0.92 0.91 0.91 0.95 0.94 0.96 \n\n\nThe overall KMO value equals 0.94. Thus, the data are excellent for data analysis.\nNext to the necessary data characteristics, you need to be aware of the non-normality of the variables and missing data. A robust estimation method is required if the variables are not normally distributed. If the data contain missing values for one or more variables, this must also be accounted for in the estimation. How to do this will be described below. Normality can be assessed by inspecting the variables’ histograms2:\n\ndataset |> pivot_longer(2:ncol(dataset), \n names_to = \"Variable\", values_to=\"Score\") |>\n   ggplot(aes(x=Score)) + geom_histogram(bins=6) +\n     scale_x_continuous(limits=c(0,6),breaks = c(1,2,3,4,5)) +\n     facet_wrap(\"Variable\",ncol = 4,scales = \"free\" ) + theme_minimal()\n\nWarning: Removed 44 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nFigure 4. Histogram of the responses to each of the items\n\n\n\n\nLooking at Figure 20.4, you can see that the distribution of the variables is somewhat left-skewed. Therefore, an estimation method that is robust against non-normality should be used.\nNext, you can check for missing data with the following command:\n\ncolSums(is.na(dataset))\n\nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5  DE1 \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n   0    0    0    0    0    0    0 \n\n\nThere are no missing observations for any of the variables in this dataset.\n\n\n5.1.3 Setting a Holdout Sample Apart\nOnce you know that the data are suited for factor analysis, you can consider setting a holdout sample apart to assess the generalizability of your findings, as explained before. However, it is important to consider the sample size in your decision. As indicated above, the minimum required sample size should at least be 5 (and preferably 10 or 20) times the number of parameters you will estimate. Do not set a holdout sample apart unless your sample size is approximately twice as large as the minimum required sample size (or larger). Otherwise, you will not have enough data to build an appropriate model in the first place. The validation of the final model then needs to be done in future research. Note, however, that the number of parameters for a CFA model is generally smaller than that for an EFA model. Therefore, it is okay if the holdout sample is somewhat smaller than the model building sample.\nAs was already determined above, the sample size was not twice the minimum required sample size for a model with 25 variables and 5 latent factors, but for illustrative purposes, a holdout sample is set apart nevertheless. To this end, you can randomly assign 438 rows to a model-building and holdout dataset. For this, you can use the following commands:\n\nset.seed(19)\nind <- sample (c (rep(\"model.building\", 438), rep (\"holdout\", 438)))\ntmp <- split (dataset, ind)\nmodel.building <- tmp$model.building\nholdout <- tmp$holdout\n\nWith the first line of code, you set a seed. Setting a seed ensures that you get the same random sequence of elements every time you rerun the code, which is crucial for replicating your results. Then, you create a vector “ind” that contains 438 times the terms “model.building” and “holdout”, respectively, in random order. This gives you a total of 876 classifications, one for each participant (i.e., row) in your data. Subsequently, you create a temporal list termed “tmp”, which contains two datasets: for each row number, the function split() checks whether it is assigned the label “model.building” or “holdout” and assigns this row to the respective dataset accordingly. For example, suppose the vector “ind” has as the first three elements “model.building”, “model.building”, and “holdout”. In that case, the first two rows of the dataset are assigned to the model-building dataset, and the third observation is assigned to the holdout dataset. In the last step, the two new datasets are extracted from the list and stored in objects named “model.building” and “holdout”. You will use the model-building data for all the following analyses until the section “Assessing Generalizability”.\n\n\n\n5.2 Step 1: Exploring the Factor Structure\nThe first step in exploring the factor structure is to determine how many dimensions are likely underlying the construct of interest. In this tutorial, you will see how to determine this using a combination of two commonly used data-driven approaches: parallel analysis [18] and the Bayesian information criterion (BIC; [19]). These two methods complement each other greatly: Parallel analysis indicates a range for the number of underlying dimensions, and the BIC tells us which specific number of dimensions from this range fits the data best. Parallel analysis is a simulation-based method that chooses the number of factors by comparing the amount of variance explained by a certain number of factors in your data (with this amount of variance explained by each factor being called the factor’s eigenvalue) to the amount of information that the same number of factors would explain on average across many parallel simulated datasets (i.e., with the same number of variables and observations), but with zero correlations between all variables. This comparison allows for testing if the amount of explained variance in your data is larger than expected based on purely random sampling error alone. The number of factors chosen by parallel analysis is the number for which the explained variance in your data is larger than the explained variance for the simulated data. Details about how this method works are beyond the scope of this chapter but can be found in the help file3 of the fa.parallel() function to perform the analysis.\nWith the argument x, you specify that you want to use your model-building data and, more specifically, all the columns corresponding to your variables. With the second argument fa = \"fa\", you specify that you assess the best number of factors for factor analysis and not the best number of components for principal components analysis, which is a related yet different method for finding dimensions in data (e.g., [31]). The output consists of two parts: a message and a figure:\n\nfa.parallel(x = model.building[,var_names], fa = \"fa\")\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n\n\n\n\nFigure 5. Parallel analysis\n\n\n\n\nThe message indicates that five factors are likely underlying the data. Figure 20.5 shows that with more than five factors (x-axis), the amount of information explained by the factors (on the y-axis) in your data is lower than for the simulated data.\nIt is important to note that parallel analysis is a purely data-driven method, and the solution depends on the specific sample. Therefore, you should see the 5 factors only as a starting point (such that the number of underlying factors is likely around five) and treat parallel analysis as giving you a plausible range for the number of underlying factors, equal to the solution plus and minus one factor (or more, if you want to). The final decision for the number of underlying factors is made by running several factor models—one for each plausible number of underlying factors as determined using the parallel analysis—and comparing these models in terms of interpretability (i.e., does the pattern of variable-factor relations make substantive sense and/or does it fit prior knowledge/assumptions) and fit (using the BIC). The BIC can be used for model selection for many analyses, including factor analysis, and the criterion balances the fit of the factor model to the data on the one hand and model parsimony (i.e., simplicity) on the other by penalizing models for each parameter they contain. The lower the BIC value, the better, so if the smallest BIC value corresponds to the model with five factors, you have stronger support for the five-factor solution. Still, the final decision should take interpretability into account. If the parallel analysis and BIC disagree, your final decision should consider interpretability even more.\nThe exploratory factor analysis can be performed with the following command:\n\nEFA <- efa (data = model.building[, var_names], nfactors = 4:6, \n            rotation = \"geomin\", estimator = \"MLR\", meanstructure = TRUE)\n\nThe efa() function is part of lavaan [32], which is a very popular, open-source R package that can be used for estimating various latent variable models, including factor analysis and structural equation modeling (see Chapter XX). With the first argument data, you again specify the dataset on which you want to perform factor analysis, nfactors indicates the range of factors for which you want to retrieve the results. Next, the argument rotation pertains to identifying the model, which is only necessary for EFA and not CFA because, in the latter, you specify restrictions (i.e., restricting loadings to be equal to zero) that automatically lead to the identification of the model. In EFA, where you do not have restrictions, there are an infinite number of solutions identical in fit; that is, your factor matrix with loadings can be transformed or “rotated” in infinite ways. To clarify, consider the following: with (continuous) observed variables, you can visualize your data using scatterplots in which each point represents an individual’s combination of scores on two variables that are represented by the x-axis and the y-axis. With observed variables, you have individuals’ actual scores on the variables in your dataset. Therefore, the locations of the points relative to the x-axis and y-axis (and, therefore, the axes’ position) are fixed. With factors, the situation is different. The latent variables are unobserved, which means that the position of the axes (which now represent latent variables) is not “pinned down” by the data. Considering Figure 20.6, for example, both axis orientations are equally likely with latent variables. Note that in both plots, the data points are in the exact same location; only the orientations of the axes representing the (in this case, two) latent factors are different. Rotation is about choosing one out of all the many possible orientations of the axes.\nThe choice of the orientation of the axes is generally made based on how easy it makes the interpretation of the factors. Typically, the aim is to find a so-called simple structure [33] in which the orientation of the axes ensures that each observed variable is strongly related to one factor (i.e., has a large loading on one factor) and is as unrelated as possible to all others (i.e., has small cross-loadings, for example, smaller in absolute value than .3). This simple structure makes interpreting the factors (which is done by looking for a common theme in the variables that load on them) easier.\n\n\n\nFigure 6. The two possible axis orientations (out of an infinite number of plausible orientations) show that with rotation, the data points stay in exactly the same place, but the axes representing the latent factors (here, two factors) change.\n\n\nIn short, rotation is like taking a good selfie of yourself in front of the Eiffel Tower. You want both yourself and the tower to be clearly visible in the picture, so you move your camera until you get a clear shot of both. You and the Eiffel Tower both stay in exactly the same place! What changes is the vantage point, or angle, from which you look at both. Similarly, rotation moves your vantage point (the axes) to make the factors stand out clearly in your results, while the position of your observed data does not change.\nGoing back to the rotation argument in the code above, you can use geomin, a method of rotation that allows your factors to be correlated with each other, which is a reasonable assumption in educational settings. For other rotation options, see [23].\nThe argument estimator allows you to choose different estimation procedures. The default is standard maximum likelihood (“ML”) estimation. However, for the current data, a robust maximum likelihood (“MLR”) estimation was chosen to account for small violations of the normality assumption. If the data would contain missing values, you could add the argument “missing” and specify it to be equal to “fiml”, which corresponds to a full information maximum likelihood approach and is a sensible approach if you have at least missing at random (MAR) data (details about missing data mechanisms are beyond the scope of this tutorial but can be found in the lavaan tutorial [23]). The final argument meanstructure = TRUE is necessary if you want to estimate intercepts for the observed variables as well, as opposed to just estimating variances and covariances. Note that if you add the “missing” argument to your model, meanstructure will be set to TRUE automatically.\nWith the following command, you can extract and sort the BIC values in ascending order.\n\nsort(fitMeasures(EFA)[\"bic\",])\n\nnfactors = 5 nfactors = 4 nfactors = 6 \n    18142.38     18167.49     18189.29 \n\n\nThe output indicates that the model with five factors is the best according to the BIC. Thus, the two techniques to determine the number of factors agree. From the original article from which the data for this tutorial was obtained, it is known that the expected number of factors was also five. Therefore, continuing the model building with the five-factor solution for this tutorial makes the most sense.\nWith the following command, you obtain the factor loadings for the five factors. Note that, by default, lavaan gives you standardized loadings, which means that the loadings can be interpreted as correlations between variables and factors.\n\nEFA$nf5\n\n\n         f1      f2      f3      f4      f5 \nTSC1  0.584*                              . \nTSC2  0.487*                              . \nTSC3  0.637*                      .         \nTSC4  0.578*      .               .         \nTSC5  0.547*                              . \nTE1           0.728*              .         \nTE2       .   0.672*                        \nTE3           0.708*      .                 \nTE4           0.651*              .         \nTE5           0.337*      .       .         \nEE1               .   0.469*      .         \nEE2       .           0.689*                \nEE3                   0.768*                \nEE4       .           0.732*              . \nEE5               .   0.479*      .         \nDE1                  -0.353*  0.744*      . \nDE2               .           0.821*        \nDE3                       .   0.755*        \nRPA1                                  0.851*\nRPA2                                  0.906*\nRPA3                                  0.624*\nRPA4                      .       .   0.350*\nRPA5                      .       .   0.338*\n\n\nIn the output, all loadings between the variables (rows) and factors (columns) larger than an absolute value of .3 are shown by default. Inspecting the results, you can clearly see a simple structure such that every variable loads on only one factor. An exception is variable DE1, which positively loads on factor 4 with the other DE variables and negatively on factor 3 with the EE variables. Besides this cross-loading, the results align with the theoretical model as all TSC variables, all TE variables, all EE variables, all DE variables, and all RPA variables load on the same factor, respectively. In the next step, the model can be further refined based on fit. Since the model without the cross-loading is entirely in line with theory, the loading of DE1 on factor 3 will be set equal to zero in the CFA in the next section. However, if the CFA model does not fit well, putting back this cross-loading would be the first logical step.\n\n\n5.3 Step 2: Building the Factor Model and Assessing Fit\nThe first step in building the model is to describe the model you want to estimate using special lavaan syntax. The arguments relevant for this tutorial are “=~”, which can be read as “is measured by”, and “~~”, which translates into “is correlated with”. In the following, a model is specified in which the 5 factors TSC, TE, EE, DE, and RPA are measured by different sets of variables (in line with theory and the EFA results from the previous step), separated by “+”. Moreover, it is explicitly stated that correlations between factors should be estimated. Intercepts are not explicitly included in the model, but these are included again by adding the argument meanstructure = TRUE to the command when estimating the CFA model.\n\nCFA_model <-'\n#  Regressing items on factors\nTSC =~ TSC1 + TSC2 + TSC3 + TSC5\nTE  =~ TE1  + TE2  + TE3  + TE5\nEE  =~ EE1  + EE2  + EE3  + EE4\nDE  =~ DE1  + DE2  + DE3\nRPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n# Correlations between factors\nTSC ~~ TE\nTSC ~~ EE\nTSC ~~ DE\nTSC ~~ RPA\n\nTE ~~ EE\nTE ~~ DE\nTE ~~ RPA\n\nEE ~~ DE\nEE ~~ RPA\n\nDE ~~ RPA\n'\n\nThe next step is to perform the CFA on the model-building data using the specified CFA model with the following command:\n\nCFA <- cfa(model = CFA_model, data = model.building[,var_names], \n           estimator = \"MLR\", std.lv = TRUE, meanstructure = TRUE)\n\nWhich model should be used is specified by the argument model. The arguments data and estimator are the same as in the code for the EFA. The argument std.lv is used to get results similar to the EFA. Since factors are not directly observed, their scale has to be set, which can be done in two different ways: i) by fixing one of the factor-loadings of each factor to 1 (which will set the scale of the factor equal to the scale of the observed variable which loadings was fixed), or ii) by setting the variance of the factor equal to 1. Here, the second option was chosen. As already mentioned, the final argument meanstructure is necessary if you also want to estimate the intercepts of the observed variables.\nAfter performing the CFA, you can assess how well the model fits to the data. There are two types of fit measures: global and local. You can start with the global fit measures that describe how well the model as a whole fits to the data. Many different global model fit indices exist in the literature. Kline [34] suggests that at least the following four indices should be considered: (1) the Chi-squared significance test, which tests whether the model has a perfect fit to the data, that is, if the model can perfectly recreate the observed relations between variables; (2) the comparative fit index (CFI), which compares the fit of the chosen model to the fit of a model assuming zero correlations between variables; (3) the root mean square error of approximation (RMSEA), which is closely related to the Chi-squared test, but does not test for perfect fit and instead quantifies (approximate) fit between the model and the data in a single number; and (4) the standardized root mean square residual (SRMR) which summarizes the difference between the sample covariance matrix of the variables and the model-implied covariance matrix into one number. Unlike the Chi-squared significance test, the CFI, RMSEA, and SRMR are no tests and assess approximate fit.\nEach fit measure is accompanied by rules of thumb to decide whether or not a model fits sufficiently to the data. The Chi-squared significance test should be nonsignificant because the null hypothesis is that the model fits to the data perfectly. It is important to note that with increasing sample size, the null hypothesis of perfect fit is easily rejected. Therefore, you should not base your decision on whether the model fits too much on this test.\nRegarding the other three measures, the CFI should be larger than 0.9 [35], the RMSEA point estimate and the upper bound of the 95 percent confidence interval should be smaller than 0.05 [36, 37], and the SRMR should be smaller than 0.08 [35]. The fit measures and their interpretation can be obtained with the following command:\n\nglobalFit(CFA)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 318.8407 with p-value\n          = 1.332268e-15\n\nCFI = 0.9476614\n\nRMSEA = 0.05332242; lower bound = 0.04589762;\n      upper bound = 0.06076663\n\nSRMR = 0.04353874\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nInspecting the output, you can see that the Chi-squared significance test rejected perfect fit, but that approximate fit holds according to the CFI and the SRMR. Ideally, at least three of the fit measures should indicate appropriate fit, but for the sake of this tutorial’s brevity, it was decided to continue with the model without further adjustments. In practice, you may further adjust the model, for example, by including the cross-loading between DE1 and factor 3 again, and re-evaluate fit by fitting the updated model and running the globalFit() function again.\nAll measures above inspect the global fit of the model, so if the model as a whole matches the data well. While informative and necessary, the above measures can miss local misfit between the model and the data. If, for example, the 5-factor model describes the relations between all but one of the observed variables well, then the global fit of the model will likely be sufficient even though the estimates of the relations between the ill-fitting observed variable and all others will be completely wrong. Because of this, you should also inspect the local fit of your model; that is, if every part of your model fits the data well. There are many local fit measures that you could use [38], but the most straightforward way of assessing local fit is to look at the absolute difference between the model-implied and the sample covariance matrix. These are the same two matrices that the SRMR is based on, but instead of quantifying the total difference between the two in one number, you obtain the difference between the two matrices for every variance and covariance separately. You can see how much the two matrices deviate for each pair of variables, as well as the maximum difference between the two matrices, by using the following command:\n\nlocalFit(CFA)\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3   EE4\nTSC1 0.000                                                                  \nTSC2 0.012 0.000                                                            \nTSC3 0.007 0.012 0.000                                                      \nTSC5 0.007 0.002 0.010 0.000                                                \nTE1  0.019 0.000 0.009 0.010 0.000                                          \nTE2  0.025 0.014 0.031 0.021 0.011 0.000                                    \nTE3  0.013 0.010 0.048 0.005 0.003 0.008 0.000                              \nTE5  0.025 0.028 0.032 0.022 0.012 0.026 0.005 0.000                        \nEE1  0.013 0.010 0.004 0.016 0.042 0.044 0.001 0.072 0.000                  \nEE2  0.004 0.009 0.025 0.003 0.029 0.050 0.027 0.043 0.002 0.000            \nEE3  0.013 0.015 0.039 0.013 0.021 0.042 0.006 0.081 0.012 0.001 0.000      \nEE4  0.002 0.002 0.000 0.013 0.042 0.021 0.006 0.039 0.017 0.017 0.010 0.000\nDE1  0.011 0.019 0.015 0.002 0.010 0.026 0.011 0.036 0.010 0.048 0.042 0.040\nDE2  0.014 0.018 0.030 0.011 0.008 0.025 0.032 0.059 0.058 0.031 0.012 0.052\nDE3  0.000 0.008 0.041 0.021 0.023 0.006 0.012 0.019 0.048 0.015 0.022 0.012\nRPA1 0.008 0.015 0.034 0.011 0.013 0.022 0.001 0.012 0.011 0.018 0.019 0.041\nRPA2 0.006 0.008 0.044 0.007 0.021 0.004 0.009 0.008 0.015 0.016 0.002 0.053\nRPA3 0.041 0.016 0.012 0.003 0.006 0.010 0.017 0.034 0.035 0.008 0.022 0.009\nRPA4 0.020 0.000 0.003 0.031 0.001 0.027 0.031 0.039 0.042 0.035 0.031 0.053\n       DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                          \nTSC2                                          \nTSC3                                          \nTSC5                                          \nTE1                                           \nTE2                                           \nTE3                                           \nTE5                                           \nEE1                                           \nEE2                                           \nEE3                                           \nEE4                                           \nDE1  0.000                                    \nDE2  0.004 0.000                              \nDE3  0.002 0.006 0.000                        \nRPA1 0.008 0.006 0.002 0.000                  \nRPA2 0.010 0.025 0.024 0.024 0.000            \nRPA3 0.002 0.016 0.021 0.009 0.017 0.000      \nRPA4 0.006 0.056 0.074 0.046 0.011 0.052 0.000\n\n$max_misfit\n[1] 0.08051991\n\n\nBased on the local fit evaluation, you can conclude that no local misfit is present since the biggest difference between the two matrices is only .08, which is small compared to the scale of the observed variables. If local misfit is present, for example, if the correlation between two observed variables had been much larger than predicted by the model, further adjustments could be made to your model that specifically address the source of local misfit, like adding an additional covariance between these observed variables. However, these adjustments should always make sense according to theory! Never just add parameters to your model to improve its fit.\nThis concludes the assessment of fit. The last part of this model-building step is to look at the loadings of the final model with the following command:\n\ninspect(object = CFA, what = \"std\")$lambda\n\n       TSC    TE    EE    DE   RPA\nTSC1 0.657 0.000 0.000 0.000 0.000\nTSC2 0.692 0.000 0.000 0.000 0.000\nTSC3 0.628 0.000 0.000 0.000 0.000\nTSC5 0.726 0.000 0.000 0.000 0.000\nTE1  0.000 0.789 0.000 0.000 0.000\nTE2  0.000 0.745 0.000 0.000 0.000\nTE3  0.000 0.788 0.000 0.000 0.000\nTE5  0.000 0.649 0.000 0.000 0.000\nEE1  0.000 0.000 0.739 0.000 0.000\nEE2  0.000 0.000 0.802 0.000 0.000\nEE3  0.000 0.000 0.786 0.000 0.000\nEE4  0.000 0.000 0.760 0.000 0.000\nDE1  0.000 0.000 0.000 0.665 0.000\nDE2  0.000 0.000 0.000 0.640 0.000\nDE3  0.000 0.000 0.000 0.738 0.000\nRPA1 0.000 0.000 0.000 0.000 0.849\nRPA2 0.000 0.000 0.000 0.000 0.854\nRPA3 0.000 0.000 0.000 0.000 0.788\nRPA4 0.000 0.000 0.000 0.000 0.587\n\n\nThe loadings are very comparable to the ones of the EFA, which is not surprising given that only a single cross-loading (with absolute value > .3) was removed. Note that if you would want to extract other model parameters like the factor correlations, you could use the inspect() command without the “$lambda` at the end.\n\n\n5.4 Step 3: Assessing Generalizability\nThe final step is assessing the generalizability of the CFA model from Step 2 by fitting the same model to the holdout sample. If the model fits this alternative dataset, too, you can be more confident that your factor model applies more generally and can capture the underlying structure of your measurement instrument in future studies and samples as well. To assess the generalizability, you use the same code as in Step 2, but now specify your holdout sample under the data argument.\n\nCFA_holdout <- cfa(model = CFA_model, data = holdout[,var_names], \n                   estimator = \"MLR\", std.lv = TRUE, meanstructure = TRUE)\n\nAfter fitting your CFA model to the holdout sample, fit measures and their interpretation can again be obtained with the globalFit() command.\n\nglobalFit(CFA_holdout)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 339.7732 with p-value\n          = 0\n\nCFI = 0.9429698\n\nRMSEA = 0.05639005; lower bound = 0.04898985;\n      upper bound = 0.06383685\n\nSRMR = 0.04161716\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nInspecting the output, you can see that the fit of the model to the holdout sample is very comparable to its fit to the model-building data. Again, the Chi-squared significance test rejects perfect fit, but approximate fit holds according to the CFI and the SRMR.\nLocal fit is also tested with the same command as in Step 2 but again applied to your results on the holdout sample.\n\nlocalFit(CFA_holdout)\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3   EE4\nTSC1 0.000                                                                  \nTSC2 0.010 0.000                                                            \nTSC3 0.012 0.015 0.000                                                      \nTSC5 0.007 0.007 0.005 0.000                                                \nTE1  0.023 0.023 0.003 0.014 0.000                                          \nTE2  0.027 0.012 0.019 0.008 0.008 0.000                                    \nTE3  0.012 0.010 0.024 0.008 0.012 0.002 0.000                              \nTE5  0.019 0.014 0.008 0.002 0.046 0.006 0.013 0.000                        \nEE1  0.037 0.023 0.009 0.005 0.035 0.009 0.002 0.011 0.000                  \nEE2  0.028 0.003 0.003 0.019 0.038 0.016 0.069 0.008 0.033 0.000            \nEE3  0.032 0.047 0.012 0.017 0.024 0.017 0.004 0.071 0.026 0.019 0.000      \nEE4  0.006 0.033 0.003 0.002 0.027 0.002 0.002 0.048 0.015 0.020 0.004 0.000\nDE1  0.056 0.005 0.007 0.007 0.005 0.020 0.003 0.032 0.037 0.072 0.020 0.036\nDE2  0.005 0.029 0.032 0.061 0.012 0.014 0.046 0.006 0.024 0.038 0.034 0.018\nDE3  0.012 0.019 0.022 0.002 0.034 0.016 0.014 0.005 0.057 0.032 0.050 0.020\nRPA1 0.019 0.009 0.012 0.028 0.018 0.001 0.003 0.004 0.030 0.031 0.037 0.003\nRPA2 0.009 0.020 0.023 0.001 0.017 0.016 0.018 0.004 0.003 0.045 0.008 0.051\nRPA3 0.000 0.007 0.004 0.009 0.000 0.006 0.000 0.028 0.011 0.021 0.025 0.002\nRPA4 0.018 0.015 0.006 0.014 0.021 0.019 0.040 0.036 0.049 0.023 0.046 0.023\n       DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                          \nTSC2                                          \nTSC3                                          \nTSC5                                          \nTE1                                           \nTE2                                           \nTE3                                           \nTE5                                           \nEE1                                           \nEE2                                           \nEE3                                           \nEE4                                           \nDE1  0.000                                    \nDE2  0.020 0.000                              \nDE3  0.019 0.006 0.000                        \nRPA1 0.014 0.029 0.004 0.000                  \nRPA2 0.006 0.010 0.005 0.020 0.000            \nRPA3 0.030 0.006 0.026 0.016 0.017 0.000      \nRPA4 0.008 0.028 0.046 0.057 0.005 0.093 0.000\n\n$max_misfit\n[1] 0.09310116\n\n\nResults show that the local fit is sufficient for the holdout sample as well (the biggest absolute difference between the two matrices is only .09) and that it is again comparable to the results on the model-building data.\nLastly, you can look at the loadings of the final model when fitted to the holdout sample.\n\ninspect(object = CFA_holdout, what = \"std\")$lambda\n\n       TSC    TE    EE    DE   RPA\nTSC1 0.679 0.000 0.000 0.000 0.000\nTSC2 0.689 0.000 0.000 0.000 0.000\nTSC3 0.691 0.000 0.000 0.000 0.000\nTSC5 0.702 0.000 0.000 0.000 0.000\nTE1  0.000 0.694 0.000 0.000 0.000\nTE2  0.000 0.772 0.000 0.000 0.000\nTE3  0.000 0.819 0.000 0.000 0.000\nTE5  0.000 0.677 0.000 0.000 0.000\nEE1  0.000 0.000 0.749 0.000 0.000\nEE2  0.000 0.000 0.794 0.000 0.000\nEE3  0.000 0.000 0.781 0.000 0.000\nEE4  0.000 0.000 0.801 0.000 0.000\nDE1  0.000 0.000 0.000 0.677 0.000\nDE2  0.000 0.000 0.000 0.659 0.000\nDE3  0.000 0.000 0.000 0.766 0.000\nRPA1 0.000 0.000 0.000 0.000 0.851\nRPA2 0.000 0.000 0.000 0.000 0.867\nRPA3 0.000 0.000 0.000 0.000 0.700\nRPA4 0.000 0.000 0.000 0.000 0.618\n\n\nAgain, you can see that results from the model-building data and holdout sample are very comparable, as the factor loadings are similar to before.\nSince the model fits the holdout sample sufficiently and equally well as the model-building data and parameter estimates are comparable between the two datasets, you can conclude that the model’s generalizability is okay. Had the model not fit the holdout sample sufficiently, you would have to conclude that while the CFA model from Step 2 fits the model-building data well, you cannot be certain that it reflects a generally applicable structure of your measure and that the factor structure needs to be further refined. Since you already used your holdout sample in this phase, however, this further refinement would require collecting a new set of data that can be split into a model-building and holdout sample and going through all three steps again."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#conclusion",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#conclusion",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nFactor analysis is a great way to study constructs that are not directly observable. Of course, factor analysis has vast applications across several fields that are usually interdisciplinary and has been extended in several ways (e.g., to multi-group factor analysis, which will play an important role in the next chapter, where you will also see a discussion of the important topic of measurement invariance). This chapter serves mainly as a primer to introduce and demonstrate the basics of the method and to get readers interested and confident in applying the method themselves."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#further-readings",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#further-readings",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "7 Further readings",
    "text": "7 Further readings\nIn this chapter, you have seen an introduction and tutorial on how to apply factor analysis in educational research. To learn more about factor analysis in general, you can consult:\n\nKline, R. B. (2015). Principles and Practice of Structural Equation Modeling (4 ed.). Guilford Press.\n\nTo learn more about the difference between EFA and CFA, you can consult:\n\nFlora, D. B., & Flake, J. K. (2017). The purpose and practice of exploratory and confirmatory factor analysis in psychological research: Decisions for scale development and validation. Canadian Journal of Behavioural Science, 49, 78–88.\n\nFinally, to learn more about the history of factor analysis, you can consult:\n\nBriggs, D. D. (2022). Historical and Conceptual Foundations of Measurement in the Human Sciences. Credos and Controversies. Routledge."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html",
    "href": "chapters/ch21-sem/ch21-sem.html",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#introduction",
    "href": "chapters/ch21-sem/ch21-sem.html#introduction",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "1 Introduction",
    "text": "1 Introduction\nEducational research involves a variety of theoretical constructs that are not directly observable (e.g., motivation, engagement, cognitive development, and self-regulation) and that can only be indirectly studied by looking at participant’s responses to observable indicators of these constructs (e.g., participants’ responses to questionnaire items). The previous chapter about factor analysis [1] showed how to assess the factor structure of these unobserved, or latent, constructs and, thus, which items are good measurements of these constructs. This is crucial for developing valid instruments to measure (i.e., quantify) the latent constructs encountered in educational research. However, good measurement of latent constructs is typically not the researchers' ultimate goal. Usually, they subsequently want to answer questions about relationships or mean differences between (multiple) of these constructs, such as, “Is teacher motivation related to student engagement?” or “Does student engagement significantly differ for small and large-scale teaching styles?” If researchers only have (i) observed variables (e.g., years of teacher experience) for both predictor and outcome variables and (ii) want to investigate the effect of one or more predictor variables on a single outcome variable at a time (e.g., the effect of teacher experience and teacher gender on student GPA), these types of questions can be answered using familiar analysis methods such as multiple regression or ANOVA. However, as soon as questions involve latent variables or testing an entire system of (complex) interrelations between variables (like the ones found in most theoretical frameworks), researchers need an analysis technique with more flexibility that allows for modeling a multitude of relationships between (latent and/or observed) variables simultaneously [2]; that is, researchers need SEM.\nAt its core, SEM is a mix of factor analysis (discussed in the previous chapter) and a method called path analysis, which was invented by Wright [3]. One can think of path analysis as a type of multiple regression analysis because it also allows estimating and testing direct effects between variables. However, while multiple regression merely allows for testing the direct effects of predictor(s) on only a single outcome variable, path analysis can look at both direct and indirect effects between whole sets of predictor(s) on whole sets of other variables simultaneously. This implies that in path analysis, a variable can simultaneously be an outcome and a predictor: a variable might be predicted by one or more variables while also serving as a predictor for other variables. In other words, with path analysis, researchers can pretty much fit any model they can dream of as long as all variables are observed. Fitting a path model to latent variables requires going beyond path analysis. Fortunately, Jöreskog and Van Thillo [4] developed a statistical software called LISREL that allowed the use of latent variables in path analysis and thereby created SEM. Over the years, their work has been integrated into increasingly user-friendly software, contributing to the widespread use of the technique in social sciences today. This widespread use has also led to the definition of a SEM model becoming less concrete as the term is now also used for models with only observed variables, even though that would officially be a path model."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#literature-review",
    "href": "chapters/ch21-sem/ch21-sem.html#literature-review",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "2 Literature review",
    "text": "2 Literature review\nPerhaps the most well-known application of SEM in education research is the technology acceptance model (TAM), used to explain teachers’ adoption of education technology [5]. The model hypothesizes that a teacher's decision to adopt and use technology is influenced by their perceptions of its ease of use and usefulness. These perceptions shape their attitude toward technology use, which, in turn, affects their intention to use it and their actual usage. Such complex interplay in factors that influence technology acceptance requires using sophisticated methods such as SEM. The model has been extensively applied and validated in several contexts [6], investigating the adoption of multiple technological innovations such as serious games [7], virtual reality [8], or artificial intelligence [9]. The model has also been extended in several ways throughout the years to address the influence of other factors such as personality traits, result demonstrability, and risk, among many others [10].\nMany other applications of SEM exist in education research. For example Kusurkar et al. [11] used SEM to investigate how motivation affects academic performance. They hypothesized that greater levels of self-determined motivation are associated with the adoption of effective study strategies and increased study dedication, resulting in improved academic achievement. Their findings confirmed that the influence of motivation on academic performance is mediated by the use of effective study strategies. The work by Kucuk and Richardson [12] examined the interconnections between the three types of Community of Inquiry presence (teaching, social, and cognitive), four engagement components (agentic, behavioral, cognitive, and emotional), and satisfaction in online courses. The findings suggest that teaching presence, cognitive presence, emotional engagement, behavioral engagement, and cognitive engagement were significant predictors of satisfaction, explaining 88% of its variance.\nThe rise of digital learning and the opportunities for unobtrusive data collection have given rise to a new wave of studies that take advantage of the trace log data that students leave behind in online systems [13], instead of relying on self-reported data from questionnaires. For instance, Koç [14] proposed a model that explains the association between student participation and academic achievement. Student participation was measured through attendance to online lectures and discussion forum submissions. His results suggest that “student success in online learning [can] be promoted by increasing student participation in discussion forums and online lectures with more engaging learning activities” [14]. Fincham et al. [15] validated a theorized model of engagement in learning analytics using factor analysis and SEM. He found that affective engagement (measured by sentiment, sadness, and joy of students’ writing) and cognitive engagement (measured by syntactic simplicity, word concreteness, and referential cohesion) are not significantly associated with students’ final grades but academic and behavioral engagement (problem submissions, videos watched, and weeks active) are.\nThe rest of this chapter presents a recap of SEM, an integrated strategy for conducting a SEM analysis that is well suited for Educational Sciences, and an illustration of how to carry out a SEM analysis in R. Like the previous chapter, the presentation in this chapter will be kept applied and focus on how to conduct the analysis in R, instead of diving into technical details (for this, interested readers are referred to the readings listed at the end of this chapter)."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#recap-of-sem",
    "href": "chapters/ch21-sem/ch21-sem.html#recap-of-sem",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "3 Recap of SEM",
    "text": "3 Recap of SEM\nIn its most common form, SEM combines confirmatory factor analysis (CFA) with path analysis. Like CFA, SEM relates observed variables to latent variables that are measured by those observed variables, and, like path analysis, SEM allows for a wide range of relations between entire sets of variables (both latent and observed). As mentioned before, SEM is nowadays an umbrella term that also includes path analysis with only observed variables. However, this chapter focuses on SEM with latent variables to show its full potential. For the sake of brevity, it is assumed that the readers of this chapter have already read the factor analysis chapter, where a good description and recap of CFA (and how it differs from exploratory factor analysis) is provided.\nThe similarity between CFA and SEM is illustrated in Figure 1. It can be seen that both models consist of a part in which the observed and latent variables are related to each other (these relations are referred to as the measurement model) and a part in which the latent factors are related to each other (these relations are referred to as the structural model). The crucial difference between CFA and SEM is that factors in CFA can only correlate with each other (see Figure 1a). In contrast, SEM gives us more flexibility regarding (the set of) relationships between (observed and latent) variables. For example, Figure 1b shows the same factor model as Figure 1a. However, in Figure 1b, Factors 2 and 3 are both predicted by Factor 1a but are unrelated beyond that. Which model researchers specify, that is, what relations between (latent) variables they model, is entirely up to them. However, this flexibility is also a danger of SEM: the fact that researchers can fit any model they can think of does not mean they should. Ideally, the chosen relationships are driven by theory and substantive knowledge of the field.\n\n\n\n\n\n\n\n(a) A 3-factor CFA model with correlated factors, and Each factor is measured by three observed variables (X1 – X3, X4 – X6, X7 – X9).\n\n\n\n\n\n\n\n(b) A a SEM with one latent predictor variable and two latent outcome variables. The model is identical to the CFA model in a). However, now there are regression relationships between factors, as indicated by the straight arrows between them.\n\n\n\n\nFigure 1. CFA with covariances between factors vs. SEM with relationships between factors. The straight arrows from the factors to the observed variables indicate factor loadings (the regression weights obtained by regressing the observed variable on the factor). Double-headed arrows between factors indicate covariance. The \\(\\epsilon\\)’s represent errors. Curved single-headed arrows indicated variances. Each observed variable can also have an intercept, and each latent variable can also have a mean, but these are not explicitly indicated in the figure."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#integrated-strategy-for-structural-equation-modeling",
    "href": "chapters/ch21-sem/ch21-sem.html#integrated-strategy-for-structural-equation-modeling",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "4 Integrated Strategy for Structural Equation Modeling",
    "text": "4 Integrated Strategy for Structural Equation Modeling\nIn the following, we guide you, the researcher, through the different steps of conducting a SEM analysis. It is important to highlight that the most crucial part of a SEM analysis is checking its assumptions and checking measurement invariance (which was already briefly mentioned in the previous chapter). Measurement invariance means that the factor model underlying your instrument apply equally to relevant subgroups in your sample. In other words, measurement invariance implies that if you analyzed each of those subgroups with factor analysis separately (using the three steps described in the previous chapter), you would find the same model in each one. Measurement invariance is crucial: As discussed in the previous chapter, the interpretation of a latent variable depends on the factor model. Therefore, if the factor model of the latent variable(s) in your SEM model differs, for example, across biological sex, you effectively throw apples and oranges together if you analyze both males and females simultaneously in your analysis, which will result in uninterpretable outcomes. At the very least, you would want the same factor model for everyone in terms of the same number of factors and the same pattern of relationships between the factors and the observed variables. This scenario is called configural invariance. However, most research questions require higher levels of invariance, in which not only the number of factors and the pattern of factor loadings are the same across subgroups, but also the actual values of (most of the) parameters such as factor loadings and intercepts are equal across groups. More information about the different levels of invariance and how and when to test for it is presented below. After you have checked the assumptions and measurement invariance, you can safely apply and interpret a SEM analysis. The actual SEM analysis is just the final step of the modeling strategy presented here.\n\n4.1 Step 1: Steps from the previous chapter while assessing configural invariance\nSince SEM analyses typically look at the relationship of one or more latent variables with one or more other (latent or observed) variables, you first need to ensure that the latent variables represent what you think they represent. To this end, the first step is going through some or all the steps described in the previous chapter for every latent variable you include. If you use a new instrument to measure the latent variables, you should go through all the steps from (1) exploring the data structure, (2) building the factor model and assessing fit, and (3) assessing generalizability1. As explained in the previous chapter, step 3 should be conducted using a holdout sample or a new sample. However, you can use the same (holdout or new) sample for your SEM analysis. More on determining sufficient sample sizes is discussed below in the “SEM in R” section.\nOnce the factor model of your latent variables has been established or verified, you must ensure that this structure applies to all relevant subgroups in your sample. That is, you need to check for measurement invariance. The first level of invariance is configural invariance and, thus, whether the number of underlying factors and the pattern of relations between those factors and the observed variables is the same in each relevant subgroup. To test for configural invariance, you must go through the steps outlined in the previous chapter for each relevant group in your sample separately. Note that this also means you need sufficient observations for model building and generalizability in each subgroup you want to consider. Which subgroups are important to consider separately will always come down to theory.\nIf you use an existing instrument on a sample from a population for which the instrument had already been validated and configural invariance attained in previous research (i.e., you have strong a priori assumptions about the CFA model), you could skip the three steps from the factor analysis chapter and go straight to assessing the fit of the CFA model to your data. If it fits, you can continue with the SEM analysis. However, even when using existing validated instruments, it is strongly recommended to follow all the aforementioned steps for each relevant subgroup of your sample, as your sample might differ in important ways from the ones on which the instrument was validated, which could potentially introduce biases into your results.\n\n\n4.2 Step 2: Assessing Higher Levels of Invariance\nAfter establishing configural invariance between relevant groups, it is time to verify that the factor model parameters (specifically the loadings and intercepts) are also invariant across the factor models of each group. To illustrate why these “higher levels of invariance” are important, assume you want to investigate the relationship between children’s age and how engaged they are in class. You measure the latent variable “classroom engagement” using a factor model on the three observed variables engages in group activities, asks for assistance when needed, and prefers working alone. Of these three variables, asks for assistance might be more indicative of classroom engagement for girls than for boys due to cultural stereotypes. Boys could be way less likely to ask for assistance, not because they are less engaged, but because society taught them that boys should not ask for help and should instead figure things out themselves. In this example, the interpretation of the latent variable would be different between girls and boys: for girls, you truly measure classroom engagement; for boys, you measure an uninterpretable mix of classroom engagement and how much they internalized gender stereotypes. If you ignored this between-group difference in the meaning of the latent variable when including it in a SEM model, you would come to invalid conclusions about the structural relationships between (latent or latent and observed) variables. Fortunately, the type of invariance in this example would show up as a difference in the relation between the factor classroom engagement and the variable asks for assistance between boys and girls. In other words, it would show up as a difference in factor loadings between groups.\nTesting for higher levels of invariance in the factor model between categorical characteristics or “groups” (e.g., gender, nationality) can be done by testing for differences in model parameters between groups using a multi-group approach [16, 17] that compares the fit of a factor model in which all parameters are allowed to differ between groups to one in which one or more parameters are constrained to be equal across groups. If constraining parameters across groups does not cause a substantially worse model fit, invariance holds, and the constrained parameters can safely be considered and modeled as equal across groups. To assess for invariance across continuous variables (e.g., age) one could split the continuous variable into groups (e.g., changing the continuous variable age into the groups “young” and “old”), although one should ideally use more advanced methods like Moderated Non-Linear Factor Analysis [18] since categorizing continuous variables leads to loss of information. This chapter, however, will focus on the multi-group approach because it is the most common and widely available approach.\nThere are two important points about the invariance of model parameters that make the lives of researchers a little easier. First, not all model parameters have to be invariant. If you investigate relationships between (latent) constructs, as opposed to mean differences, only the factor loadings have to be invariant. For assessing differences in means, the factor loadings and the intercepts must be invariant. Note that the effects of categorical predictors on other variables also pertain to differences in means and that both loading- and intercept invariance are needed to interpret those effects. Note that a factor model comprises not only factor loadings and intercepts but also unique variances. However, unique variances do not have to be invariant across groups when looking at relationships or mean differences across latent variables. Therefore, we will not further consider them in this chapter. Second, the discussion of measurement invariance so far in this section has been about what is called full invariance, in which all parameters of a certain type (i.e., all factor loadings and/or all intercepts) are equivalent across groups (or across levels of a continuous variable). While this would represent an ideal situation, full invariance will rarely hold and is also not necessary. Partial invariance in which several, but not all, parameters of a certain type are invariant across groups is typically considered enough (e.g., the factor loadings of 4 out of 6 variables used to measure a factor are invariant across groups). Specifically, as long as at least two loadings are invariant across groups, one can look at relations between the corresponding latent variable and other (observed and/or latent) variables, and as long as at least two factor loadings and at least two intercepts are equal, one can meaningfully look at differences in means too ([19]; but also see [20]).\n\n\n4.3 Step 3: Building the Structural Equation Model and Assessing Fit\nAfter verifying the required level of measurement invariance for all latent variables that will be used in the SEM analysis, you are ready to investigate structural relations (e.g., regression relationships) between the factors (as well as observed variables if desired). Like with the factor models in the previous chapter, you first have to evaluate if the model fits your data sufficiently. You should only interpret the structural relations if the model fits the data. If the model does not fit, it is not a good description of the data and does not warrant further interpretation."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#sem-in-r",
    "href": "chapters/ch21-sem/ch21-sem.html#sem-in-r",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "5 SEM in R",
    "text": "5 SEM in R\nIn the following, you will be taken through the essential steps of performing SEM in the open-source software R. To this end, the same dataset [21] will be used as the one to which factor analysis was applied to in the previous chapter [1]. The dataset contains survey data about teacher burnout in Indonesia. In total, 876 respondents have answered questions on five domains: Teacher Self-Concept (TSC, 5 questions), Teacher Efficacy (TE, 5 questions), Emotional Exhaustion (EE, 5 questions), Depersonalization (DP, 3 questions), and Reduced Personal Accomplishment (RPA, 7 questions). Thus, the total number of variables equals 25. The questions were assessed on a 5-point Likert scale (ranging from 1 = “never” to 5 = “always”). For more information on the dataset, the interested reader is referred to the data chapter of the book [22].\nIn line with the seven hypotheses about the structural relationships in the original article, the SEM analysis presented below tests whether TE is predicted by TSC and whether EE, DE, and RPA are respectively predicted by TE and TSC. Note that, as SEM builds up on factor analysis, the code presented below also builds up on the factor analysis results and syntax from the previous chapter. Without reading the previous chapter, the steps in this chapter might therefore be harder to follow.\nTo provide an example of how to assess between-group invariance of the constructs, it will be tested whether the same factor structure holds across gender. More specifically, because the running example is only interested in relationships between constructs and not mean differences, only loading invariance will be tested for. However, code to also test for intercept invariance will also be provided.\n\n5.1 Preparation\nTo follow all the steps, you have to install the following packages with the function install.packages(). You only have to install them the first time you want to use them; therefore, the commands are commented below. Once you have the packages installed, you must load them with the library() function whenever you open the R script.\n\nlibrary(lavaan) # install.packages(\"lavaan\")\nlibrary(tidyverse) # install.packages(\"tidyverse\")\nlibrary(rio) # install.packages(\"rio\") \nlibrary(psych) # install.packages(\"psych\")\nlibrary(combinat) # install.packages(\"combinat\")\nlibrary(devtools) # install.packages(\"devtools\")\nlibrary(sleasy) # devtools::install_github(\"JoranTiU/sleasy\")\n\n\n5.1.1 Reading in the data\nThe data can be read in, and the variable names can be extracted with the following commands:\n\ndataset <- import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")\nvar_names <- colnames(dataset)\n\nIn the following, you will also find commands to add a gender variable to the dataset. This variable is only added to demonstrate how to perform invariance tests. Although the original data contains gender, it is unfortunately not part of the publicly shared data.2 The proportion of men (gender = 0) and women (gender = 1) in the created variable aligns with the reported values from the article corresponding to the dataset.\n\nset.seed(1611)\ndataset$gender <- as.factor((sample(c(rep(0, 618), rep(1, 258)))))\n\n\n\n5.1.2 Are the data suited for SEM?\nSeveral data characteristics are necessary for SEM. These largely overlap with the necessary data characteristics for factor analysis, as discussed in the previous chapter (i.e., are the variables continuous? Are the correlations between the variables sufficiently large? Is there enough common variance among the variables? Are the data normally distributed?). Therefore, the steps in R to check whether the data are suited will not be presented here, as you can look those up in the previous chapter. To summarize: The data were suited for analysis with factor analysis and SEM, but the distribution of the variables is somewhat left-skewed. Therefore, an estimation method that is robust against non-normality should be used.\nThe one characteristic that needs additional investigation is the sample size. As mentioned in the previous chapter on factor analysis, Bentler and Chou [23] recommend having 5 observations per estimated parameter, while Jackson [24] recommends having 10, and preferably 20 observations, for each parameter you want to estimate. Therefore, the total number of observations depends on the exact model fitted to the data. If you start your SEM analysis with going through all three recommended model-building steps from the previous chapter (see the “Integrated Strategy for Structural Equation Modeling” above), you need to base your required sample size on the number of parameters estimated by an exploratory factor analysis (e.g., for a two-factor model fitted to 10 variables, you would need about (20 factor loadings + 20 intercepts + 20 error-variances) * 10 = 600 cases) and multiply that by 2 to also have enough observations for a new or holdout sample to check generalizability of your factor-model and for fitting your SEM model. Note that the sample size calculated this way is what you need in each relevant subgroup for which you want to assess measurement invariance. As mentioned above, you could go straight to the SEM analysis if you use an existing instrument on a sample from a population for which the instrument has already been validated and shown to be invariant across relevant groups in previous research. In that case, you would need fewer observations because a SEM model (like a CFA model) will have more constraints than an exploratory factor analysis model and hence fewer parameters to estimate.\nYou will perform the SEM analysis using the holdout sample from the previous chapter. The sample size for that is 438, which is sufficiently large to estimate 20 intercepts, 20 loadings, 20 unique variances, and 10 relations between factors according to the more lenient rules by Bentler and Chou [23]. As you will see in Step 2, this number of parameters aligns well with the SEM model that will be fitted to the data.\n\n\n\n5.2 Step 1: Steps from the previous chapter\nAs explained in the strategy section, it is recommended to go through all the steps mentioned in the previous chapter (exploring the data structure, building the factor model and assessing fit, and assessing generalizability), and the R code for these steps can also be found there. For brevity, the steps and code will not be repeated here. Additionally, configural invariance (tested by following the steps from the previous chapter for each relevant subgroup separately) will be assumed to hold so that this section can focus on the code needed for a SEM analysis. We begin with picking up at the code for randomly assigning 438 rows of the data to a holdout dataset:\n\nset.seed(19)\nind <- sample(c(rep(\"model.building\", 438), rep(\"holdout\", 438)))\ntmp <- split(dataset, ind)\nmodel.building <- tmp$model.building\nholdout <- tmp$holdout\n\nFor an explanation of the code, please go back to the previous chapter.\n\n\n5.3 Step 2: Assessing Higher Levels of Invariance\nBefore looking into the structural relationships of interest, you have to ensure that the loading (and possibly intercept) invariance holds across groups for which this might be violated according to theory. To this end, you first have to specify the model, which is very similar to the model that was specified in the previous chapter for the CFA model. The model syntax looks as follows:\n\nSEM_model <- '\n# Regressing items on factors \nTSC =~ TSC1 + TSC2 + TSC3 + TSC4 + TSC5\nTE =~ TE1 + TE2 + TE3 + TE5\nEE =~ EE1 + EE2 + EE3 + EE4\nDE =~ DE1 + DE2 + DE3\nRPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n# Relations between factors\nTE ~ TSC\nEE ~ TE + TSC\nDE ~ TE + TSC\nRPA ~ TE + TSC\n'\n\nThe first part of this syntax is exactly the same as the syntax that defined the CFA model in the previous chapter, with the syntax elements “=~” indicating that a factor is measured by specific observed variables. The code above defines a model where the factors TSC, TE, EE, DE, and RPA are measured by different sets of variables, which are separated by “+”. In addition to the measurement part, this syntax includes the specification of relations between factors using the “~” operator. For example, TE is regressed onto TSC, indicating a directional relationship from TSC to TE. When comparing this syntax to the one from the previous chapter, you will see that the only thing that changes when you move from CFA to SEM is that you specify concrete structural relations (i.e., which latent factors are regressed on each other) instead of solely specifying correlations. Although intercepts are not explicitly mentioned in the model, they can be included by using the meanstructure = TRUE argument in the command when estimating the model.\nOnce the model syntax is specified, you can check whether invariance across groups (here, gender) holds. More specifically, you check this by comparing a model with and without equality constraints on the parameters of interest (here, as motivated before, only the loadings) and check whether adding constraints reduces model fit more than desired. To evaluate the degree of fit reduction, you can look at the changes in the global fit measures described in the previous chapter. These were (1) the Chi-squared significance test, (2) the comparative fit index (CFI), (3) the root mean square error of approximation (RMSEA), and (4) the standardized root mean square residual (SRMR). Recall that, unlike the Chi-squared significance test that assesses perfect fit, the CFI, RMSEA, and SRMR assess approximate fit. Similarly, only the Chi-squared significance test is a formal test for assessing invariance. As for assessing global fit, each fit measure is accompanied by rules of thumb when assessing invariance, allowing you to decide whether or not (approximate) invariance holds. The difference is that the rules of thumb now pertain to differences in the criteria between models with and without constraints. The Chi-significance test should be nonsignificant because otherwise, the model with constraints fits significantly worse. Keep in mind, however, that this test easily rejects the assumption of invariance with increasing sample size. Regarding the other three measures, the rules of thumb are that differences in CFI should be smaller than or equal to .01, differences in RMSEA should be smaller than or equal to .015, and differences in SRMR should be smaller than or equal to .030 [25].\nYou do not have to perform the invariance assessment by constraining parameters yourself. Instead, you can use the command below. The argument model asks you to specify the model syntax, data asks for the dataset that you want to use for the analysis, and group refers to the variable for which you want to assess invariance. The argument estimator indicates which estimation procedure is used. The default is standard maximum likelihood (“ML”) estimation. However, for the current data, a robust maximum likelihood (“MLR”) estimation is applied to account for small violations of the normality assumption. If the data contain missing values, you can add the argument missing and specify it as equal to “fiml”, corresponding to a full information maximum likelihood approach. As already discussed in the previous chapter, this is a sensible approach if you have at least missing at random (MAR) data details about missing data mechanisms can be found in the lavaan tutorial [26]. Next, the argument “intercept” indicates whether intercept invariance should be assessed in addition to loading invariance. If put to TRUE, the output contains information on both loading and intercept invariance unless two or more of the four fit criteria indicate that loading invariance is violated because then, assessing intercept invariance does not make sense. This information would be provided in the form of a message. Finally, the argument “display” indicates whether output about group-specific parameters and differences are provided (when set to TRUE) or not (when set to FALSE).\n\ninvarianceCheck(model = SEM_model, data = holdout, \n                group = \"gender\", estimator = \"MLR\", intercept = FALSE, \n                missing = FALSE, display = FALSE)\n\nNested Model Comparison------------------------------------------------------------------- \n \n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan NOTE:\n    The \"Chisq\" column contains standard test statistics, not the\n    robust test that should be reported per model. A robust difference\n    test is a function of two standard (not robust) statistics.\n \n                    Df   AIC   BIC  Chisq Chisq diff Df diff Pr(>Chisq)\nLoadings_Free      320 15711 16283 648.06                              \nLoadings_Invariant 335 15693 16203 660.17     9.8747      15     0.8275\n\nModel Fit Indices ------------------------------------------------------------------------ \n \n                   chisq.scaled df.scaled pvalue.scaled rmsea.robust cfi.robust\nLoadings_Free          632.823†       320          .000        .068       .923 \nLoadings_Invariant     638.987        335          .000        .065†      .925†\n                    srmr\nLoadings_Free      .050†\nLoadings_Invariant .053 \n\nDifferences in Fit Indices --------------------------------------------------------------- \n \n                                   df.scaled rmsea.robust cfi.robust  srmr\nLoadings_Invariant - Loadings_Free        15       -0.002      0.002 0.002\n\nLoading Invariance Interpretation -------------------------------------------------------- \n \nThe hypothesis of perfect loading invariance *is not* rejected according to the\n      Chi-Square difference test statistics because the p-value is larger or equal to 0.05. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n        according to the CFI because the difference in CFI value is smaller than\n        or equal to 0.01. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n       according to the RMSEA because the difference in RMSEA value is smaller\n       than or equal to 0.015. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n       according to the SRMR because the difference in SRMR value is smaller than\n       or equal to 0.030. \n \n\n\nInspecting the output, you can see several sections. You may directly go to the last section “Loading Invariance Interpretation” because this provides you with information on whether or not invariance is rejected according to the four fit measures (following the cut-off values provided before). If you are interested in (reporting) details about the Chi-square significance test and the (differences in) model fit criteria, you can also inspect the beginning of the output.3 It can be seen that invariance is not rejected according to any of the criteria. Similar to assessing model fit in factor analysis, it is advised that no more than one criterion should reject invariance for concluding that invariance holds. If this is initially not the case, you may release one equality constraint at a time (i.e., moving towards partially invariant models) and run the invarianceCheck() function again until partial invariance holds. In order to decide which constraints to release, you can consult the information provided when the “display” argument is set to TRUE. The output will then show the loadings and intercepts per group as well as the differences in these parameters across groups. You will also get a message indicating which loading (and intercept) differs most between groups. These would be the first targets to estimate freely across groups (while making sure that the minimum requirements for partial invariance mentioned above hold). How to update the model syntax to freely estimate parameters between groups is explained on the lavaan package website [26].\n\n\n5.4 Step 3: Building the Structural Equation Model and Assessing Fit\nThe next step is to perform the SEM analysis on the holdout data using the specified SEM model with the following command:\n\nsem_robust <- sem(model = SEM_model, data = holdout, std.lv = TRUE,\n              estimator = \"MLR\", meanstructure = TRUE)\n\nThe arguments are the same as for the cfa() function discussed in the previous chapter. The relevant (standardized) output about the structural relations can be extracted from the SEM model with the command below. The first argument asks you to specify the lavaan object from which you want to extract the structural relations, nd lets you specify the number of decimals you want to display (with default 3).\n\nsem_structural_results(sem_robust, nd = 3)\n\n  outcome predictor std estimate    se p-value\n1     TSC        TE        0.693 0.121       0\n2     TSC        EE        0.547  0.17       0\n3     TSC        DE          0.4 0.143       0\n4     TSC       RPA        0.413 0.111       0\n5      TE        EE         0.32 0.114       0\n6      TE        DE        0.293 0.088   0.001\n7      TE       RPA        0.326 0.084       0\n\n\nLooking at the output from left to right, you can see the outcome variable of the structural relationship, the predictor of the structural relationship, the regression coefficient, the standard error of the regression coefficient, and the p-value of the effect. Here, you see that all structural relations are significantly different from zero and thus that TE, EE, DE, and RPA are all related to TSC and that EE, DE, and RPA are related to TE. As mentioned, the function sem_structural_results() displays the most relevant information about the structural relations. For more information and unstandardized estimates, you can use the summary() function on the lavaan object sem_robust also see the lavaan tutorial by [27]."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#conclusion",
    "href": "chapters/ch21-sem/ch21-sem.html#conclusion",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nSEM is a rich, powerful, and mature statistical framework that has more than a century of evolution and expansion. Several other extensions have been developed to include dynamic structural equation modelling for the analysis of temporal data, latent growth curve modelling for the analysis of systematic change over time, and multilevel SEM for the analysis of hierarchically clustered (or nested) data, to mention just a few. This chapter merely provided a primer that introduces the basics of the methods and an example application. Hopefully, it opens the door for interested readers to explore such a powerful framework that offers an excellent solution to many of the analytical tasks that researchers seek."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#further-readings",
    "href": "chapters/ch21-sem/ch21-sem.html#further-readings",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "7 Further readings",
    "text": "7 Further readings\nIn this chapter, you have seen an introduction and tutorial on how to apply SEM in educational research. To learn more about how SEM can be applied to this field, you can consult these resources:\n\nTeo, T., Ting Tsai, L., & Yang, C. 2013. \"Applying Structural Equation Modeling (SEM) in Educational Research: An Introduction\". In Application of Structural Equation Modeling in Educational Research and Practice.\nKhine, M. S., ed. 2013. “Application of Structural Equation Modeling in Educational Research and Practice”. Contemporary Approaches to Research in Learning Innovations.\n\nTo learn more about SEM in general, you can refer to the following:\n\nKline, R. B. 2015. “Principles and Practice of Structural Equation Modeling”. 4th Edition. Guilford Publications.\nHoyle, Rick H. 2012. “Handbook of Structural Equation Modeling”. Guilford Press."
  }
]